<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>IEEE 机器人与自动化快报 - 新目录</title>
    <link>http://ieeexplore.ieee.org</link>
    <description>出版物目录提醒# 7083369</description>
    <lastBuildDate>Wed, 06 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>CMGFA：基于跨模态群组混合注意特征聚合器的 BEV 分割模型</title>
      <link>http://ieeexplore.ieee.org/document/10749835</link>
      <description><![CDATA[鸟瞰图 (BEV) 分割图是自动驾驶领域的最新进展，可提供有效的环境信息，例如可驾驶区域和车道分隔线。大多数现有方法使用摄像头和激光雷达作为分割的输入，不同模态的融合是通过连接或加法运算完成的，无法充分利用模态之间的相关性和互补性。本信介绍了 CMGFA（跨模态组混合注意特征聚合器），这是一个端到端学习框架，可以适应用于 BEV 分割的多模态特征组合。CMGFA 包括以下组件：i）摄像头具有双分支结构，可加强局部和全局特征之间的联系。ii）多头可变形交叉注意被用作跨模态特征聚合器，以聚合 BEV 中的摄像头、激光雷达和雷达特征图以进行隐式融合。iii）组混合注意用于丰富注意图特征空间并增强在不同类别之间分割的能力。我们在 nuScenes 和 Argoverse2 数据集上评估了我们提出的方法，其中 CMGFA 的表现明显优于基线。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10749835</guid>
      <pubDate>Mon, 11 Nov 2024 13:17:25 GMT</pubDate>
    </item>
    <item>
      <title>一步步思考：机器人手术视频中用于错误检测的手势链提示</title>
      <link>http://ieeexplore.ieee.org/document/10750058</link>
      <description><![CDATA[尽管机器人系统和手术数据科学取得了进步，但确保机器人辅助微创手术 (RMIS) 的安全执行仍然具有挑战性。目前的手术错误检测方法通常涉及两个部分：识别手势，然后检测每个手势片段中的错误。这些方法通常忽略了手术视频中固有的丰富上下文和语义信息，由于依赖于准确的手势识别，性能有限。受自然语言处理中的思路链提示的启发，这封信提出了一种新颖的实时端到端错误检测框架，即手势链 (COG) 提示，逐步整合了手术视频中的上下文信息。这包括两个模拟专家外科医生决策的推理模块：一个使用转换器和注意力架构进行手势提示的手势-视觉推理模块和一个采用多阶段时间卷积网络的多尺度时间推理模块，该网络具有慢速和快速路径，用于提取时间信息。我们在 JIGSAWS 数据集上验证了我们的方法，结果显示该方法比最先进的方法有所改进，F1 得分提高了 4.6%，准确率提高了 4.6%，Jaccard 指数提高了 5.9%，平均帧处理时间为 6.69 毫秒。这证明了我们的方法有潜力提高 RMIS 安全性和外科教育效果。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10750058</guid>
      <pubDate>Mon, 11 Nov 2024 13:17:25 GMT</pubDate>
    </item>
    <item>
      <title>HuMAn – 基于循环神经网络的人体运动预测算法</title>
      <link>http://ieeexplore.ieee.org/document/10750298</link>
      <description><![CDATA[预测人体运动可能为人机交互带来显著优势，尤其是当机器人运动和用户运动之间的精确同步至关重要时。人类行为固有的随机性，加上有限的响应窗口，可能会在交互过程中产生残余和不良力，从而可能对用户造成伤害。因此，有效预测人体关节运动可以提高可穿戴机器人中使用的各种交互控制框架的性能。本文提出了基于循环神经网络预测人体关节运动的 HuMAn 算法。该算法由一个用于解释姿势序列的长期记忆网络和一个用于在指定时间范围内构建最可能的未来用户姿势的预测层组成。网络训练使用涵盖各种主题和运动类型的数据集进行。结果证明了所提算法的有效性，平均一般预测误差低于 0.1 弧度，预测范围长达 500 毫秒。此外，周期性跑步机行走的平均绝对误差为 0.026 弧度。仿真结果表明，在上肢外骨骼机器人的案例研究中，透明度控制性能得到了很大的改善。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10750298</guid>
      <pubDate>Mon, 11 Nov 2024 13:17:25 GMT</pubDate>
    </item>
    <item>
      <title>基于 RNN 的视觉引导，增强具有时变延迟的远程操作中的主体感</title>
      <link>http://ieeexplore.ieee.org/document/10750248</link>
      <description><![CDATA[直观的远程操作使操作员能够体现远程机器人，在控制过程中提供机器人是自己身体一部分的感觉。代理感 (SoA)，即控制机器人的感觉，有助于增强远程操作期间的动机和体现。然而，与远程操作相关的随时间变化的通信延迟可能会削弱 SoA。我们提出了一种视觉引导系统，以协助操作，同时在随时间变化的延迟远程操作机器人时保持较高的 SoA，从而提高定位精度。在所提出的系统中，一个循环神经网络 (RNN) 模型，在熟练操作员的倾倒任务上进行训练，在新手操作员输入前 500 毫秒预测输入位置，并将其实时可视化为末端执行器目标位置。具有随时间变化的延迟的实验证实，所提出的方法提供了从操作员的实时输入在时间和空间上插入的目标位置的视觉表示，引导操作员与熟练操作员的轨迹保持一致。与其他条件相比，所提出的方法即使在时变延迟下也能显著提高任务性能，同时保持较高的 SoA。将本研究中开发的预测系统应用于人机协作控制可能会实现保持 SoA 的干预措施。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10750248</guid>
      <pubDate>Mon, 11 Nov 2024 13:17:25 GMT</pubDate>
    </item>
    <item>
      <title>通过软磁材料可控制减少粘附的磁性爬壁轮</title>
      <link>http://ieeexplore.ieee.org/document/10750310</link>
      <description><![CDATA[随着桥梁和工厂设施等关键基础设施的老化，开发使用永磁体的创新型爬壁机器人变得越来越重要。这种机器人的传统设计依赖于控制升降机或永磁体的位置来控制粘附情况，这会带来严重的安全隐患，包括表面转换期间的粘附不一致以及失去控制时跌倒的风险。为了克服这些问题，本文介绍了一种新型磁轮设计，该设计利用软磁材料 (SMM) 来控制特定方向上的粘附力降低。通过对各种磁铁和 SMM 配置的比较分析，展示了对粘附的影响。基于这些分析，我们提供了一种轮子设计，并研究了 SMM 覆盖区域的影响。为了验证减少粘附的有效性，我们展示了一种带有所提轮子的机器人，并对其进行了分析，以实现爬壁任务。在实验中，配备了所提轮子的原型机器人在受控条件下执行爬壁任务时表现出更高的安全性。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10750310</guid>
      <pubDate>Mon, 11 Nov 2024 13:17:25 GMT</pubDate>
    </item>
    <item>
      <title>电粘附垫设计可增强微型机器人在不同地形上的攀爬附着力</title>
      <link>http://ieeexplore.ieee.org/document/10750275</link>
      <description><![CDATA[虽然之前的研究已经使用昆虫级 Harvard Ambulatory Microrobot 平台探索了电粘附攀爬，但机器人在崎岖地形上可靠攀爬的能力仍然有限。为了评估潜在的解决方案，我们对电粘附垫设计空间进行了调查，并描述了具有不同垫设计的机器人的剪切力攀爬能力。我们发现，在光滑平坦的地形上，大型简单的圆形脚垫结构表现出最大的剪切力。然而，在粗糙的倾斜表面上，调整宽度、长度和辐条状特征数量的垫可提供更大的柔顺性并实现更一致的剪切粘附力。这种柔顺辐条垫设计在粗糙表面上的粘附可靠性为 84%，平均粘附力为 1.02 kPa，而同类圆形垫的粘附可靠性为 45%，平均粘附力为 0.81 kPa。我们展示了 4.5 厘米机器人在粗糙度为 75 $\mu$m 的地形上改进的攀爬能力，并观察到在 0-45 度的角度范围内攀爬速度平均增加了 48%。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10750275</guid>
      <pubDate>Mon, 11 Nov 2024 13:17:25 GMT</pubDate>
    </item>
    <item>
      <title>利用天光偏振模式约束的视觉惯性定位</title>
      <link>http://ieeexplore.ieee.org/document/10748383</link>
      <description><![CDATA[在本信中，我们开发了一种紧密耦合的偏振-视觉-惯性定位系统，该系统利用自然属性的偏振天光来提供全局航向。我们引入了一个焦平面偏振相机，其瞬时视场误差可忽略不计，以收集偏振天光。然后，我们设计了一种从偏振天光确定航向的稳健方法，并构建了一个全局稳定航向约束。具体而言，此约束补偿了标准 VINS 中存在的航向不可观测性。除了 VINS 中使用的标准稀疏视觉特征测量外，还在紧密耦合的 VINS 更新中构建和共同优化了偏振航向残差。设计了一种自适应融合策略来校正累积漂移。户外真实世界实验表明，所提出的方法在定位精度方面优于最先进的 VINS-Fusion，并且在树木繁茂的校园环境中比 VINS-Fusion 提高了 22%。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10748383</guid>
      <pubDate>Mon, 11 Nov 2024 13:17:24 GMT</pubDate>
    </item>
    <item>
      <title>SGLC：用于 LiDAR SLAM 的语义图引导粗-精-细全环路闭合</title>
      <link>http://ieeexplore.ieee.org/document/10750042</link>
      <description><![CDATA[回环闭合是 SLAM 中的关键组件，它通过两个主要步骤帮助消除累积误差：回环检测和回环姿势校正。第一步确定是否应执行回环闭合，而第二步估计 6-DoF 姿势以校正里程计漂移。当前的方法主要侧重于开发用于回环闭合检测的稳健描述符，通常忽略回环姿势估计。一些包含姿势估计的方法要么准确度低，要么计算成本高。为了解决这个问题，我们引入了 SGLC，这是一种实时语义图引导的全回环闭合方法，具有稳健的回环闭合检测和 6-DoF 姿势估计功能。SGLC 考虑了前景和背景点的不同特征。对于前景实例，它构建了一个语义图，不仅可以抽象点云表示以快速生成和匹配描述符，还可以指导后续的回环验证和初始姿势估计。同时，利用背景点为扫描描述符构建提供更多几何特征，并为进一步的姿势细化提供稳定的平面信息。循环姿势估计采用粗-精-细配准方案，同时考虑实例点和背景点的对齐，效率高、准确度高。在多个公开数据集上进行的大量实验证明了其优于最先进的方法。此外，我们将 SGLC 集成到 SLAM 系统中，消除了累积误差并提高了整体 SLAM 性能。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10750042</guid>
      <pubDate>Mon, 11 Nov 2024 13:17:24 GMT</pubDate>
    </item>
    <item>
      <title>多智能体路径寻找中的虚拟障碍物调控</title>
      <link>http://ieeexplore.ieee.org/document/10747284</link>
      <description><![CDATA[多智能体路径查找 (MAPF) 涉及为多个智能体查找无碰撞路径，同时最小化总路径成本。显式估计冲突搜索 (EECBS) 代表广泛使用的基于冲突的搜索 (CBS) 方法的最先进的变体，提供有界次优解决方案。然而，CBS 及其变体都依赖于成对冲突解决方法。冲突激增意味着许多冲突发生在一个位置，这在大量智能体在小空间中操作的场景中经常存在，并且通常会导致繁重的计算负担。冲突激增发生的位置被视为冲突激增顶点。这封信提出了一种新方法，即虚拟障碍物调节，以加快 MAPF 的算法求解过程（例如 EECBS）。所提出的方法识别冲突激增顶点并策略性地将它们调节为全局或局部虚拟障碍物，以规避集中冲突。然后，冲突繁荣顶点上的成对冲突解决过程被大大简化，从而加速了整个算法的运行时间——通常由冲突解决主导。数值研究验证了这种方法的有效性。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10747284</guid>
      <pubDate>Fri, 08 Nov 2024 13:16:55 GMT</pubDate>
    </item>
    <item>
      <title>无界 GS：通过混合表示扩展 3D 高斯分层，实现无界大规模场景重建</title>
      <link>http://ieeexplore.ieee.org/document/10747249</link>
      <description><![CDATA[由于视觉质量和计算成本之间的权衡困境，从多视角图像建模大规模场景具有挑战性。现有的基于 NeRF 的方法已经通过体积光线行进在神经隐式表示方面取得了进展，但仍然难以处理大规模场景中立方增长的采样空间。幸运的是，基于 3D 高斯溅射（3DGS）的渲染方法已经显示出有希望的结果，激发了在溅射设置中的进一步探索。然而，3DGS 的局限性是高斯点不足以建模远处的背景，从而导致“斑点”伪影。为了解决这个问题，我们引入了一种称为无界 3D 高斯的新型混合神经表示。对于前景区域，我们采用显式 3D 高斯表示，通过溅射加权高斯来有效地建模几何和外观。对于远处的背景，我们还引入了一个隐式模块，该模块由多层感知器 (MLP) 组成，可直接从视点位置和射线方向的位置编码预测远处的背景颜色。此外，我们设计了一种无缝融合机制，将显式 splatting 和隐式分支的颜色预测结合起来，以重建整体场景。大量实验表明，我们提出的 Unbounded-GS 继承了更快收敛和高保真渲染质量的优势。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10747249</guid>
      <pubDate>Fri, 08 Nov 2024 13:16:55 GMT</pubDate>
    </item>
    <item>
      <title>一种用于磁导航系统增强旋转磁场的电压最小化控制方法</title>
      <link>http://ieeexplore.ieee.org/document/10748361</link>
      <description><![CDATA[磁螺旋机器人 (MHR) 可由旋转磁场 (RMF) 驱动，并可利用其旋转运动穿过堵塞的血管。我们提出了一种方法，使每个线圈产生磁导航系统 (MNS) 的 RMF 所需的电压最小化。所提出的方法在 MNS 的额定电压下最大化 RMF。此外，为了抑制 MHR 高速旋转运动期间阻抗的增加，所提出的方法利用了应用极小极大优化方法的共振。产生 RMF 所需的电压是通过分析得出的，目的是实现快速优化过程，以便可以实时控制 MHR。通过测量磁通密度对所提出的方法进行了实验验证。此外，我们从体外实验中展示了 MHR 增强的导航和隧道性能。最后，我们在猪的股浅动脉内的体内实验中验证了 MHR 的导航和隧道运动。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10748361</guid>
      <pubDate>Fri, 08 Nov 2024 13:16:55 GMT</pubDate>
    </item>
    <item>
      <title>两架无人机之间悬挂电缆的形状视觉伺服</title>
      <link>http://ieeexplore.ieee.org/document/10747267</link>
      <description><![CDATA[在本信中，我们提出了一种形状视觉伺服方法来操纵连接在两个四旋翼无人机之间的悬挂电缆。提出了一种领导者-跟随者控制策略，其中人类操作员通过遥控一架无人机（领导者）来控制电缆的刚性运动，而第二架无人机（跟随者）执行形状视觉伺服任务以自动对电缆施加所需的变形。所提出的形状视觉伺服方法使用嵌入在跟随无人机上的 RGB-D 摄像头，其优势在于依赖于只需要知道其长度的简单电缆几何模型。同时，我们的控制策略保持了电缆在摄像头视野范围内的最佳可见性。强大的图像处理管道允许根据机载 RGB-D 摄像头提供的数据实时检测和跟踪电缆形状。实验结果证明了所提出的视觉控制方法将柔性电缆塑造成所需形状的有效性。此外，我们通过实验证明，该系统可用于执行空中运输任务，即用电缆抓住装有钩子的物体，然后移动并在另一个位置释放它。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10747267</guid>
      <pubDate>Thu, 07 Nov 2024 13:18:47 GMT</pubDate>
    </item>
    <item>
      <title>适用于人机协作领域的具有联邦学习功能的无人机近端控制</title>
      <link>http://ieeexplore.ieee.org/document/10742553</link>
      <description><![CDATA[人机交互 (HRI) 是一个不断发展的研究领域。在 HRI 中，复杂的命令（动作）分类仍然是一个悬而未决的问题，通常会阻碍这种技术的实际应用。文献介绍了一些使用神经网络来检测这些动作的研究。然而，遮挡仍然是 HRI 中的一个主要问题，尤其是在使用无人驾驶飞行器 (UAV) 时，因为在机器人移动过程中，人类操作员经常不在机器人的视野范围内。此外，在多机器人场景中，分布式训练也是一个悬而未决的问题。从这个意义上讲，这项研究提出了一种基于长短期记忆 (LSTM) 深度神经网络的动作识别和控制方法，该网络具有两层，与三层紧密连接的层相关联，并嵌入在多个无人机中的联邦学习 (FL)。FL 使我们的方法能够以分布式方式进行训练，即无需云或其他存储库即可访问数据，这有助于多机器人系统的学习。此外，我们的多机器人方法结果还避免了遮挡情况，对真实机器人的实验实现了超过 96% 的准确率。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10742553</guid>
      <pubDate>Mon, 04 Nov 2024 13:19:21 GMT</pubDate>
    </item>
    <item>
      <title>服务环境中协作 SLAM 的基准数据集</title>
      <link>http://ieeexplore.ieee.org/document/10742554</link>
      <description><![CDATA[我们为各种室内服务环境中的多个服务机器人引入了一种新的多模态协作 SLAM (C-SLAM) 数据集，称为服务环境中的 C-SLAM 数据集 (CSE)。我们使用 NVIDIA Isaac Sim 在各种室内服务环境中生成数据，并应对现实服务环境中可能出现的挑战。通过使用模拟器，我们可以提供精确时间同步的传感器数据，例如立体 RGB/深度、IMU 和地面真实 (GT) 姿势。我们配置了三种常见的室内服务环境（医院、办公室和仓库），每种环境都具有执行适合环境的运动的动态物体。此外，我们驱动机器人模仿真实服务机器人的动作。通过这些因素，我们为多个服务机器人生成了逼真的 C-SLAM 数据集。我们通过评估各种最先进的单机器人 SLAM 和多机器人 SLAM 方法来展示我们的 CSE 数据集。此外，我们还提供了有关使用模拟器生成 C-SLAM 数据的详细教程。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10742554</guid>
      <pubDate>Mon, 04 Nov 2024 13:19:21 GMT</pubDate>
    </item>
    <item>
      <title>事件框架立体相机系统的动态校准框架</title>
      <link>http://ieeexplore.ieee.org/document/10742513</link>
      <description><![CDATA[事件相机与传统帧相机的融合是一个新颖的研究领域，由事件相机和帧相机组成的立体结构可以兼具两者的优点。本文开发了一种事件帧立体相机系统的动态标定框架。在该框架中，第一步是在圆网格标定图案上完成初始检测，并提出一种滑动窗口时间匹配方法匹配事件帧对。然后，设计了一种针对两台相机的细化方法以获得图案的精确信息。特别地，对于事件相机，设计了一种高计算效率的块大小运动补偿方法，实现两台相机的时间同步并拟合扭曲事件图像中的圆。最后，通过构建具有两类边的姿势地标图来全局优化两台相机之间的姿势。提出的标定框架具有高实时性和易于部署的优点，并通过基于自记录数据集的实验验证了其有效性。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10742513</guid>
      <pubDate>Mon, 04 Nov 2024 13:19:21 GMT</pubDate>
    </item>
    </channel>
</rss>