<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>IEEE 机器人与自动化快报 - 新目录</title>
    <link>http://ieeexplore.ieee.org</link>
    <description>出版物目录提醒# 7083369</description>
    <lastBuildDate>Mon, 30 Sep 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>一种受折纸启发的具有触觉的内窥镜胶囊，可用于早期组织异常检测</title>
      <link>http://ieeexplore.ieee.org/document/10697264</link>
      <description><![CDATA[视频胶囊内窥镜 (VCE) 是目前检测肠道疾病最有效的方法之一。然而，用这种方法检测早期小结节具有挑战性，因为它们缺乏明显的颜色或形状特征。在本文中，我们介绍了一种新的折纸胶囊内窥镜，利用触觉感应检测早期小肠结节。四个由压阻材料制成的软触觉传感器输入四通道相移数据，这些数据使用粒子滤波器进行处理。粒子滤波器使用重要性分配模板，该模板是使用来自六种已知大小结节的实验数据设计的。此外，所提出的胶囊可以利用形状变化在蠕动下被动地向前或向后移动。在实验中，它能够返回到直的二维肠道模型中的特定区域进行重复检测。实验结果表明，所提出的胶囊可以 100% 的准确率检测直径超过 3 毫米的结节。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10697264</guid>
      <pubDate>Fri, 27 Sep 2024 13:18:22 GMT</pubDate>
    </item>
    <item>
      <title>膀胱镜经尿道连续手术机器人系统远端模块的概念验证开发</title>
      <link>http://ieeexplore.ieee.org/document/10696949</link>
      <description><![CDATA[经尿道膀胱肿瘤切除术 (TURBT) 是治疗非肌层浸润性膀胱肿瘤的典型手术。然而，目前使用刚性手术工具的 TURBT 很难处理膀胱肿瘤的整块切除和前部肿瘤切除。因此，本文提出了一种基于远程操作的膀胱镜经尿道连续手术机器人系统，并对其远端模块进行了概念验证开发。远端模块采用并联连续机器人 (PCR) 形式，充分利用尿道横截面并实现无标记视觉闭环控制。远端模块的设计从 PCR 的拓扑优化开始。然后，对 PCR 进行建模和优化以获得更好的结构刚度。此外，由于 PCR 的移动平台具有环形形状，根据轮廓检测算法，在内窥镜图像中很容易识别，因此使用 PCR 可以固有地促进无标记视觉闭环控制。通过对概念验证结构进行实验来评估远端模块的功能。远端模块可在遥操作下通过双极环抓握膀胱黏膜并进行肿瘤切除。实验结果表明，外径为7 mm 的3自由度远端模块在闭环控制下实现了φ25 mm球体的功能工作空间体积和0.40 mm的平均位置误差。闭环和开环控制过渡的遥操作在整块肿瘤切除方面表现出巨大的潜力。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10696949</guid>
      <pubDate>Thu, 26 Sep 2024 13:16:56 GMT</pubDate>
    </item>
    <item>
      <title>工业机器人工作单元中人机协作的多摄像机手眼校准</title>
      <link>http://ieeexplore.ieee.org/document/10694716</link>
      <description><![CDATA[在工业场景中，尽管机器人工作单元中通常会出现遮挡，但有效的人机协作依赖于多摄像头系统来稳健地监控人类操作员。在这种情况下，在机器人坐标系中精确定位人员至关重要，因此摄像头网络的手眼校准至关重要。当需要在短时间内实现高校准精度以最大限度地减少生产停机时间，以及处理用于监控广阔区域的广泛摄像头网络（例如工业机器人工作单元）时，此过程会带来重大挑战。我们的论文介绍了一种创新且强大的多摄像头手眼校准方法，旨在优化每个摄像头相对于机器人底座和每个摄像头的姿势。这种优化集成了两种类型的关键约束：i）单个板到末端执行器的转换，以及 ii）相对的摄像头到摄像头的转换。我们通过使用 METRIC 数据集和在工业场景中收集的真实数据进行全面实验，证明了我们方法的卓越性能，即使使用不到 10 张图像，也显示出比最先进的技术有显着的进步。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10694716</guid>
      <pubDate>Wed, 25 Sep 2024 13:17:20 GMT</pubDate>
    </item>
    <item>
      <title>面向通用装配状态识别的监督表征学习</title>
      <link>http://ieeexplore.ieee.org/document/10694722</link>
      <description><![CDATA[装配状态识别有助于执行装配程序，提供反馈以提高效率并最大限度地减少错误。然而，识别装配状态在可扩展性方面带来了挑战，因为零件经常更新，而且对执行错误的鲁棒性仍未得到充分探索。为了应对这些挑战，本信提出了一种基于表示学习和新颖的中间状态知情损失函数修改 (ISIL) 的方法。ISIL 利用状态之间的未标记转换，并展示了所有测试架构和损失的聚类和分类性能的显著改进。尽管我们只对没有执行错误的图像进行训练，但对错误状态的彻底分析表明，我们的方法可以准确区分正确状态和具有各种类型执行错误的状态。所提出的算法的集成可以为工人提供有意义的帮助，并减轻工业环境中因程序失误造成的意外损失。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10694722</guid>
      <pubDate>Wed, 25 Sep 2024 13:17:20 GMT</pubDate>
    </item>
    <item>
      <title>用于多个磁控微型机器人独立控制的双层四象限正交微线圈平台设计</title>
      <link>http://ieeexplore.ieee.org/document/10694697</link>
      <description><![CDATA[与仅依赖全局磁场响应的局限性相比，利用局部磁场可以独立控制多个微型机器人，而不管它们的尺寸和方向如何变化。在本文中，我们开发了一种双层正交局部磁场生成系统，将工作场所划分为四个象限并在它们之间设置过渡微线圈。设计的系统可以同时运动多个微型机器人，允许单个微型机器人跨象限运动或多个微型机器人独立运动。这项创新以精心制作的微线圈模型的开发和验证为基础，该模型分析了输入电流的影响并研究了其工作空间内的磁场分布。在此基础上，我们提出了一种利用微线圈平台的微型机器人创新驱动和驱动策略。此外，还为各种激活状态下的微线圈开发了磁场分析模型，证实了所提出的驱动策略的有效性。最后，我们通过实验平台验证了微线圈驱动系统，突出了新设计的稳健性和效率。实验结果表明，该方法对于微操作任务具有巨大的潜力。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10694697</guid>
      <pubDate>Wed, 25 Sep 2024 13:17:20 GMT</pubDate>
    </item>
    <item>
      <title>通过自适应保形预测在动态代理之间实现安全的 POMDP 在线规划</title>
      <link>http://ieeexplore.ieee.org/document/10694720</link>
      <description><![CDATA[部分可观察马尔可夫决策过程 (POMDP) 的在线规划为机器人在不确定情况下的决策提供了有效的技术。然而，现有的方法不足以防止动态环境中的安全违规行为。本信介绍了一种新颖的安全 POMDP 在线规划方法，该方法可在由多个动态代理填充的环境中提供概率安全保障，同时最大化预期回报。我们的方法利用动态代理的数据驱动轨迹预测模型，并应用自适应共形预测 (ACP) 来量化这些预测中的不确定性。利用获得的基于 ACP 的轨迹预测，我们的方法可以动态构建安全防护罩，以防止 POMDP 在线规划中的不安全行为。通过使用现实世界的行人轨迹数据在各种动态环境中进行实验评估，所提出的方法已被证明可以有效地保持概率安全保障，同时容纳多达数百个动态代理。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10694720</guid>
      <pubDate>Wed, 25 Sep 2024 13:17:20 GMT</pubDate>
    </item>
    <item>
      <title>MA-Stereo：通过多尺度注意力融合和空间误差感知细化实现实时立体匹配</title>
      <link>http://ieeexplore.ieee.org/document/10694714</link>
      <description><![CDATA[立体匹配是计算机视觉中的一项基本任务。实时立体匹配最近在机器人和自动驾驶应用中显示出巨大的潜力。然而，实时立体匹配中现有的成本聚合在不适定区域中受到精度限制。此外，大多数实时立体匹配方法难以预测物体细节和边缘区域的视差，导致视差图相对模糊且缺乏详细性。为了解决这些问题，我们提出了一种称为 MA-Stereo 的实时立体匹配架构，它具有多尺度注意力融合 (MAF) 模块和基于注意力的空间错误感知细化 (ASER) 模块。MAF 通过注意力机制自适应地融合上下文和几何信息，有效地改善了成本聚合。此外，ASER 细化了预测的视差图，充分利用高频信息和空间证据来准确预测尖锐边缘和薄结构的视差。 SceneFlow 和 KITTI 基准上的实验结果表明，MA-Stereo 优于几乎所有当前最先进的实时立体匹配方法，同时保持相对较低的运行时间，在准确性和速度之间实现了良好的平衡。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10694714</guid>
      <pubDate>Wed, 25 Sep 2024 13:17:20 GMT</pubDate>
    </item>
    <item>
      <title>不确定机器人的安全稳健规划：闭环状态灵敏度方法</title>
      <link>http://ieeexplore.ieee.org/document/10693498</link>
      <description><![CDATA[在本信中，我们详细介绍了在存在模型不确定性的情况下对机器人进行安全稳健规划的综合框架。我们的框架基于最近的闭环状态灵敏度概念，在本研究中，该概念得到了扩展，还包括初始状态中的不确定性。所提出的框架考虑了标称闭环系统相对于模型参数和初始状态不匹配的灵敏度，用于计算准确捕捉所考虑不确定性的最坏情况影响的管道。与目前最先进的安全和稳健规划相比，所提出的闭环状态灵敏度框架具有计算简单和对底层机器人闭环动力学的假设（和简化）最少的重要优势。该方法通过广泛的模拟和现实世界的实验得到验证。在实验中，我们将非线性轨迹优化问题作为案例研究，旨在为空中机器人生成本质上稳健和安全的轨迹，以便在存在不确定性的情况下安全地执行避障操作。模拟和实验结果进一步证实了所提出方法的可行性和趣味性。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10693498</guid>
      <pubDate>Wed, 25 Sep 2024 13:17:20 GMT</pubDate>
    </item>
    <item>
      <title>根据运动和深点对应关系自动校准无目标相机激光雷达</title>
      <link>http://ieeexplore.ieee.org/document/10694691</link>
      <description><![CDATA[机器人平台的传感器设置通常包括摄像头和激光雷达，因为它们提供互补信息。然而，融合这两种模式通常需要在它们之间进行高精度校准。在这封信中，我们提出了 MDPCalib，这是一种新颖的摄像头-激光雷达校准方法，既不需要人工监督，也不需要任何特定的目标物体。相反，我们利用视觉和激光雷达里程计的传感器运动估计以及基于深度学习的 2D 像素到 3D 点的对应关系，这些对应关系无需域内再训练即可获得。我们将摄像头-激光雷达校准表示为优化问题，并尽量减少传感器运动和点对应约束引起的成本。在大量实验中，我们证明了我们的方法可以产生高精度的外部校准参数，并且对随机初始化具有鲁棒性。此外，我们的方法可以推广到各种传感器设置，我们通过将其应用于各种机器人平台（包括自动驾驶感知汽车、四足机器人和无人机）来证明这一点。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10694691</guid>
      <pubDate>Wed, 25 Sep 2024 13:17:20 GMT</pubDate>
    </item>
    <item>
      <title>使用混合整数线性规划的刚体路径规划</title>
      <link>http://ieeexplore.ieee.org/document/10694695</link>
      <description><![CDATA[在拥挤的环境中引导刚体物体可能具有挑战性，尤其是在狭窄的通道中。现有的基于采样的规划器和基于优化的方法（如混合整数线性规划 (MILP) 公式）在工作空间大小或障碍物数量方面都存在有限的可扩展性。为了解决可扩展性问题，我们提出了一种三阶段算法，该算法首先在没有碰撞的工作空间中生成凸多面体的图，然后提出一大组小型 MILP 来生成多面体之间的可行路径，最后在线查询一对起始和结束配置以获得可行路径。凸多面体的图用作自由工作空间的分解，每个 MILP 中的决策变量数量通过将子问题限制在两个或三个自由多面体而不是整个自由区域内来限制。我们的模拟结果表明，与基线方法相比，在线计算时间更短，并且与基于采样的规划器相比，在 2D 和 3D 环境中，随着环境大小和隧道宽度的扩展，其扩展性更好。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10694695</guid>
      <pubDate>Wed, 25 Sep 2024 13:17:20 GMT</pubDate>
    </item>
    <item>
      <title>LCD-RIG：有限通信分散机器人信息收集系统</title>
      <link>http://ieeexplore.ieee.org/document/10693516</link>
      <description><![CDATA[协作信息收集系统中的有效数据收集在很大程度上依赖于保持不间断的连接。然而，现实世界的通信中断通常会对信息收集过程构成挑战。为了解决这个问题，我们引入了一种新方法——一种有限通信分散式信息收集系统，供多个机器人探索以未知空间场为特征的环境现象。我们的方法利用四叉树结构来确保全面的工作空间覆盖和高效的探索。与依赖全局和同步通信的传统系统不同，我们的方法使机器人能够在有限的传输范围内共享本地经验，并通过成对和异步通信协调它们的任务。信息估计由具有注意力内核的高斯过程促进，允许自适应地捕获关键行为和数据模式。我们提出的系统通过模拟标量场研究在多个机器人探索空间场的非平稳环境中得到验证。理论保证确保分布式区域覆盖的收敛和分布式在线标量场映射的遗憾界限。我们还在水质监测场景中通过经验验证了我们的方法，该场景以三艘自主水面航行器为特色，负责构建一个空间场。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10693516</guid>
      <pubDate>Wed, 25 Sep 2024 13:17:20 GMT</pubDate>
    </item>
    <item>
      <title>针对退化场景使用基于块的梯度优化的点线 LIVO</title>
      <link>http://ieeexplore.ieee.org/document/10688407</link>
      <description><![CDATA[基于 3-D 光检测和测距 (LiDAR) 的同步定位和建图在无结构环境中往往会退化，导致定位精度和建图精度明显降低。本文提出了一种基于 FAST-LIVO 系统实现的点线 LiDAR-视觉惯性里程计 (PL-LIVO)，用于在 LiDAR 退化场景中实现稳健的定位。关键思想是将点和线集成到所提出的直接视觉里程计子系统 (PL-DVO) 中。通过最小化基于块的梯度残差进行状态优化，PL-DVO 提供了与 LiDAR 互补的额外约束。此外，提出了一种 LiDAR 地图辅助视觉特征深度提取 (LM-VDE) 方法，通过将视觉特征映射到 LiDAR 地图的 3-D 平面上来恢复视觉特征的 3-D 位置。该方法与单次扫描的密度无关，并且在各种 LiDAR 传感器上具有出色的泛化能力。在公共数据集和我们的数据集上进行的大量实验表明，PL-LIVO 可确保稳健的定位，并且在 LiDAR 退化场景中优于其他最先进的系统。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10688407</guid>
      <pubDate>Mon, 23 Sep 2024 13:17:08 GMT</pubDate>
    </item>
    <item>
      <title>SurgEM：基于视觉的手术环境建模框架，用于构建数字孪生以实现自主软组织操作</title>
      <link>http://ieeexplore.ieee.org/document/10685073</link>
      <description><![CDATA[机器人手术中的自主软组织操作仍然具有挑战性。对手术操作过程中的工具-组织相互作用进行建模、分析组织结构变形和监测生物力学状态可能有利于自主手术的发展；然而，目前研究不足。我们提出了一种基于视觉的手术环境建模框架，利用基于模型的姿势估计和基于场景流的网格优化，同时重建和跟踪钳子和组织。我们还提出了一种基于该框架的数字孪生，用于连续建模工具-组织相互作用并监测组织表面的变形和应变。进行了定量和定性的体外实验，从各个角度评估所提出的系统。结果表明，变形恢复精度为 $\bm {0.38\pm 0.30}$ 毫米，对遮挡具有鲁棒性；仪器姿势估计精度在旋转中为 $\bm {0.85\pm 0.57}$ 度，在平移中为 $\bm {2.09\pm 1.41}$ 毫米。在接触检测方面，可以正确建模组织和钳子之间的相对定位。该系统还正确地揭示了两种工具-组织相互作用（触诊和牵引）中应变分布的差异。利用所提出的系统，未来可以开发具有组织变形感知能力的手术机器人系统，以实现优化和自主手术。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10685073</guid>
      <pubDate>Fri, 20 Sep 2024 13:16:32 GMT</pubDate>
    </item>
    <item>
      <title>GISR：单视图机器人姿势和配置估计的几何初始化和基于轮廓的细化</title>
      <link>http://ieeexplore.ieee.org/document/10685077</link>
      <description><![CDATA[在自主机器人技术中，测量机器人的内部状态和对环境的感知（包括与其他代理（如协作机器人）的交互）至关重要。从单一视图估计机器人手臂的姿势有可能取代传统的眼手校准方法，并且对于在线估计和动态环境特别有吸引力。除了姿势之外，恢复机器人配置还可以提供对观察到的机器人的完整空间理解，可用于预测高级机器人用例中其他代理的动作。此外，这种额外的冗余使得在传感器故障或外部干扰的情况下可以规划和执行恢复协议。我们引入了 GISR - 一种深度配置和机器人到相机姿势估计方法，优先考虑实时执行。GISR 由两个模块组成：(i) 一个几何初始化模块，可有效计算近似的机器人姿势和配置，以及 (ii) 一个深度迭代轮廓细化模块，只需几次迭代即可得出最终解决方案。我们在公开数据上对 GISR 进行了评估，结果表明它在速度和准确性方面均优于现有的同类方法，并且可以与依赖于真实本体感受并仅恢复姿势的方法相媲美。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10685077</guid>
      <pubDate>Fri, 20 Sep 2024 13:16:32 GMT</pubDate>
    </item>
    <item>
      <title>用于近距离人体网格恢复的多模式主动测量</title>
      <link>http://ieeexplore.ieee.org/document/10685076</link>
      <description><![CDATA[对于物理人机交互 (pHRI)，机器人需要估计目标人的准确身体姿势。然而，在这些 pHRI 场景中，机器人无法使用配备的摄像头完全观察目标人的身体，因为目标人必须靠近机器人才能进行物理交互。这种近距离会导致严重的截断和遮挡，从而导致人体姿势估计的准确性较差。为了在这种具有挑战性的环境中获得更高的准确性，我们提出了配备触摸和测距传感器（如 2D LiDAR）的摄像头的主动测量和传感器融合框架。触摸和测距传感器测量是稀疏但可靠且有用的线索，可用于定位人体部位。在我们的主动测量过程中，相机视点和传感器位置经过动态优化，以测量具有更高估计不确定性的身体部位，这与截断或遮挡密切相关。在我们的传感器融合过程中，假设触摸和测距传感器的测量比基于相机的估计更可靠，我们通过将估计的姿势与测量点对齐，将传感器测量融合到基于相机的估计姿势中。我们提出的方法在模拟主动测量的标准遮挡基准上优于以前的方法。此外，即使存在诸如被毯子遮挡等实际限制，我们的方法也能使用真实机器人可靠地估计人体姿势。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10685076</guid>
      <pubDate>Fri, 20 Sep 2024 13:16:32 GMT</pubDate>
    </item>
    </channel>
</rss>