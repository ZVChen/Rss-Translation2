<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>IEEE 机器人与自动化快报 - 新目录</title>
    <link>http://ieeexplore.ieee.org</link>
    <description>出版物 TOC 警报# 7083369</description>
    <lastBuildDate>Thu, 11 Apr 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>MorAL：在具有挑战性的地形上为四足机器人学习形态自适应运动控制器</title>
      <link>http://ieeexplore.ieee.org/document/10463132</link>
      <description><![CDATA[由于近十年来四足机器人产业的快速发展，各种具有鲜明物理属性的商用四足机器人应运而生。与之前设计的控制器是针对机器人的工作不同，本文提出了一种基于学习的控制框架——MorAL，它能够适应四足机器人的不同形态和具有挑战性的地形。我们的框架同时训练控制策略和自适应模块，该模块考虑时间机器人状态。该模块使控制策略能够隐式在线识别不同机器人平台的属性并估计车身速度。现实世界和模拟中的大量实验表明，我们的控制器能够使具有显着不同形态的机器人克服各种室内和室外恶劣地形。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10463132</guid>
      <pubDate>Tue, 12 Mar 2024 13:16:33 GMT</pubDate>
    </item>
    <item>
      <title>无需特定任务知识的自主强化学习的自监督课程生成</title>
      <link>http://ieeexplore.ieee.org/document/10465617</link>
      <description><![CDATA[将当前强化学习算法应用于现实场景的一个重要瓶颈是需要在每个情节之间重置环境。这个重置过程需要大量的人为干预，使得代理很难持续、自主地学习。最近的几项工作引入了自主强化学习（ARL）算法，该算法生成用于联合训练重置和转发策略的课程。虽然他们的课程可以通过考虑代理的学习进度来减少所需的手动重置次数，但他们依赖于特定于任务的知识，例如预定义的初始状态或重置奖励函数。在本文中，我们提出了一种新颖的 ARL 算法，该算法可以生成适应代理学习进度的课程，而无需特定于任务的知识。我们的课程使智能体能够自主重置为多样化且信息丰富的初始状态。为了实现这一目标，我们引入了一个成功判别器，用于估计代理遵循前向策略时每个初始状态的成功概率。成功判别器以自我监督的方式通过重新标记的转换进行训练。我们的实验结果表明，我们的 ARL 算法可以生成自适应课程，并使代理能够有效地引导解决稀疏奖励迷宫导航和操作任务，在手动重置显着减少的情况下优于基线。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10465617</guid>
      <pubDate>Mon, 11 Mar 2024 13:18:32 GMT</pubDate>
    </item>
    <item>
      <title>使用预先训练的视觉语言模型和黑盒优化对烹饪机器人进行连续物体状态识别</title>
      <link>http://ieeexplore.ieee.org/document/10465607</link>
      <description><![CDATA[机器人对环境和物体的状态识别一般是以对当前状态的判断为分类问题。另一方面，烹饪过程中食物的状态变化是连续发生的，不仅需要在某个时间点捕获，还需要随着时间的推移连续捕获。另外，食物的状态变化比较复杂，无法通过手动编程轻松描述。因此，我们提出了一种使用预先训练的大规模视觉语言模型通过口语来识别烹饪机器人食物的连续状态变化的方法。通过使用可以随时间连续计算图像和文本之间相似度的模型，我们可以捕获食物在烹饪时的状态变化。我们还表明，通过将相似度变化拟合到 sigmoid 函数来调整每个文本提示的权重，然后进行黑盒优化，可以实现更准确和鲁棒的连续状态识别。我们通过对水沸腾、黄油融化、煮鸡蛋和洋葱炒的识别来证明该方法的有效性和局限性。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10465607</guid>
      <pubDate>Mon, 11 Mar 2024 13:18:32 GMT</pubDate>
    </item>
    <item>
      <title>Buffalo Byte：高度移动和自主的 Millirobot 平台</title>
      <link>http://ieeexplore.ieee.org/document/10465605</link>
      <description><![CDATA[Buffalo Byte 是一个小型、高度移动和自主的毫机器人平台，可以在不平坦的自然地形上移动。每个 Buffalo Byte 都有两个硅胶胎面，带有可增加牵引力的柔顺脊柱。硅胶踏板由小型直流电机独立驱动。单个机器人的速度可达 $235.7 \,\mathrm{m}\mathrm{m}\mathrm{/}\mathrm{s}$，并可在崎岖的地形上行驶，倾斜度可达 $12.5\deg$。每个 Buffalo Byte 都有一个板载 ESP8266EX 片上系统 (SoC)，用于通过 Wi-Fi 进行计算和通信。一个完全自主的 Buffalo Byte 携带自己的电源，质量为 11.35 美元 \,\mathrm{g}$，体积足迹为 28.5 美元 \,\mathrm{m}\mathrm{m}$ × $29.5 \,\mathrm{m }\mathrm{m}$× $21.4 \,\mathrm{m}\mathrm{m}$ 平均运行时间为 30 分钟。 Buffalo Byte 甚至能够利用板载惯性测量单元 (IMU) 的反馈在崎岖地形上进行闭环控制。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10465605</guid>
      <pubDate>Mon, 11 Mar 2024 13:18:32 GMT</pubDate>
    </item>
    <item>
      <title>全景分布外分割</title>
      <link>http://ieeexplore.ieee.org/document/10463091</link>
      <description><![CDATA[深度学习在场景理解方面取得了显着的进步，全景分割成为一项关键的整体场景解释任务。然而，在存在分布外（OOD）对象（即偏离训练分布的对象类别）的情况下，全景分割的性能会受到严重影响。为了克服这一限制，我们提出了全景分布外分割，用于联合像素级语义分布内和分布外分类与实例预测。我们使用分布外实例分割注释扩展了两个已建立的全景分割基准 Cityscapes 和 BDD100 K，提出了合适的评估指标，并提出了多个强大的基线。重要的是，我们提出了一种新颖的 PoDS 架构，该架构具有共享主干、用于学习全局和局部 OOD 对象线索的 OOD 上下文模块，以及具有特定任务头的双对称解码器，该解码器采用我们的对齐不匹配策略来实现更好的 OOD 泛化。与我们的数据增强策略相结合，这种方法有助于渐进式学习分布外对象，同时保持分布内性能。我们进行了广泛的评估，证明我们提出的 PoDS 网络有效地解决了主要挑战，并且大大优于基线。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10463091</guid>
      <pubDate>Fri, 08 Mar 2024 13:16:55 GMT</pubDate>
    </item>
    <item>
      <title>用于动力假肢端到端控制的深度学习框架</title>
      <link>http://ieeexplore.ieee.org/document/10460999</link>
      <description><![CDATA[深度学习为主动下肢假肢的手动调节控制提供了一种潜在的强大替代方案，能够产生连续的端到端联合级辅助。通过将整个控制问题分解为深度神经网络，消除了对传统任务分类、状态机和中级控制方程的需要。在这封信中，从开源驱动的膝踝假体 (OSL) 收集了五种运动模式的传感器数据和常规命令扭矩：平地、斜坡上/下和楼梯上/下。使用专家调整的基于有限状态机的阻抗控制器为每种模式和经股截肢者参与者生成参考命令扭矩（N = 12）。然后使用时间卷积网络 (TCN) 估计输出的站立阶段，该网络产生独立于模式和用户的膝盖和脚踝扭矩，RMSE 分别为 0.154 ± 0.06 和 0.106 ± 0.06 Nm/kg。使用特定模式数据训练模型仅显着减少楼梯下降，将膝盖和脚踝 RMSE 分别降低 0.06 ± 0.028 和 0.033 ± 0.008 Nm/kg (p &lt; 0.05)。此外，TCN 还适应步行速度和坡度变化的参考命令扭矩。这些结果表明，这种深度学习模型不仅消除了对启发式状态机和模式分类的需要，而且还可以完全减少或消除对假肢辅助调整的需要。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10460999</guid>
      <pubDate>Wed, 06 Mar 2024 13:16:59 GMT</pubDate>
    </item>
    <item>
      <title>一种用于鲁棒协作感知的空间校准方法</title>
      <link>http://ieeexplore.ieee.org/document/10461035</link>
      <description><![CDATA[协作感知是通过车辆与万物 (V2X) 协作实现智能网联车辆的一项有前景的技术，前提是提供准确的姿态信息和相对姿态变换。然而，获得精确的定位信息通常需要与导航系统相关的高成本。因此，需要校准多智能体协作感知的相对姿态信息。这封信提出了一种简单但有效的对象关联方法，称为基于上下文的匹配（$\mathtt{CBM}$），该方法使用代理内的几何上下文来识别代理间的对象对应关系。具体来说，该方法使用检测到的边界框的相对位置构建上下文，然后进行局部上下文匹配和全局共识最大化。根据匹配的对应关系估计最佳相对姿态变换，然后进行协作感知融合。在模拟和现实数据集上进行了大量的实验。即使存在较大的智能体间定位误差，协作智能体之间也能实现较高的对象关联精度和分米级相对位姿校准精度。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10461035</guid>
      <pubDate>Wed, 06 Mar 2024 13:16:59 GMT</pubDate>
    </item>
    <item>
      <title>用一只手抓住多个物体</title>
      <link>http://ieeexplore.ieee.org/document/10460998</link>
      <description><![CDATA[人手复杂的运动学使得能够同时抓取和操纵多个物体，这对于物体转移和手动操纵等任务至关重要。尽管具有重要意义，但机器人多物体抓取领域相对尚未被探索，并且在运动学、动力学和物体配置方面提出了显着的挑战。这封信介绍了 MultiGrasp，这是一种新颖的两阶段方法，使用桌面上灵巧的多指机器人手来抓取多物体。该过程包括 (i) 生成预抓取建议和 (ii) 执行抓取并举起物体。我们的实验重点主要是双物体抓取，成功率达到 44.13%，突出了对新物体配置的适应性和对不精确抓取的容忍度。此外，该框架还展示了以推理速度为代价来抓取两个以上物体的潜力。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10460998</guid>
      <pubDate>Wed, 06 Mar 2024 13:16:59 GMT</pubDate>
    </item>
    <item>
      <title>FusedNet：动态大规模场景中的端到端移动机器人重定位</title>
      <link>http://ieeexplore.ieee.org/document/10457947</link>
      <description><![CDATA[为了提高静态和动态环境中的机器人重定位精度，我们引入了一种新颖的网络 FusedNet，它结合了交叉注意力来融合全局和局部图像特征以实现端到端重定位。这种方法仅依赖于固定在移动机器人上的单目相机传感器，并直接根据输入的 RGB 图像预测绝对姿态。此外，我们还使用 Unity 3D 仿真平台和真实的移动机器人收集了一个移动机器人重定位数据集，称为 moBotReloc，由动态大型场景组成。通过对 7Scenes 和 moBotReloc 的大量实验，我们证明 FusedNet 在静态场景中的 6-DoF 相机重定位中实现了显着的精度，并且在移动机器人应用的动态大规模场景中表现出卓越的重定位性能，优于现有的端到端方法仅依赖于单个全局或局部特征。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10457947</guid>
      <pubDate>Mon, 04 Mar 2024 13:17:35 GMT</pubDate>
    </item>
    <item>
      <title>Ajisai：一种将力和颜色之间的关系与人类感知相匹配的结构</title>
      <link>http://ieeexplore.ieee.org/document/10458288</link>
      <description><![CDATA[人类发现很难通过视觉感知由位置控制操作的机器人施加在物体上的力的大小。根据研究，人类感知颜色的权重顺序为黄、绿、蓝、红，在色相环上按顺时针方向排列。我们相信，通过开发一种按这种特定顺序改变颜色的结构，我们可以实现与人类感知的力和颜色之间的关系相一致的视觉力传递。在这封信中，我们利用了各向异性透明材料的光弹性，当夹在两个偏光板之间时，该材料会根据施加的力而改变颜色。该材料呈哑铃形，可确保施加的力均衡。基于此，我们开发了一种名为 Ajisai 的结构，它由人造肌肉提供动力，其颜色会根据施加的力而变化，以匹配人类的感知。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10458288</guid>
      <pubDate>Mon, 04 Mar 2024 13:17:34 GMT</pubDate>
    </item>
    <item>
      <title>从裁剪图像块中学习车辆动力学，用于未铺砌的室外地形中的机器人导航</title>
      <link>http://ieeexplore.ieee.org/document/10453657</link>
      <description><![CDATA[在自主移动机器人领域，在未铺砌的室外环境中安全导航仍然是一项具有挑战性的任务。由于传感器数据的高维性质，提取相关信息成为一个复杂的问题，这阻碍了充分的感知和路径规划。之前的工作在从全尺寸图像中提取全局特征方面表现出了良好的性能。然而，他们在获取重要的本地信息方面经常面临挑战。在这封信中，我们提出了 Crop-LSTM，它迭代地获取当前机器人位置周围的裁剪图像块，并预测未来的位置、方向和凹凸程度。我们的方法通过关注 2D 图像平面中预测的机器人轨迹上的相应图像块来执行局部特征提取。这使得能够更准确地预测机器人未来的轨迹。通过我们的轮式移动机器人平台 Raicart，我们展示了 Crop-LSTM 在未铺砌的室外环境中进行点目标导航的有效性。我们的方法可以在具有挑战性的未铺砌的室外地形中使用 RGBD 图像实现安全、稳健的导航。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10453657</guid>
      <pubDate>Thu, 29 Feb 2024 13:16:47 GMT</pubDate>
    </item>
    <item>
      <title>Dynamic Duo：自主额状和矢状驱动髋部外骨骼的设计和验证，用于扰动运动期间的平衡调节</title>
      <link>http://ieeexplore.ieee.org/document/10452751</link>
      <description><![CDATA[人类在充满挑战的环境中运动时需要保持平衡，这对有平衡障碍的人来说是一个更大的挑战。外骨骼驱动的平衡增强是在这些环境中为用户提供帮助的一种有前景的途径，但在为这些应用开发外骨骼设备方面几乎没有开展任何工作。在这项工作中，我们展示了具有正面和矢状驱动的自主机器人髋部外骨骼的设计、实现和验证。该装置包含四个准直接驱动驱动的自由度，可在髋关节处提供正面和矢状辅助。该设备重量相对较轻，重 7.76 千克，额面位置较低，使用户的手臂摆动不受阻碍。我们发现佩戴该设备不会改变步行运动学，验证该设计不会抑制用户的自然运动。我们使用双边 bang-bang 控制器验证了外骨骼，该控制器在稳态和扰动行走期间成功调节了所有基本方向和序数方向的步宽和长度（所有 p&lt;0.001）。我们还发现步进调制能力受到摆腿运动学和扰动环境的影响。从广义上讲，这项工作提出了一种轻型、自主、动力外骨骼，可用于研究增强平衡的控制方法。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10452751</guid>
      <pubDate>Wed, 28 Feb 2024 13:17:21 GMT</pubDate>
    </item>
    <item>
      <title>Text2Reaction：使用大型语言模型启用反应式任务规划</title>
      <link>http://ieeexplore.ieee.org/document/10452794</link>
      <description><![CDATA[为了在动态环境中完成任务，机器人需要及时更新计划以应对环境变化。传统的条带式或基于学习的规划器很难实现这一目标，因为它们高度依赖精心预定义的规划规则或标记数据。幸运的是，最近的工作发现大型语言模型（LLM）可以有效地促进解决规划问题。因此，我们研究了法学硕士在无需复杂定义和额外培训的情况下掌握反应性规划问题的策略。我们提出了Text2Reaction，一个基于LLM的框架，使机器人能够根据最新的环境变化不断推理和更新计划。受人类逐步重新规划过程的启发，我们提出了重新规划提示，它向法学硕士告知重新规划的基本原则，并通过三跳推理促进当前计划逐步发展为新计划方式原因分析、结果推断和计划调整。此外，Text2Reaction的设计是在执行之前首先根据任务描述生成一个初始计划，以便后续对该计划进行迭代更新。我们展示了 Text2Reaction 在响应各种环境变化和完成各种任务方面比之前的作品具有优越的性能。此外，我们通过消融实验验证了重新规划提示的可靠性及其部署在现实世界机器人中的能力，从而能够在面对各种变化时进行持续推理，直到成功完成用户指令。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10452794</guid>
      <pubDate>Wed, 28 Feb 2024 13:17:21 GMT</pubDate>
    </item>
    <item>
      <title>用于分散式多机器人目标分配的图神经网络</title>
      <link>http://ieeexplore.ieee.org/document/10452797</link>
      <description><![CDATA[为机器人团队分配一组空间目标的问题在各种多机器人规划应用中起着至关重要的作用，包括但不限于探索、搜索和救援或监视。线性和分配问题 (LSAP) 是表述和解决该问题的常用方法。该优化问题旨在将任务分配给机器人，在尊重一对一匹配约束的同时最小化成本总和。然而，现实场景中的通信限制带来了重大挑战。现有的去中心化解决方案通常依赖于大量的通信交互来收敛到无冲突的最优解决方案，或者假设先前的无冲突随机分配。在本文中，我们提出了一种新颖的用于多机器人目标分配的去中心化图神经网络方法（DGNN-GA）。我们利用异构图表示来建模机器人间通信以及目标和机器人之间的分配关系。我们在模拟中将其性能与其他去中心化的最先进方法进行比较。具体来说，我们的方法在严格限制的通信场景中优于流行的最先进方法，并且与其他两种算法相比，不依赖于任何初始无冲突猜测。最后，DGNN-GA 还在现实世界的实验中进行了部署和验证。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10452797</guid>
      <pubDate>Wed, 28 Feb 2024 13:17:21 GMT</pubDate>
    </item>
    <item>
      <title>实体特工能找到你的“猫形杯子”吗？基于法学硕士的零样本对象导航</title>
      <link>http://ieeexplore.ieee.org/document/10373065</link>
      <description><![CDATA[我们提出了语言引导探索（LGX），这是一种用于语言驱动的零样本对象目标导航（L-ZSON）的新颖算法，其中实体代理在以前未见过的环境中导航到唯一描述的目标对象。我们的方法利用大型语言模型 (LLM) 来完成此任务，利用 LLM 的常识推理功能来做出顺序导航决策。同时，我们使用预先训练的视觉语言基础模型执行广义目标对象检测。我们在 RoboTHOR 上实现了最先进的零射击对象导航结果，与 OWL-ViT CLIP on Wheels (OWL CoW) 的当前基线相比，成功率 (SR) 提高了 27% 以上。此外，我们研究了法学硕士在机器人导航中的使用，并对影响模型输出的各种提示策略进行了分析。最后，我们通过现实世界的实验展示了我们的方法的优点，这些实验表明 LGX 在检测和导航到视觉上独特的物体方面具有卓越的性能。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10373065</guid>
      <pubDate>Mon, 25 Dec 2023 13:23:38 GMT</pubDate>
    </item>
    </channel>
</rss>