<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>IEEE 机器人与自动化快报 - 新目录</title>
    <link>http://ieeexplore.ieee.org</link>
    <description>出版物目录提醒# 7083369</description>
    <lastBuildDate>Thu, 26 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>改进的多维滤波器组典型相关分析用于基于 SSVEP 的 BCI 识别</title>
      <link>http://ieeexplore.ieee.org/document/10803084</link>
      <description><![CDATA[本文介绍了一种改进的基于稳态视觉诱发电位（SSVEP）的脑机接口（BCI）系统的多维滤波器组典型相关分析（FBCCA）方法。这是一种基于FBCCA的免训练SSVEP识别方法，它集成了偏最小二乘回归（PLSR）和自适应多维扩展（AME）。与FBCCA相比，这种新方法可以通过最小化分布误差，进一步消除降维和回归过程中EEG信号中的噪声和伪影。此外，它更有效地利用了多通道EEG信号中的宝贵信息，从而提高了SSVEP的识别性能。在两个不同的开源数据集上进行的离线实验验证了该方法在不同注视时间上实现了比免训练方法更好的性能。在实时八目标BCI系统的在线测试中，该方法实现了98.44％的峰值准确率和45.68比特/分钟的信息传输速率（ITR）。该方法提高了免训练SSVEP识别的准确性和效率，有利于BCI系统在实际场景中的更广泛应用。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10803084</guid>
      <pubDate>Mon, 16 Dec 2024 13:19:52 GMT</pubDate>
    </item>
    <item>
      <title>用于机器人远程操作的具有并行神经网络的自适应用户界面</title>
      <link>http://ieeexplore.ieee.org/document/10803036</link>
      <description><![CDATA[近年来，人机交互 (HRI) 已成为一个日益重要的研究领域。人类在远程操作或轮流等 HRI 任务中的体验很大程度上取决于机器人与用户之间的界面设计。在任意 $M$ 维输入设备和 $N$ 自由度 (DOF) 机器人之间设计一个直观的用户界面 (UI) 仍然是一项重大挑战。本文提出了一种名为并行神经网络自适应用户界面 (PNNUI) 的新型 UI 设计方法。PNNUI 利用两个并行神经网络进行学习，然后通过最小化任务完成时间和最大化运动平滑度来提高用户的远程操作性能。我们的方法旨在通过在基于神经网络 (NN) 和遗传算法的离线无监督学习方案中最小化任务完成时间来学习用户界面硬件和机器人之间非直观的输入输出映射。其次，PNNUI 通过调整并行神经网络的权重在线最小化远程操作抖动。我们通过实验评估了通过带有三个输入的传统操纵杆远程操作 3 自由度非完整机器人所产生的 UI。20 名人类受试者在几种条件下沿着障碍赛道操作机器人。用户试验数据的统计分析表明，PNNUI 通过最大限度地提高流畅度并保持离线学习方案的完成时间，改善了机器人远程操作的人类体验。此外，我们公式的抽象性质使得性能指标可以自定义，从而将其适用性扩展到其他接口设备和 HRI 任务，特别是那些一开始并不直观的任务。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10803036</guid>
      <pubDate>Mon, 16 Dec 2024 13:19:52 GMT</pubDate>
    </item>
    <item>
      <title>事实：基于视觉的无人机群的快速主动坐标初始化</title>
      <link>http://ieeexplore.ieee.org/document/10803028</link>
      <description><![CDATA[坐标初始化是完成机器人群协作任务的第一步，决定了任务的质量。然而，在基于视觉的无人机群中，快速而稳健的坐标初始化仍然难以实现。为此，我们的信函提出了一个完整的初始相对姿态估计系统，包括相对状态估计和主动规划。具体来说，我们的工作将机载视觉惯性里程计与基于视觉的观测融合在一起，产生方位和距离测量，这些测量是匿名的、部分相互的和嘈杂的。这是第一种基于凸优化的方法，使用基于视觉的观测来初始化坐标。此外，我们设计了一个轻量级模块来主动控制机器人的运动，以获取观测值并避免碰撞。仅使用立体摄像机和惯性测量单元作为传感器，我们验证了我们的系统在模拟和有障碍物且没有全球导航卫星系统信号的现实世界区域中的实用性。与基于局部优化和过滤器的方法相比，我们的系统可以更稳定、更快速地实现坐标初始化的全局最优，适用于具有尺寸、重量和功率限制的机器人。源代码发布出来供参考。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10803028</guid>
      <pubDate>Mon, 16 Dec 2024 13:19:51 GMT</pubDate>
    </item>
    <item>
      <title>同伦感知高效自适应状态格用于杂乱环境下移动机器人运动规划</title>
      <link>http://ieeexplore.ieee.org/document/10803070</link>
      <description><![CDATA[采用规划算法来提供单一最佳路径的移动机器人导航架构在非结构化、快速变化的环境中存在缺陷。随着环境的更新，最佳计划通常会围绕离散障碍物震荡，这对于偏向遵循计划路线的路径跟随控制器来说是个问题。一种可能更好的方法是生成多个计划，每个计划在其自己的同伦类中都是最优的，以便为路径跟随控制器提供更全面的成本目标近似值。在本文中，我们提出了同伦感知高效自适应状态格 (HAEASL)，它使用多个开放列表将搜索偏向具有不同同伦类的路线。我们进行了实验，测量了在 80 个随机生成的环境中的 3,200 个规划问题中生成的解决方案的数量、最优性和多样性。HAEASL 的性能与之前的两种方法进行了对比：具有同伦类约束的基于搜索的路径规划 (A*HC) 和同伦感知 RRT* (HARRT*)。实验结果表明，HAEASL 可以生成比 A*HC 更多的路径和更多样化的路径，而不会显著降低最优性。此外，结果表明，HAEASL 可以生成比 HARRT* 更多的路径，并且路径成本更低。最后演示了 HAEASL 使用从越野移动机器人收集的数据生成受时间、资源和运动动力学约束的多个解决方案，说明了该方法适用于激励示例。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10803070</guid>
      <pubDate>Mon, 16 Dec 2024 13:19:51 GMT</pubDate>
    </item>
    <item>
      <title>LBSNet：针对透明和反射物体的轻量级联合边界检测和语义分割</title>
      <link>http://ieeexplore.ieee.org/document/10801210</link>
      <description><![CDATA[透明和反射物体的准确视觉检测仍然是移动机械手面临的挑战。对于最常见的深度相机和激光雷达传感器，透明和反射物体固有的独特光学属性构成了重大挑战。为了解决这个问题，本研究提出了一种轻量级的联合边界检测和语义分割网络，名为 LBSNet。LBSNet 旨在仅使用 RGB 图像来增强复杂和动态环境中透明和反射物体的感知。它通过特征融合和多任务学习机制利用边界检测和语义分割之间的协同作用。编码器由两条路径组成：一条捕获类别感知的语义信息，另一条从多尺度特征中辨别边界。门控通道自适应 (GCA) 模块通过学习通道参数来增强边界特征。动态自适应特征融合 (DAFF) 模块通过跨特征融合动态调整语义和边界信息。这些方法有效地捕捉了透明和反射物体的独特特征，例如光折射、边界模糊和低对比度。实验结果表明，LBSNet 在多个公开数据集上相比现有方法具有更高的准确率和更快的处理速度，且其轻量级的设计使其适用于资源受限的移动机械手。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10801210</guid>
      <pubDate>Fri, 13 Dec 2024 13:18:17 GMT</pubDate>
    </item>
    <item>
      <title>在不同身体耦合任务中单独和协作三手操作多肢的比较</title>
      <link>http://ieeexplore.ieee.org/document/10792937</link>
      <description><![CDATA[通过使用机器人的额外肢体，有人提出，单个用户可以执行目前需要团队合作才能完成的手术或工业装配等任务。尽管通常在虚拟现实中进行的验证研究已经表明，个人可以学会控制额外肢体，但比较结果通常表明，团队最初的表现优于操作额外肢体的个人。在这项研究中，我们研究了 (i) 使用市售的物理机器人设置而不是虚拟现实系统的影响，以及 (ii) 肢体耦合对用户在一系列三手操作过程中的表现的影响。与先前的研究结果相反，我们的结果表明，在作为三手用户工作时，在拾取和放置三个物体时，与作为团队工作时相比，用户的表现没有明显差异。此外，对于这项任务，我们观察到，虽然用户在控制大多数肢体时更喜欢与伙伴一起工作，但我们发现，他们在单独三手操作和与伙伴一起工作并控制第三肢体时的偏好没有明显差异。这些发现表明，视觉遮挡和触觉反馈等虚拟现实中通常不存在的因素对于多余肢体的有效操作可能是至关重要的，并为多余肢体在一系列身体任务中的可行性提供初步证据。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10792937</guid>
      <pubDate>Wed, 11 Dec 2024 13:19:15 GMT</pubDate>
    </item>
    <item>
      <title>双臂机器人协同运动控制的自适应噪声抑制策略</title>
      <link>http://ieeexplore.ieee.org/document/10783029</link>
      <description><![CDATA[双臂机器人具有良好的协作能力与多功能性，在各个领域展现出广阔的应用前景。作为双臂机器人的一个重要研究领域，对协调运动控制的要求也逐渐提高。在实际应用中，机器人不可避免地会受到噪声干扰，导致协调运动控制性能达不到最优。本文研究了谐波噪声下的双臂机器人协同运动控制。在相对雅可比方法的基础上，提出了一种自适应噪声抑制策略，用于受谐波噪声干扰的双臂机器人协同运动控制。该策略加入了补偿器，可以模拟和抑制谐波噪声的干扰。理论分析表明，所提策略产生的笛卡尔误差表现出收敛性。在两个Panda机器人机械手组成的双臂系统下的仿真和实验结果进一步验证了所提策略在谐波噪声存在下的抗噪性和适用性。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10783029</guid>
      <pubDate>Mon, 09 Dec 2024 13:21:16 GMT</pubDate>
    </item>
    <item>
      <title>使用带圆圈放置的约束优化进行摄像头-LiDAR 外部校准</title>
      <link>http://ieeexplore.ieee.org/document/10778314</link>
      <description><![CDATA[单目相机-激光雷达数据融合在各个领域都表现出了卓越的环境感知能力。数据融合的成功依赖于图像和点云中对应特征的准确匹配。在本文中，我们提出了一种基于目标的相机-激光雷达外部校准方法，通过匹配两种数据中的对应关系。具体来说，为了从点云中提取准确的特征，我们提出了一种新方法，通过优化初始位置的概率分布来估计圆心。该优化涉及从圆边缘点生成圆心的概率分布，并使用拉格朗日乘数法估计圆心的最佳位置。我们进行了两种类型的实验：定量结果的模拟和定性评估的真实系统评估。与现有方法相比，我们的方法在 20 个目标姿势的模拟校准性能上表现出了 $\mathbf{\text{0.03}\,m}$ 的提升，并且在将点云重新投影到真实场景中的图像上时也表现出较高的视觉质量。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10778314</guid>
      <pubDate>Thu, 05 Dec 2024 13:19:17 GMT</pubDate>
    </item>
    <item>
      <title>CusADi：用于符号表达式和最优控制的 GPU 并行化框架</title>
      <link>http://ieeexplore.ieee.org/document/10778410</link>
      <description><![CDATA[GPU 提供的并行性在通过强化学习 (RL) 训练控制器方面具有显著优势。然而，将基于模型的优化集成到此过程中仍然具有挑战性，因为在数千个实例中制定和解决优化问题的复杂性。在这项工作中，我们提出了 CusADi，这是 casadi 符号框架的扩展，用于支持使用 CUDA 在 GPU 上并行化任意闭式表达式。我们还制定了一个闭式近似来解决一般最优控制问题，从而实现 MPC 控制器的大规模并行化和评估。我们的结果显示，与 CPU 上的类似 MPC 实现相比，速度提高了十倍，并且我们展示了 CusADi 在各种应用中的使用，包括并行模拟、参数扫描和策略训练。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10778410</guid>
      <pubDate>Thu, 05 Dec 2024 13:18:54 GMT</pubDate>
    </item>
    <item>
      <title>CMIF-VIO：一种用于视觉惯性里程计的新型跨模态交互框架</title>
      <link>http://ieeexplore.ieee.org/document/10777572</link>
      <description><![CDATA[视觉惯性里程计 (VIO) 通过自身运动估计预测轨迹。随着人工智能的普及，基于深度学习的 VIO 方法表现出比传统基于几何的 VIO 方法更好的性能。然而，在深度学习方法中，如何更好地实现来自摄像机的视觉图像与来自 IMU 传感器的惯性测量单元 (IMU) 测量之间的融合和互补以输出准确的姿态仍然是一个挑战。在本文中，我们提出了一种新颖的 VIO 跨模态交互框架，称为 CMIF-VIO，它提高了 VIO 的准确性并具有良好的实时性。具体而言，我们首先使用现有的主干网络并构建一个简单的主干网络分别从摄像机和 IMU 中提取特征，确保低复杂度。然后，我们探索了一种自适应地集成来自不同模态特征的信息的跨模态交互模块，实现视觉和 IMU 模态特征之间的深度交互，同时保持每个模态分支中的特征主导地位。最后，引入长短期记忆（LSTM）网络来建模时间运动相关性并输出高精度六自由度（6-DOF）位姿。实验结果表明，我们的方法与最先进的VIO方法相比表现出更好的性能，其实时性可以满足实际应用场景的需求。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10777572</guid>
      <pubDate>Wed, 04 Dec 2024 13:19:19 GMT</pubDate>
    </item>
    <item>
      <title>使用基于视觉的管道实现人机软切换</title>
      <link>http://ieeexplore.ieee.org/document/10777566</link>
      <description><![CDATA[交接物体是人机协作场景中的一项重要任务。先前的研究主要使用刚性夹持器进行交接，重点是避免与人身体接触的抓握。在本文中，我们提出了一种基于视觉的张开手掌交接解决方案，其中软机械手利用与人手的接触来提高抓握成功率和稳健性。人机物理交互允许机械手在人手掌上滑动并牢固地抓住物体。通过利用单个 RGB-D 摄像头的多功能感知管道，可以识别人手平面和物体姿势。通过实验，我们表明该系统可以成功抓取具有不同几何形状和纹理的多个物体。比较分析评估了所提出的软交接方法与基线方法的稳健性。一项有 30 名参与者的研究评估了用户在交接过程中对人机交互的感知，突出了所提出的管道的有效性和偏好。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10777566</guid>
      <pubDate>Wed, 04 Dec 2024 13:19:19 GMT</pubDate>
    </item>
    <item>
      <title>ChatEMG：用于控制中风机械手矫形器的合成数据生成</title>
      <link>http://ieeexplore.ieee.org/document/10777608</link>
      <description><![CDATA[由于数据收集困难，对中风患者的手部矫形器进行意图推断具有挑战性。此外，EMG 信号在不同条件、会话和主题之间表现出显著差异，这使得分类器难以概括。传统方法需要来自新条件、会话或主题的大量标记数据集来训练意图分类器；然而，这个数据收集过程既繁琐又耗时。在这封信中，我们提出了 ChatEMG，这是一种自回归生成模型，可以生成以提示（即给定的 EMG 信号序列）为条件的合成 EMG 信号。ChatEMG 使我们能够从新条件、会话或主题中仅收集一小部分数据集，并使用以来自这一新上下文的提示为条件的合成样本对其进行扩展。ChatEMG 通过生成训练利用了大量以前的数据，同时仍然通过提示保持上下文特定性。我们的实验表明，这些合成样本与分类器无关，可以提高不同类型分类器的意图推断准确性。我们证明，我们的完整方法可以集成到单个患者会话中，包括使用分类器执行功能性矫形器辅助任务。据我们所知，这是第一次部署部分基于合成数据训练的意图分类器，供中风幸存者对矫形器进行功能控制。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10777608</guid>
      <pubDate>Wed, 04 Dec 2024 13:19:19 GMT</pubDate>
    </item>
    <item>
      <title>一种三指自适应夹持器，带有嵌入手指的吸盘，用于增强物体抓取机制</title>
      <link>http://ieeexplore.ieee.org/document/10777060</link>
      <description><![CDATA[随着物流自动化的发展，对先进夹持器的需求也日益增加。本研究提出了一种将吸盘集成到指尖的夹持器，以克服传统机器人抓取方法的局限性。夹持器采用 5 自由度结构设计，可以调整吸盘的角度，便于在各种环境中有效抓取。其自适应抓取机制通过使用指尖和远端指骨来夹住物体而无需手动控制，从而简化了控制。通过进行混合手指吸盘抓取以及传统手指和吸盘抓取来测试夹持器的多功能性。这些先进的抓取策略旨在提高物流自动化在处理各种物体时的灵活性和效率。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10777060</guid>
      <pubDate>Wed, 04 Dec 2024 13:19:19 GMT</pubDate>
    </item>
    <item>
      <title>FFBGNet：基于混合架构的全流双向特征融合抓握检测网络</title>
      <link>http://ieeexplore.ieee.org/document/10777534</link>
      <description><![CDATA[有效地整合来自 RGB-D 图像的互补信息对机器人抓取提出了重大挑战。在本文中，我们提出了一种基于混合架构的全流程双向特征融合抓取检测网络（FFBGNet），以从 RGB-D 图像生成准确的抓取姿势。首先，我们构建一个高效的跨模态特征融合模块作为两个分支全流程信息交互的桥梁，其中融合应用于每个编码和解码层。然后，两个分支可以充分利用 RGB 图像中的外观信息和深度图像中的几何信息。其次，开发了一种用于 CNN 和 Transformer 并行的混合架构模块，以实现更好的局部特征和全局信息表示。最后，我们在 Cornell 和 Jacquard 数据集上进行了定性和定量对比实验，分别实现了 99.2${\%}$ 和 96.5${\%}$ 的抓取检测准确率。同时，在物理抓取实验中，FFBGNet 在杂乱场景中实现了 96.7${\%}$ 的成功率，进一步证明了所提方法的可靠性。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10777534</guid>
      <pubDate>Wed, 04 Dec 2024 13:19:19 GMT</pubDate>
    </item>
    <item>
      <title>SemRaFiner：稀疏且嘈杂的雷达点云中的全景分割</title>
      <link>http://ieeexplore.ieee.org/document/10758203</link>
      <description><![CDATA[语义场景理解（包括对移动主体的感知和分类）对于实现自动驾驶汽车的安全和稳健驾驶行为至关重要。摄像头和激光雷达通常用于语义场景理解。然而，这两种传感器模式在恶劣天气下都面临局限性，通常不提供运动信息。雷达传感器克服了这些限制，通过测量多普勒速度直接提供有关移动主体的信息，但测量结果相对稀疏且嘈杂。在本信中，我们解决了稀疏雷达点云中的全景分割问题，以增强场景理解。我们的方法称为 SemRaFiner，它考虑了稀疏雷达点云中密度的变化并优化了特征提取以提高准确性。此外，我们提出了一种优化的训练程序，通过结合专用的数据增强来优化实例分配。我们的实验表明，我们的方法优于基于雷达的全景分割的最先进的方法。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10758203</guid>
      <pubDate>Tue, 19 Nov 2024 13:17:18 GMT</pubDate>
    </item>
    </channel>
</rss>