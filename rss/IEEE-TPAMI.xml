<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>IEEE 模式分析与机器智能学报 - 新目录</title>
    <link>http://ieeexplore.ieee.org</link>
    <description>出版物 # 34 的目录提醒</description>
    <lastBuildDate>Thu, 09 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>论独立媒体合集中故事的提炼及其叙事弧线的传递</title>
      <link>http://ieeexplore.ieee.org/document/10734853</link>
      <description><![CDATA[讲故事是人类的基本行为。这项工作引入了叙事信息的概念，我们将其定义为故事与组成故事的项目之间的信息空间重叠。使用对比学习方法，我们展示了如何利用现代人工神经网络来提炼故事并提取叙事信息的表示。然后，我们展示了进化算法如何利用这一点来提取一组叙事模板曲线，以及这些曲线与我们引入的新型曲线拟合算法相结合如何重新排序音乐专辑以自动在其中引入故事。通过这样做，我们给出了统计上显着的证据，表明 (1) 这些叙事信息模板曲线存在于现有专辑中，并且 (2) 人们更喜欢通过这些学习到的模板曲线之一排序的专辑，而不是随机排序的专辑。我们工作的前提扩展到任何形式的（很大程度上）独立媒体，作为证据，我们还表明我们的方法适用于图像数据。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10734853</guid>
      <pubDate>Thu, 24 Oct 2024 13:15:35 GMT</pubDate>
    </item>
    <item>
      <title>适用于恶劣环境的仿生智能生物神经元极化定向方法</title>
      <link>http://ieeexplore.ieee.org/document/10723758</link>
      <description><![CDATA[我们开发了一种智能创新定位方法，以提高偏振罗盘在恶劣条件下的定位精度：不利天气条件（如雾霾、沙尘暴）导致的天光偏振模式微弱，或遮挡（如建筑物、树木）导致天光偏振状态局部破坏。首先，采用线性偏振度阈值分析法判断天光偏振状态，构建仿生偏振增强传感模型，模拟食蚜蝇视觉神经通路中发现的增强感知机制，在黑暗或弱光环境下高效感知，仿生模型成功增强了从弱偏振模式中提取的信息内容。其次，采用卷积神经网络进行图像分割，去除天光偏振模式局部破坏条件下遮挡引起的偏振像素干扰，确定感兴趣的天空区域。最后，采用优化的自适应反对称环算法对图像分割后得到的不完整偏振角度图进行拟合。基于太阳子午线沿线强偏振角反对称性，分析稀疏不规则偏振像素提取信息，获得高精度偏振定向解。整个方法智能化实现模式分析和深度学习智能处理，高效旋转以管理偏振定向障碍。实验结果证明了所提方法在偏振条件恶化下补偿定向精度下降的性能、对干扰的鲁棒性以及对仿生偏振罗盘环境适应性的有益影响。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10723758</guid>
      <pubDate>Mon, 21 Oct 2024 13:15:30 GMT</pubDate>
    </item>
    <item>
      <title>VALOR：视觉-音频-语言全感知预训练模型和数据集</title>
      <link>http://ieeexplore.ieee.org/document/10721284</link>
      <description><![CDATA[在本文中，我们提出了用于多模态理解和生成的视觉-音频-语言全感知预训练模型 (VALOR)。与广泛研究的视觉-语言预训练模型不同，VALOR 以端到端的方式联合建模视觉、音频和语言之间的关系。它由三个用于单模态表示的独立编码器和一个用于多模态条件文本生成的解码器组成。我们设计了两个借口任务来预训练 VALOR 模型：多模态分组对齐 (MGA) 和多模态分组字幕 (MGC)。MGA 将视觉、语言和音频投射到同一个公共空间，同时构建视觉语言、音频语言和视听语言对齐。MGC 学习在视觉、音频或两者条件下生成文本标记。为了促进视觉-音频-语言预训练研究，我们构建了一个名为 VALOR-1M 的大规模高质量三模态数据集，其中包含 100 万个带有人工注释的视听字幕的可听视频。大量实验表明，VALOR 可以学习强大的多模态相关性，并推广到具有不同输入模态（例如视觉语言、音频语言和视听语言）的各种下游任务（例如检索、字幕和问答）。VALOR 在一系列公共跨模态基准上实现了新的最佳性能。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10721284</guid>
      <pubDate>Thu, 17 Oct 2024 13:16:39 GMT</pubDate>
    </item>
    <item>
      <title>迈向数据和知识驱动的人工智能：神经符号计算调查</title>
      <link>http://ieeexplore.ieee.org/document/10721277</link>
      <description><![CDATA[神经符号计算 (NeSy) 追求认知的符号和统计范式的整合，多年来一直是人工智能 (AI) 的一个活跃研究领域。由于 NeSy 有望协调符号表示的推理和可解释性优势以及神经网络中的鲁棒学习，它可能成为下一代人工智能的催化剂。在本文中，我们系统地概述了 NeSy 研究的最新发展和重要贡献。首先，我们介绍该领域的研究历史，包括早期工作和基础。我们进一步讨论背景概念并确定 NeSy 发展背后的关键驱动因素。之后，我们根据该研究范式的几个主要特征对最近的里程碑式方法进行分类，包括神经符号集成、知识表示、知识嵌入和功能。接下来，我们简要讨论现代 NeSy 方法在多个领域的成功应用。然后，我们在三个代表性应用任务上对几种 NeSy 方法进行了基准测试。最后，我们确定了未解决的问题以及未来的潜在研究方向。这项调查有望帮助新的研究人员进入这个快速发展的领域，并加速数据和知识驱动的人工智能的进程。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10721277</guid>
      <pubDate>Thu, 17 Oct 2024 13:16:39 GMT</pubDate>
    </item>
    <item>
      <title>Changen2：多时相遥感生成变化基础模型</title>
      <link>http://ieeexplore.ieee.org/document/10713915</link>
      <description><![CDATA[深度视觉模型极大地推进了我们对地球表面时间动态的理解，这些模型通常需要大量带标签的多时态图像进行训练。然而，收集、预处理和注释大规模多时态遥感图像并非易事，因为它成本高昂且需要大量知识。在本文中，我们提出了基于生成模型的可扩展多时态变化数据生成器，它们成本低廉且自动化，可以缓解这些数据问题。我们的主要思想是模拟随时间变化的随机变化过程。我们将随机变化过程描述为概率图模型，即生成概率变化模型 (GPCM)，它将复杂的模拟问题分解为两个更易处理的子问题，即条件级变化事件模拟和图像级语义变化合成。为了解决这两个问题，我们提出了 Changen2，这是一个使用分辨率可扩展的扩散变换器实现的 GPCM，它可以从标记甚至未标记的单时态图像中生成遥感图像的时间序列以及相应的语义和变化标签。Changen2 是一个“生成性变化基础模型”，可以通过自监督进行大规模训练，并且能够从未标记的单时态图像中产生变化监督信号。与现有的“基础模型”不同，我们的生成性变化基础模型综合变化数据来训练特定于任务的变化检测基础模型。生成的模型具有固有的零样本变化检测能力和出色的可迁移性。全面的实验表明 Changen2 在数据生成方面具有出色的时空可扩展性，例如，在 256$^{2}$2 像素单时态图像上训练的 Changen2 模型可以产生任意长度的时间序列和 1,024$^{2}$2 像素的分辨率。 Changen2 预训练模型表现出优异的零样本性能（与全监督模型相比，在 LEVIR-CD 上的性能差距缩小至 3%，在 S2Looking 和 SECOND 上的性能差距缩小至约 10%）并且可在多种类型的变化任务中转移，包括普通和非天底建筑物变化、土地利用/土地覆盖变化和灾害评估。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10713915</guid>
      <pubDate>Thu, 10 Oct 2024 13:16:13 GMT</pubDate>
    </item>
    <item>
      <title>通过参数高效自适应实现具有缺失模态的稳健多模态学习</title>
      <link>http://ieeexplore.ieee.org/document/10713849</link>
      <description><![CDATA[多模态学习旨在利用来自多个来源的数据来提高下游任务的整体性能。数据中的冗余度可以使多模态系统对某些相关模态中缺失或损坏的观测具有鲁棒性。然而，我们观察到，如果在测试时缺少一种或多种模态，则现有几种多模态网络的性能会显著下降。为了实现对缺失模态的鲁棒性，我们提出了一种简单且参数高效的预训练多模态网络自适应程序。具体来说，我们利用中间特征的调制来补偿缺失的模态。我们证明，这种自适应可以部分弥补由于缺失模态而导致的性能下降，并且在某些情况下优于针对可用模态组合进行训练的独立专用网络。所提出的自适应需要极少的参数（例如，不到总参数的 1%），并且适用于广泛的模态组合和任务。我们进行了一系列实验，以强调我们提出的方法在七个数据集的五种不同多模态任务上的缺失模态鲁棒性。我们提出的方法展示了跨各种任务和数据集的多功能性，并且优于现有的缺少模态的稳健多模态学习方法。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10713849</guid>
      <pubDate>Thu, 10 Oct 2024 13:16:13 GMT</pubDate>
    </item>
    <item>
      <title>使用高时间分辨率事件相机进行连续时间物体分割</title>
      <link>http://ieeexplore.ieee.org/document/10713285</link>
      <description><![CDATA[事件相机是一种新型的仿生传感器，其中各个像素独立且异步地运行，以事件的形式产生强度变化。利用事件的微秒分辨率（无运动模糊）和高动态范围（与极端光照条件兼容），在各种应用中直接从稀疏和异步事件流中分割对象具有相当大的前景。然而，与视频对象分割中的丰富线索不同，从稀疏事件流中分割完整的对象具有挑战性。在本文中，我们提出了第一个从事件流中进行连续时间对象分割的框架。给定初始时间的对象掩码，我们的任务旨在在事件流中的任何后续时间分割完整对象。具体来说，我们的框架由基于新颖的 ResLSTM 的循环时间嵌入提取 (RTEE) 模块、跨时间时空特征建模 (CSFM) 模块（一种具有长期和短期匹配模块的转换器架构）和分割头组成。历史事件和掩码（参考集）与当前时间事件一起循环输入到我们的框架中。随着新事件的输入，时间嵌入会更新，从而使我们的框架能够持续处理事件流。为了训练和测试我们的模型，我们构建了真实世界和模拟的基于事件的对象分割数据集，每个数据集都包含事件流、APS 图像和对象注释。在我们的数据集上进行的大量实验证明了所提出的递归架构的有效性。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10713285</guid>
      <pubDate>Thu, 10 Oct 2024 13:16:13 GMT</pubDate>
    </item>
    <item>
      <title>像素就是你所需要的：用于显著物体检测的对抗性时空集成主动学习</title>
      <link>http://ieeexplore.ieee.org/document/10711208</link>
      <description><![CDATA[虽然弱监督技术可以减少标记工作量，但是尚不清楚使用弱监督数据（例如点注释）训练的显著性模型是否能达到与全监督版本相当的性能。本文试图通过证明一个假设来回答这个尚未探索的问题：存在一个点标记数据集，在其上训练的显著性模型可以在密集注释的数据集上训练时达到等效的性能。为了证明这个猜想，我们提出了一种新颖而有效的对抗性时空集成主动学习。我们的贡献有四个方面：1）我们提出的引发不确定性的对抗性攻击可以克服现有主动学习方法的过度自信，并准确定位这些不确定的像素。2）我们提出的时空集成策略不仅实现了出色的性能，而且显着降低了模型的计算成本。3）我们提出的关系感知多样性采样可以克服过采样，同时提高模型性能。 4) 理论证明此类点标记数据集的存在。实验结果表明，我们的方法可以找到此类点标记数据集，在该数据集上训练的显著性模型在每幅图像仅有 10 个标注点的情况下，可获得全监督版本的 98%–99% 的性能。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10711208</guid>
      <pubDate>Wed, 09 Oct 2024 13:17:29 GMT</pubDate>
    </item>
    <item>
      <title>FocalPose++：通过渲染和比较估计焦距和物体姿势</title>
      <link>http://ieeexplore.ieee.org/document/10706831</link>
      <description><![CDATA[我们引入了 FocalPose++，这是一种神经渲染和比较方法，用于在给定描绘已知物体的单个 RGB 输入图像的情况下联合估计相机物体 6D 姿势和相机焦距。这项工作的贡献有三方面。首先，我们推导出一个焦距更新规则，该规则扩展了现有的最先进的渲染和比较 6D 姿势估计器以解决联合估计任务。其次，我们研究了几种不同的损失函数，用于联合估计物体姿势和焦距。我们发现直接焦距回归与重新投影损失相结合，解开了平移、旋转和焦距的贡献，从而获得了更好的结果。第三，我们探索了不同的合成训练数据对我们方法性能的影响。具体而言，我们研究了在渲染合成图像时用于采样物体的 6D 姿势和相机焦距的不同分布，并表明在真实训练数据上拟合的参数分布效果最好。我们在三个具有挑战性的基准数据集上展示了结果，这些数据集在不受控制的设置中描绘了已知的 3D 模型。我们证明我们的焦距和 6D 姿势估计比现有的最先进方法具有更低的误差。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10706831</guid>
      <pubDate>Mon, 07 Oct 2024 13:17:54 GMT</pubDate>
    </item>
    <item>
      <title>探索多模态图像融合的协同高阶相互作用</title>
      <link>http://ieeexplore.ieee.org/document/10706703</link>
      <description><![CDATA[多模态图像融合旨在通过整合和区分来自多个源图像的跨模态互补信息来生成融合图像。虽然具有全局空间交互的交叉注意机制看起来很有前景，但它只捕获二阶空间交互，而忽略了空间和通道维度中的高阶交互。这种限制阻碍了多模态之间协同作用的利用。为了弥补这一差距，我们引入了协同高阶交互范式 (SHIP)，旨在系统地研究跨两个基本维度的多模态图像之间的空间细粒度和全局统计协作：1) 空间维度：我们通过元素乘法构建空间细粒度交互，在数学上等同于全局交互，然后通过迭代聚合和发展互补信息来培养高阶格式，从而提高效率和灵活性。2) 通道维度：在具有一阶统计（平均值）的通道交互的基础上进行扩展，我们设计了高阶通道交互，以便于基于全局统计来辨别源图像之间的相互依赖关系。我们进一步介绍了 SHIP 模型的增强版本 SHIP++，该模型通过跨阶注意力演化机制、跨阶信息集成和残差信息记忆机制增强了跨模态信息交互表示。利用高阶交互显著增强了我们的模型利用多模态协同作用的能力，从而带来了优于最先进替代方案的性能，这在两个重要的多模态图像融合任务（全色锐化和红外与可见光图像融合）中通过各种基准的全面实验得到了证实。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10706703</guid>
      <pubDate>Mon, 07 Oct 2024 13:17:53 GMT</pubDate>
    </item>
    <item>
      <title>GUPNet++：用于单目 3D 物体检测的几何不确定性传播网络</title>
      <link>http://ieeexplore.ieee.org/document/10706891</link>
      <description><![CDATA[几何形状在单目 3D 物体检测中起着重要作用。它可用于利用物体的物理尺寸与图像平面中的 2D 投影之间的透视投影来估计物体深度，从而将数学先验引入深度模型。然而，这种投影过程也引入了误差放大，其中估计高度的误差被放大并反映到投影深度中。这会导致不可靠的深度推断，也会损害训练稳定性。为了解决这个问题，我们提出了一种新颖的几何不确定性传播网络 (GUPNet++)，以概率方式对几何投影进行建模。这确保了深度预测具有良好的界限并与合理的不确定性相关联。引入这种几何不确定性的意义有两方面：(1)。它在训练期间对几何投影的不确定性传播关系进行建模，提高了端到端模型学习的稳定性和效率。(2)。它可以得出高度可靠的置信度来指示 3D 检测结果的质量，从而实现更可靠的检测推断。实验表明，所提出的方法不仅在基于图像的单目 3D 检测中获得了（最先进的）SOTA 性能，而且在简化的框架下也表现出了卓越的功效。代码和模型将在 https://github.com/SuperMHP/GUPNet_Plus 上发布。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10706891</guid>
      <pubDate>Mon, 07 Oct 2024 13:17:53 GMT</pubDate>
    </item>
    <item>
      <title>稀疏非局部 CRF 及其应用</title>
      <link>http://ieeexplore.ieee.org/document/10705039</link>
      <description><![CDATA[CRF 在经典和深度学习计算机视觉中对空间连贯性进行建模。最常见的 CRF 称为成对 CRF，因为它连接像素对。成对 CRF 有两种类型：稀疏和密集。稀疏 CRF 连接附近的像素，导致图像大小的连接数为线性。密集 CRF 连接所有像素对，导致连接数为二次方。虽然密集 CRF 是一种更通用的模型，但其效率远低于稀疏 CRF。事实上，在实践中只使用高斯边缘密集 CRF，即使在那时也使用近似值。我们提出了一种新的成对 CRF，我们称之为稀疏非局部 CRF。与密集 CRF 一样，它具有非局部连接，因此比稀疏 CRF 更通用。与稀疏 CRF 一样，连接数是线性的，因此我们的模型是高效的。除了效率之外，另一个优点是我们的边缘权重不受限制。我们表明，我们的稀疏非局部 CRF 模型的属性与高斯密集 CRF 的属性相似。我们还讨论了与其他 CRF 模型的联系。我们展示了我们的模型在经典和深度学习应用中的实用性，适用于两个和多个标签。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10705039</guid>
      <pubDate>Thu, 03 Oct 2024 13:16:14 GMT</pubDate>
    </item>
    <item>
      <title>竞争像素：一种用于弱监督语义分割的自博弈算法</title>
      <link>http://ieeexplore.ieee.org/document/10705046</link>
      <description><![CDATA[弱监督语义分割 (WSSS) 方法依赖于指示对象存在的图像级标签，缺乏标签与感兴趣区域 (ROI) 之间的明确对应关系，这带来了重大挑战。尽管如此，WSSS 方法还是引起了人们的关注，因为它们的注释成本比全监督分割低得多。利用强化学习 (RL) 自博弈，我们提出了一种新颖的 WSSS 方法，将 ROI 的图像分割游戏化。我们将分割表述为两个代理之间的竞争，它们竞争选择包含 ROI 的补丁，直到用尽所有这样的补丁。每个时间步的分数用于计算代理训练的奖励，表示选择范围内存在对象的可能性，由仅使用对象存在的图像级二元分类标签进行预训练的对象存在检测器确定。此外，我们提出了一个游戏终止条件，在用尽所有包含 ROI 的补丁后，任何一方都可以调用该条件，然后从每个补丁中选择一个最终补丁。终止时，如果包含 ROI 的补丁用尽，代理将获得奖励；如果竞争对手找到包含 ROI 的补丁，代理将失去奖励。这种竞争设置可确保最大限度地减少过度分割或分割不足，这是 WSSS 方法的常见问题。在四个数据集上进行的大量实验表明，与最近的最先进方法相比，性能有显著的改进。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10705046</guid>
      <pubDate>Thu, 03 Oct 2024 13:16:14 GMT</pubDate>
    </item>
    <item>
      <title>俄罗斯套娃：利用深度学习模型的过度参数化进行隐蔽数据传输</title>
      <link>http://ieeexplore.ieee.org/document/10612241</link>
      <description><![CDATA[存储在本地数据中心的高质量隐私机器学习 (ML) 数据成为 AI 公司的关键竞争因素。在本文中，我们提出了一种名为 Matryoshka 的新型内部攻击，以揭示即使在没有暴露接口的情况下破坏 ML 数据隐私的可能性。我们的攻击采用计划发布的 DNN 模型作为载体模型，以隐秘传输秘密模型，这些秘密模型会记忆隐私 ML 数据的信息，否则这些数据与外部人员没有接口。在我们攻击的核心，我们提出了一种新颖的参数共享方法，该方法利用载体模型的学习能力进行信息隐藏。我们的方法同时实现了：（i）高容量——在载体模型几乎没有效用损失的情况下，Matryoshka 可以在载体模型中传输超过 10,000 个真实世界数据样本，该载体模型的参数比被盗数据的总大小少 $220\times$220×，并且可以在单个载体模型中以极低的失真率同时传输多个异构数据集或模型，而现有的隐写术都无法做到这一点； (ii) 解码效率——一旦下载已发布的载体模型，外部合谋者仅凭几个整数秘密和隐藏模型架构的知识就可以从载体模型中独家解码隐藏模型；(iii) 有效性——此外，几乎所有恢复的模型要么具有与在私有数据上独立训练的模型相似的性能，要么可以进一步用于以低错误率提取记忆的原始训练数据；(iv) 鲁棒性——自然实现信息冗余，以实现在载体发布之前对常见后处理技术的弹性；(v) 隐蔽性——具有不同先验知识水平的模型检查者几乎无法区分载体模型和正常模型。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10612241</guid>
      <pubDate>Fri, 26 Jul 2024 13:17:22 GMT</pubDate>
    </item>
    <item>
      <title>以人为中心的 Transformer，用于领域自适应动作识别</title>
      <link>http://ieeexplore.ieee.org/document/10599825</link>
      <description><![CDATA[我们研究了动作识别的领域自适应任务，即领域自适应动作识别，旨在有效地将动作识别能力从标签充足的源域转移到无标签的目标域。由于动作是由人执行的，因此在跨领域识别动作时，利用视频中的人为线索至关重要。然而，现有的方法容易丢失人为线索，而倾向于利用非人类环境与相关动作之间的相关性进行识别，而与动作无关的兴趣环境会降低目标域中的识别性能。为了解决这个问题，我们专注于发现以人为中心的领域自适应动作识别的动作线索，我们的概念是研究以人为中心的动作线索的两个方面，即人为线索和人与环境的交互线索。因此，我们提出的以人为中心的 Transformer (HCTransformer) 开发了一种解耦的以人为中心的学习范式，以明确专注于领域变体视频特征学习中的以人为中心的动作线索。我们的 HCTransformer 首先通过人类编码器进行人机感知时间建模，旨在避免在领域不变的视频特征学习过程中丢失人类线索。然后，通过类似 Transformer 的架构，HCTransformer 通过上下文编码器利用领域不变和动作相关上下文，并进一步建模人类与动作相关上下文之间的领域不变交互。我们在三个基准上进行了广泛的实验，即 UCF-HMDB、Kinetics-NecDrone 和 EPIC-Kitchens-UDA，最先进的性能证明了我们提出的 HCTransformer 的有效性。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10599825</guid>
      <pubDate>Tue, 16 Jul 2024 13:16:28 GMT</pubDate>
    </item>
    </channel>
</rss>