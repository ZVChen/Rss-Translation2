<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>IEEE关于模式分析和机器智能的交易 - 新的TOC</title>
    <link>http://ieeexplore.ieee.org</link>
    <description>TOC警报出版＃34</description>
    <lastBuildDate>Wed, 05 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>扩散++：扩散动作分割</title>
      <link>http://ieeexplore.ieee.org/document/10772006</link>
      <description><![CDATA[了解长形视频需要精确的时间动作细分。尽管现有的研究通常采用遵循迭代改进过程的多阶段模型，但我们提出了一个基于降级扩散模型的新框架，该模型保留了该核心迭代原理。在此框架内，模型迭代产生动作预测，从随机噪声开始，以输入视频的特征为条件。为了有效捕获人类行为的三个关键特征，即先前的立场，边界歧义和关系依赖性，我们提出了针对调节特征的凝聚力掩盖策略。此外，提出了一致性梯度引导技术，该技术可最大程度地提高带有或不带掩盖的输出之间的相似性，从而在推理过程中丰富条件信息。大量实验在四个数据集上进行，即GTEA，50SALADS，早餐和汇编101。结果表明，我们提出的方法优于现有的最新技术，强调了生成方法的动作分割方法的潜力。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10772006</guid>
      <pubDate>Fri, 29 Nov 2024 13:16:48 GMT</pubDate>
    </item>
    <item>
      <title>IBCS：学习信息瓶颈约束的因果子图，用于图形分类</title>
      <link>http://ieeexplore.ieee.org/document/10771715</link>
      <description><![CDATA[图形学习的重大成功激发了一项有意义但具有挑战性的任务，即提取可以解释和改善预测的确切因果子图。不幸的是，目前的作品仅集中在部分消除虚假或嘈杂的部分，同时忽略了一个事实，即在更实际和一般的情况下，无论是虚假和嘈杂的子图与因果关系。这带来了巨大的挑战，并使以前的方法无法提取真正的因果子结构。与现有的研究不同，在本文中，我们提出了一个更合理的问题制定，假设该图是因果，虚假和嘈杂的子图的混合物。在这方面，开发了信息瓶颈限制的因果关系子图（IBCS）学习模型，该模型能够同时排除虚假和嘈杂的部分。具体而言，对于虚假的相关性，我们设计了一个新颖的因果学习目标，其中除了最大程度地减少因果和虚假子图分类的经验风险之外，该干预措施进一步进行了伪造特征，以切断其与因果关系的相关性。在此基础上，我们进一步施加了信息瓶颈约束，以滤除标签 -  iRrex-relevant噪声信息。从理论上讲，我们证明由IBC提取的因果子图可以近似地面真相。从经验上讲，对九个基准数据集进行广泛的评估证明了我们优于最先进的基准。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10771715</guid>
      <pubDate>Thu, 28 Nov 2024 13:18:14 GMT</pubDate>
    </item>
    <item>
      <title>Anchor3Dlane ++：通过样品自适应稀疏3D锚回归的3D车道检测</title>
      <link>http://ieeexplore.ieee.org/document/10771714</link>
      <description><![CDATA[在本文中，我们专注于单眼3D车道检测的具有挑战性的任务。以前的方法通常采用反视角映射（IPM）将前视图（FV）图像或特征转换为鸟眼观察（BEV）空间以进行泳道检测。但是，IPM对BEV表示中的平坦地面假设和上下文信息丢失的依赖导致3D信息估计不准确。尽管已经努力绕过BEV并直接从FV表示中预测3D车道，但由于缺乏3D车道的结构化建模，它们的性能仍然落后于基于BEV的方法。在本文中，我们提出了一种名为Anchor3dlane ++的新型无BEV方法，该方法将3D车道锚定为结构表示，并直接从FV特征进行预测。我们还设计了一个基于原型的自适应锚定（PAAG）模块，以动态生成样品自适应稀疏3D锚。另外，开发出相等的宽度（EW）损失，以利用泳道的平行特性进行正规化。此外，还基于Anchor3Dlane ++探索了相机范围的融合，以利用互补信息。在三个流行的3D车道检测基准上进行的广泛实验表明，我们的Anchor3dlane ++的表现优于先前的最新方法。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10771714</guid>
      <pubDate>Thu, 28 Nov 2024 13:18:14 GMT</pubDate>
    </item>
    <item>
      <title>实用紧凑的深度压缩感</title>
      <link>http://ieeexplore.ieee.org/document/10763443</link>
      <description><![CDATA[近年来，深层网络在压缩传感（CS）中取得了成功，这使采样成本大大降低，自成立以来就引起了人们的关注。在本文中，我们提出了一个新的实用，紧凑的网络，称为通用图像CS的PCNET。具体而言，在PCNET中，设计了一种新颖的协作采样操作员，它由深度条件过滤步骤和双分支快速采样步骤组成。前者学习了线性变换矩阵中的隐式表示，并首先在输入图像上执行自适应局部过滤，而后者随后使用离散的余弦变换和拼命的块状diagonal高斯矩阵来生成不足采样的测量值。我们的PCNET配备了增强的重建近端梯度下降算法网络。一旦训练，它为任意抽样率提供了灵活性，可解释性和强大的恢复性能。此外，我们为单像素CS成像系统提供了面向部署的提取方案，该方案允许将任何线性采样操作员方便地转换为其矩阵表单，以将其加载到数字微型摩尔设备等硬件上。与现有的最新方法相比，对自然图像CS，量化CS和自我监督的CS进行了广泛的实验，证明了PCNET的优异重构精度和概括能力，尤其是对于高分辨率图像。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10763443</guid>
      <pubDate>Fri, 22 Nov 2024 13:16:35 GMT</pubDate>
    </item>
    <item>
      <title>以任务为导向的渠道注意细粒度分类</title>
      <link>http://ieeexplore.ieee.org/document/10763467</link>
      <description><![CDATA[细颗粒图像分类的困难主要来自跨班级的共同整体外观。因此，认识到歧视性细节，例如鸟类的眼睛和喙，是任务的关键。但是，当培训数据受到限制时，这尤其具有挑战性。为了解决这个问题，我们提出了任务差异最大化（TDM），这是一种面向任务的通道注意方法，该方法针对精细颗粒的几弹性分类量使用，其中有两个新型模块支持注意力模块（SAM）和查询注意模块（QAM）。 SAM突出显示了编码类别歧视性特征的通道，而QAM将更高的权重分配给查询的对象相关通道。基于这些子模型，TDM通过专注于编码类别歧视性细节的通道来产生任务自适应的功能，并同时使用查询，以提供支持和查询实例之间的准确类别敏感的相似度度量。尽管TDM通过任务自适应校准通道的重要性影响高级特征图，但我们进一步介绍了在特征提取器的中间层中运行的实例注意模块（IAM），以实例化突出显示对象与对象相关的通道，并扩展QAM。 TDM和IAM的优点及其互补益处在精细粒度的少量分类任务中得到了实验验证。此外，IAM在粗粒和跨域几乎没有分类中也有效。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10763467</guid>
      <pubDate>Thu, 21 Nov 2024 13:20:31 GMT</pubDate>
    </item>
    <item>
      <title>图像检索及其内存足迹优化的相关验证</title>
      <link>http://ieeexplore.ieee.org/document/10759842</link>
      <description><![CDATA[在本文中，我们提出了一个名为“相关验证网络”（CVNET）的新型图像检索网络，以用4D卷积神经网络替换传统的几何重新排列，该网络学习多样化的几何匹配可能性。为了启用有效的跨尺度匹配，我们构建特征金字塔并在单个推理中建立跨尺度特征相关性，从而替换了昂贵的多尺度推断。此外，我们采用捉迷藏策略的课程学习来处理具有挑战性的样本。我们提出的CVNET证明了在几个图像检索基准测试中的最新性能。但是，从实现角度来看，CVNET有一个缺点：它需要高内存使用情况，因为它需要存储所有数据库图像的密集特征。在实际应用中，这种高内存需求可能是一个重要的限制。为了解决此问题，我们引入了CVNET的扩展名，称为密集到SPARSE CVNET（CVNET $^{DS} $ DS），该扩展可以通过占用数据库图像的功能来大大减少内存使用情况。 CVNET $^{DS} $ DS中的稀疏模块学会使用Gumbel估算器端到端选择图像功能的相关部分。由于泄漏是离线执行的，因此CVNET $^{DS} $ DS不会增加在线提取和匹配时间。 cvnet $^{ds} $ ds会大大降低内存足迹，同时保持性能水平几乎相同。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10759842</guid>
      <pubDate>Thu, 21 Nov 2024 13:20:31 GMT</pubDate>
    </item>
    <item>
      <title>细粒度的视觉文本提示</title>
      <link>http://ieeexplore.ieee.org/document/10763465</link>
      <description><![CDATA[视觉语言模型（vlms），例如剪辑，在零拍图像级的视觉理解中出色，但与需要精确本地化和识别的基于对象的任务进行斗争。建议视觉提示，例如彩色盒子或圆圈，以增强当地的感知。但是，这些方法通常包括无关紧要和嘈杂的像素，导致次优性能。更好的视觉提示的设计及其与文本提示的合作仍然没有充满反感。本文介绍了细粒度的视觉文本提示（FGVTP），这是一种使用精确的语义掩码和增强的图像文本对齐的新的零摄像框架，用于基于对象的任务。 FGVTP包括细粒度的视觉提示（FGVP）和一致性增强文本提示（CETP）。具体而言，我们通过探索更多形状和形式变化的视觉标记来仔细研究视觉提示设计。 FGVP使用分段器中的语义掩码，例如任何模型（SAM），并采用背景模糊（模糊的反向掩码）来突出目标，同时保持空间相干性。此外，CETP通过基于FGVP处理的图像提示字幕来增强图像文本对齐。结果，FGVTP实现了对reccoco/+/g基准测试的优质零射击表达理解，平均比以前的SOTA方法平均超过5.8％。在PACO数据集上进行的零件检测实验进一步验证了FGVTP比现有作品的优势。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10763465</guid>
      <pubDate>Thu, 21 Nov 2024 13:20:31 GMT</pubDate>
    </item>
    <item>
      <title>流式传感器用于在线，高性能成像和视觉</title>
      <link>http://ieeexplore.ieee.org/document/10758928</link>
      <description><![CDATA[最近，Quanta图像传感器（QIS） - 超快速，零阅读的噪声二进制图像传感器 - 在许多具有挑战性的情况下表现出了显着的成像功能。尽管具有潜力，但这些传感器的采用受（a）高数据速率和（b）需要新的计算管道来处理非常规原始数据的需求严重阻碍了这些传感器。我们介绍了一个简单的低带宽计算管道，以应对这些挑战。我们的方法基于具有较小的记忆足迹的新型流媒体表示，在多个时间尺度上有效捕获强度信息。更新表示形式仅需要24个浮动点操作/像素，可以通过二进制框架的本机帧速率在线计算它们。我们使用在此表示形式上运行的神经网络实时重建视频（10-30 fps）。我们说明了为什么这种表示形式适合这些新兴传感器，以及如何提供低潜伏期和高帧速率，同时保留了下游计算机视觉的灵活性。我们的方法导致大量数据带宽减少（$ \ sim 100 \ times $ 〜100×）和实时图像重建和计算机视觉$ -10^{4} \ text { - } 10^{5} \ times $  - 比现有的最新方法（Ma等，2020）的计算减少104-105×，同时保持了可比的质量。据我们所知，我们的方法是第一个在QIS上实现在线实时图像重建的方法。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10758928</guid>
      <pubDate>Wed, 20 Nov 2024 13:16:48 GMT</pubDate>
    </item>
    <item>
      <title>部分场景文字检索</title>
      <link>http://ieeexplore.ieee.org/document/10758313</link>
      <description><![CDATA[部分场景文本检索的任务涉及本地位置和搜索与图像库中给定查询文本相同或相似的文本实例。但是，现有方法只能处理文本行实例，而由于培训数据中缺乏补丁注释，因此在这些文本行实例中搜索部分补丁的问题。为了解决这个问题，我们提出了一个可以同时检索文本线实例及其部分补丁的网络。我们的方法将两种类型的数据（查询文本和场景文本实例）嵌入共享特征空间，并测量其跨模式相似性。为了处理部分补丁，我们提出的方法采用多个实例学习（MIL）方法来学习与查询文本的相似之处，而无需额外的注释。但是，构造袋子是常规MIL方法的标准步骤，可以引入许多嘈杂的样本进行训练，并降低推理速度。为了解决这个问题，我们提出了一种排名MIL（RANKMIL）的方法来适应过滤这些嘈杂的样本。此外，我们提出了一种动态的部分匹配算法（DPMA），该算法可以在推理阶段直接从文本行实例中直接搜索目标部分贴片，而无需袋子。这大大提高了搜索效率和检索部分补丁的性能。我们在两个任务中评估了英语和中文数据集的提议方法：检索文本线实例和部分补丁。对于英语文本检索，我们的方法在这两个任务的三个数据集中，我们的方法的平均地图分别超过8.04％的地图和平均12.71％的地图。对于中文文本检索，我们的方法超过了最新的方法24.45％地图和三个任务数据集中的平均地图分别超过了38.06％的地图。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10758313</guid>
      <pubDate>Tue, 19 Nov 2024 13:16:30 GMT</pubDate>
    </item>
    <item>
      <title>Bokehme ++：多功能散景的古典和神经渲染的和谐融合</title>
      <link>http://ieeexplore.ieee.org/document/10756626</link>
      <description><![CDATA[尽管在模拟数字单镜头反射摄像机（DSLR）的散景效果方面取得了重大进步，但在处理突出显示点上仍然存在挑战，从而保留了对焦对象的边界细节，并有效地处理了高分辨率图像。为了解决这些问题，我们首先开发了基于射线追踪的散景模拟器。引入了具有重量重新分配的创新管道，以处理突出显示的渲染。通过考虑镜头枪管的前长，我们可以模拟逼真的猫眼效应。此散景模拟器是创建我们的培训数据集的基础。在此数据集的基础上，我们引入了一个混合框架Bokehme ++，结合了一个经典的渲染器和神经渲染器。经典渲染器是通过基于层次散射的方法实现的，该方法遭受边界不准确性。这些错误的区域将通过错误映射发生器确定，并通过两阶段的神经渲染器进行纠正。在神经渲染器中引入了自适应调整和迭代式提升采样，以有效地处理任意模糊大小。广泛的实验表明，Bokehme ++优于现有方法，并提供高度可定制的渲染功能，例如可调节的模糊量，焦平面，高光模式和猫眼效应。此外，Bokehme ++可以通过辅助Alpha地图输入在肖像中保持头发细节的清晰度。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10756626</guid>
      <pubDate>Mon, 18 Nov 2024 13:16:41 GMT</pubDate>
    </item>
    <item>
      <title>一项全面的调查，对不断学习的深度学习遗忘</title>
      <link>http://ieeexplore.ieee.org/document/10752992</link>
      <description><![CDATA[忘记是指先前获得的知识的损失或恶化。尽管现有有关忘记的调查主要集中在持续学习上，但忘记是在深度学习中其他各种研究领域中观察到的普遍现象。忘记在研究领域（例如生成器移动引起的生成模型）以及由于跨客户的异质数据分布而导致的联合学习。解决忘记的解决涵盖了几个挑战，包括平衡旧任务知识与新任务的保留，管理任务干扰与冲突的目标以及防止隐私泄漏等。 。相比之下，我们的调查认为，忘记是一把双刃剑，在某些情况下，例如保护隐私的情况可能是有益的和可取的。通过在更广泛的背景下探索忘记，我们对这种现象有了更细微的理解，并强调了其潜在的优势。通过这项全面的调查，我们渴望通过利用涉及遗忘的各个领域的思想和方法来揭示潜在的解决方案。通过检查忘记超越其传统界限，我们希望鼓励发展新颖的策略来缓解，利用，甚至拥抱在实际应用中遗忘。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10752992</guid>
      <pubDate>Thu, 14 Nov 2024 13:16:50 GMT</pubDate>
    </item>
    <item>
      <title>Patnas：基于小路的无训练神经建筑搜索</title>
      <link>http://ieeexplore.ieee.org/document/10753099</link>
      <description><![CDATA[神经体系结构搜索（NAS）的开发受到与评估网络体系结构相关的高成本的阻碍。最近，已经提出了一些零成本代理，作为降低NAS网络体系结构评估成本的有前途的方法。他们可以在初始阶段的几秒钟内快速估算网络的最终性能。但是，现有的零成本代理要么忽略网络结构对性能的影响，要么仅限于特定任务。为了解决这些问题，我们提出了一个新颖的零成本代理，称为骨架路径内核跟踪（SPKT），该代理利用整个网络体系结构的骨骼路径结构信息。然后，我们将其集成到称为PATNAS的NAS框架的有效贝叶斯优化中，并在不同数据集上演示其功效。结果表明，我们提出的SPKT零成本代理可以在多个任务中与网络的最终性能达到高度相关性。此外，它可以显着加速寻找表现最佳的网络体系结构的搜索过程。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10753099</guid>
      <pubDate>Thu, 14 Nov 2024 13:16:50 GMT</pubDate>
    </item>
    <item>
      <title>DIFFI2I：图像到图像翻译的有效扩散模型</title>
      <link>http://ieeexplore.ieee.org/document/10752976</link>
      <description><![CDATA[扩散模型（DM）已成为图像合成的SOTA方法。但是，现有的DM在某些图像到图像翻译（I2i）任务上的表现无法很好地表现。与图像合成不同，某些I2I任务（例如超分辨率）需要根据GT图像生成结果。图像合成的传统DMS需要广泛的迭代和大型的DeNoing模型来估计整个图像，这具有强大的生成能力，但也导致I2i的伪像和效率低下。为了应对这一挑战，我们为I2i提出了一个简单，高效且功能强大的DM框架，称为DIFFI2I。具体而言，DIFFI2I包括三个关键组件：紧凑的I2I先验提取网络（CPEN），动态I2I变压器（DI2Siformer）和一个脱氧网络。我们分两个阶段训练Diffi2i：预训练和DM培训。为了进行预训练，GT和输入图像被送入CPEN $ _ {S1} $ S1中，以捕获紧凑的I2I先验表示（IPR）引导DI2Siformer。在第二阶段，对DM进行训练，仅使用输入映像来估计与CPEN $ _ {S1} $ S1相同的IRP。与传统的DMS相比，紧凑型知识产权使Diffi2i能够获得更准确的结果，并采用了更轻的Denoising网络和更少的迭代。通过对各种I2I任务的广泛实验，我们证明了DIFFI2I可以达到SOTA性能，同时显着降低了计算负担。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10752976</guid>
      <pubDate>Thu, 14 Nov 2024 13:16:50 GMT</pubDate>
    </item>
    <item>
      <title>用于3D域概括和适应的多尺度零件特征表示</title>
      <link>http://ieeexplore.ieee.org/document/10750436</link>
      <description><![CDATA[3D点云的深网在分类任务中取得了显着成功，但仍然容易受到数据采集程序不一致的几何变化的影响。当对源域上训练的模型进行了测试时，这会导致显着的性能降解，从而在分布范围的目标域上进行了测试，从而强调了3D域概括和适应性的挑战。在本文中，我们介绍了一种新型的基于多尺度零件的特征表示，称为MSPR，作为点云域的概括和适应性的可推广表示。我们不依赖于全局形状功能，而是将不同尺度的形状的零件级特征与一组可学习的零件模板特征，这些特征编码源和目标域之间共享的本地几何结构。具体而言，来自不同域的形状被组织成各种尺度的零件级特征，然后与零件板特征对齐。为了平衡不同尺度上零件的概括和歧视能力，我们进一步设计了一个跨尺度的特征融合模块，以在不同尺度的基于基于零件的特征之间交换信息。基于融合的零件表示最终由基于零件的特征聚合模块汇总。为了改善基于对齐的部分表示的鲁棒性和对几何变化的全局形状表示，我们进一步提出了形状表示（CLSR）的对比度学习框架。实验是在3D结构域的概括和适应基准上进行点云分类的。对3D域的概括和适应基准的广泛实验表明，在这两个任务中，提出的方法都优于先前的最新方法。消融研究证实了组件在我们的模型中的有效性。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10750436</guid>
      <pubDate>Mon, 11 Nov 2024 13:16:35 GMT</pubDate>
    </item>
    <item>
      <title>hi-sam：嫁给段的任何模型用于层次文本细分</title>
      <link>http://ieeexplore.ieee.org/document/10750316</link>
      <description><![CDATA[该细分市场模型（SAM）是在大规模数据集上预测的深刻视觉基础模型，它破坏了一般细分的界限，并引发了各种下游应用程序。本文介绍了HI-SAM，这是一个统一的模型，利用SAM进行分层文本分割。 HI-SAM在分段方面划分了四个层次结构，包括像素级文本，单词，文本行和段落，同时也实现了布局分析。具体而言，我们首先通过参数有效的微调方法将SAM变成高质量像素级文本分割（TS）模型。我们使用此TS模型以半自动方式迭代生成像素级文本标签，从而在HierText DataSet中的四个文本层次结构上统一标签。随后，使用这些完整的标签，我们使用定制的层次掩码解码器启动了基于TS体系结构的端到端训练HI-SAM。在推断期间，HI-SAM既提供自动掩码生成（AMG）模式，又提供迅速分割（PS）模式。在AMG模式下，HI-SAM片段像素级文本最初是前景掩码，然后为层次文本掩码生成的前景点进行样品，并通过传递进行布局分析。至于PS模式，HI-SAM单击单点，提供单词，文本行和段落掩码。实验结果表明，我们的TS模型的最新性能：用于像素级文本分割的TextSeg上的总文本率为84.86％，FGIOU为88.96％。此外，与先前用于HierText的联合分层检测和布局分析的专家相比，HI-SAM取得了重大改进：文本线级别的4.73％PQ和5.39％F1，5.49％PQ和7.39％F1在段级别的布局上分析，需要$ 20 \ Times $ 20×较少的培训时期。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10750316</guid>
      <pubDate>Mon, 11 Nov 2024 13:16:35 GMT</pubDate>
    </item>
    </channel>
</rss>