<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>IEEE 模式分析与机器智能学报 - 新目录</title>
    <link>http://ieeexplore.ieee.org</link>
    <description>出版物 # 34 的目录提醒</description>
    <lastBuildDate>Wed, 05 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>用于学习迭代模型的深度损失凸化</title>
      <link>http://ieeexplore.ieee.org/document/10777605</link>
      <description><![CDATA[由于非凸优化的性质，用于点云配准的迭代方法（例如迭代最近点 (ICP)）通常会受到不良局部最优性（例如鞍点）的影响。为了解决这一根本挑战，在本文中，我们提出学习将深度迭代方法的损失景观相对于测试时预测的损失景观在给定数据的情况下局部形成围绕每个地面真实值的凸形，即深度损失凸化 (DLC)，这要归功于神经网络中的过度参数化。为此，我们通过操纵地面真实值预测而不是输入数据来制定基于对抗性训练的学习目标。具体而言，我们建议使用星形凸性（一类结构化非凸函数，它们在通过全局最小化器的所有线上都是单峰的）作为重塑损失景观的几何约束，从而导致 (1) 在原始损失上附加额外的新型铰链损失和 (2) 接近最优预测。我们展示了使用 DLC 和现有技术架构在训练循环神经网络 (RNN)、3D 点云配准和多模型图像配准任务方面的最佳性能。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10777605</guid>
      <pubDate>Wed, 04 Dec 2024 13:17:52 GMT</pubDate>
    </item>
    <item>
      <title>DiffAct++：扩散动作分割</title>
      <link>http://ieeexplore.ieee.org/document/10772006</link>
      <description><![CDATA[理解长视频需要精确的时间动作分割。虽然现有研究通常采用遵循迭代细化过程的多阶段模型，但我们提出了一个基于去噪扩散模型的新框架，该框架保留了这一核心迭代原理。在此框架内，该模型以随机噪声为起点，以输入视频的特征为条件，迭代地产生动作预测。为了有效地捕捉人类动作的三个关键特征，即位置先验、边界模糊性和关系依赖性，我们提出了一种针对条件特征的内聚掩蔽策略。此外，还提出了一种一致性梯度引导技术，该技术可以最大化有或没有掩蔽的输出之间的相似性，从而丰富推理过程中的条件信息。在四个数据集上进行了广泛的实验，即 GTEA、50Salads、Breakfast 和 Assembly101。结果表明，我们提出的方法优于或与现有的最先进技术相当，凸显了生成方法在动作分割方面的潜力。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10772006</guid>
      <pubDate>Fri, 29 Nov 2024 13:16:48 GMT</pubDate>
    </item>
    <item>
      <title>IBCS：学习信息瓶颈约束去噪因果子图进行图分类</title>
      <link>http://ieeexplore.ieee.org/document/10771715</link>
      <description><![CDATA[图学习的巨大成功引发了一项有意义但具有挑战性的任务，即提取能够解释和改进预测的精确的因果子图。不幸的是，当前的研究仅仅集中在部分消除虚假或噪声部分，而忽略了在更实际和一般的情况下，虚假和噪声子图与因果子图共存的事实。这带来了巨大的挑战，并且使得以前的方法无法提取真正的因果子结构。与现有研究不同，在本文中，我们提出了一个更合理的问题公式，假设图是因果、虚假和噪声子图的混合。在这方面，开发了一种信息瓶颈约束的去噪因果子图 (IBCS) 学习模型，该模型能够同时排除虚假和噪声部分。具体来说，对于虚假相关性，我们设计了一个新颖的因果学习目标，除了最小化因果和虚假子图分类的经验风险之外，还对虚假特征进行干预以切断其与因果部分的相关性。在此基础上，我们进一步施加信息瓶颈约束以过滤掉与标签无关的噪声信息。从理论上讲，我们证明了我们的 IBCS 提取的因果子图可以近似于地面真相。从经验上讲，对九个基准数据集的广泛评估证明了我们优于最先进的基线。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10771715</guid>
      <pubDate>Thu, 28 Nov 2024 13:18:14 GMT</pubDate>
    </item>
    <item>
      <title>Anchor3DLane++：通过样本自适应稀疏 3D 锚点回归进行 3D 车道检测</title>
      <link>http://ieeexplore.ieee.org/document/10771714</link>
      <description><![CDATA[在本文中，我们专注于单目 3D 车道检测这一具有挑战性的任务。以前的方法通常采用逆透视映射 (IPM) 将正面 (FV) 图像或特征转换为鸟瞰 (BEV) 空间以进行车道检测。然而，IPM 对平地假设的依赖和 BEV 表示中的上下文信息丢失导致 3D 信息估计不准确。尽管已经努力绕过 BEV 并直接从 FV 表示中预测 3D 车道，但由于缺乏 3D 车道的结构化建模，它们的性能仍然落后于基于 BEV 的方法。在本文中，我们提出了一种新颖的无 BEV 方法 Anchor3DLane++，它将 3D 车道锚点定义为结构表示并直接从 FV 特征进行预测。我们还设计了一个基于原型的自适应锚点生成 (PAAG) 模块来动态生成样本自适应稀疏 3D 锚点。此外，我们还开发了等宽 (EW) 损失函数，以利用车道的平行特性进行正则化。此外，我们还基于 Anchor3DLane++ 探索了摄像头-LiDAR 融合，以利用互补信息。在三个流行的 3D 车道检测基准上进行的大量实验表明，我们的 Anchor3DLane++ 优于之前最先进的方法。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10771714</guid>
      <pubDate>Thu, 28 Nov 2024 13:18:14 GMT</pubDate>
    </item>
    <item>
      <title>实用紧凑深度压缩感知</title>
      <link>http://ieeexplore.ieee.org/document/10763443</link>
      <description><![CDATA[近年来，深度网络在压缩感知 (CS) 领域取得了成功，它显著降低了采样成本，自诞生以来就受到越来越多的关注。在本文中，我们提出了一种新的实用紧凑网络，称为 PCNet，用于一般图像 CS。具体而言，在 PCNet 中，设计了一种新颖的协同采样算子，它由深度条件过滤步骤和双分支快速采样步骤组成。前者将线性变换矩阵的隐式表示学习为几个卷积，并首先对输入图像执行自适应局部滤波，而后者则使用离散余弦变换和加扰块对角高斯矩阵来生成欠采样测量值。我们的 PCNet 配备了增强型近端梯度下降算法展开网络用于重建。一旦经过训练，它就能为任意采样率提供灵活性、可解释性和强大的恢复性能。此外，我们还为单像素 CS 成像系统提供了一种面向部署的提取方案，允许将任何线性采样算子方便地转换为其矩阵形式，以便加载到数字微镜设备等硬件上。对自然图像 CS、量化 CS 和自监督 CS 进行的大量实验表明，与现有的最先进方法相比，PCNet 具有更优异的重建精度和泛化能力，尤其是对于高分辨率图像。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10763443</guid>
      <pubDate>Fri, 22 Nov 2024 13:16:35 GMT</pubDate>
    </item>
    <item>
      <title>面向任务的通道注意力机制，用于细粒度小样本分类</title>
      <link>http://ieeexplore.ieee.org/document/10763467</link>
      <description><![CDATA[细粒度图像分类的难度主要来自于不同类别之间共享的整体外观。因此，识别鸟类的眼睛和喙等判别性细节是这项任务的关键。然而，当训练数据有限时，这尤其具有挑战性。为了解决这个问题，我们提出了任务差异最大化 (TDM)，这是一种面向任务的通道注意方法，适用于细粒度小样本分类，具有两个新模块：支持注意模块 (SAM) 和查询注意模块 (QAM)。SAM 突出显示编码类别判别特征的通道，而 QAM 为查询的对象相关通道分配更高的权重。基于这些子模块，TDM 通过关注编码类别判别性细节并同时由查询拥有的通道来生成任务自适应特征，从而在支持和查询实例之间实现准确的类别敏感相似性度量。虽然 TDM 通过任务自适应校准通道重要性来影响高级特征图，但我们通过扩展 QAM 进一步引入了实例注意模块 (IAM)，该模块在特征提取器的中间层中运行，以实例方式突出显示与对象相关的通道。在细粒度的少样本分类任务中，实验验证了 TDM 和 IAM 的优点及其互补优势。此外，IAM 在粗粒度和跨域少样本分类中也有效。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10763467</guid>
      <pubDate>Thu, 21 Nov 2024 13:20:31 GMT</pubDate>
    </item>
    <item>
      <title>图像检索的相关性验证及其内存占用优化</title>
      <link>http://ieeexplore.ieee.org/document/10759842</link>
      <description><![CDATA[在本文中，我们提出了一种名为相关性验证网络 (CVNet) 的新型图像检索网络，以使用学习各种几何匹配可能性的 4D 卷积神经网络取代传统的几何重新排序。为了实现高效的跨尺度匹配，我们构建了特征金字塔并在一次推理中建立跨尺度特征相关性，从而取代了昂贵的多尺度推理。此外，我们采用课程学习和捉迷藏策略来处理具有挑战性的样本。我们提出的 CVNet 在多个图像检索基准上以很大的优势展示了最先进的性能。然而，从实现的角度来看，CVNet 有一个缺点：它需要高内存使用率，因为它需要存储所有数据库图像的密集特征。这种高内存需求在实际应用中可能是一个重大限制。为了解决这个问题，我们引入了 CVNet 的扩展，称为密集到稀疏的 CVNet (CVNet$^{DS}$DS)，它可以通过稀疏化数据库图像的特征来显著减少内存使用量。 CVNet$^{DS}$DS 中的稀疏化模块学习使用 Gumbel 估计器端到端地选择图像特征的相关部分。由于稀疏化是离线执行的，因此 CVNet$^{DS}$DS 不会增加在线提取和匹配时间。CVNet$^{DS}$DS 显著减少了内存占用，同时保持了与 CVNet 几乎相同的性能水平。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10759842</guid>
      <pubDate>Thu, 21 Nov 2024 13:20:31 GMT</pubDate>
    </item>
    <item>
      <title>细粒度的视觉文本提示</title>
      <link>http://ieeexplore.ieee.org/document/10763465</link>
      <description><![CDATA[视觉语言模型 (VLM)，例如 CLIP，在零样本图像级视觉理解方面表现出色，但在需要精确定位和识别的基于对象的任务中却举步维艰。建议使用彩色框或圆圈等视觉提示来增强局部感知。然而，这些方法通常包含不相关和嘈杂的像素，导致性能不佳。更好的视觉提示的设计及其与文本提示的协作仍未得到充分探索。本文介绍了细粒度视觉文本提示 (FGVTP)，这是一种使用精确语义掩码和强化图像文本对齐的基于对象任务的新零样本框架。FGVTP 包括细粒度视觉提示 (FGVP) 和一致性增强文本提示 (CETP)。具体来说，我们通过探索更多形状和形式各异的视觉标记来仔细研究视觉提示设计。FGVP 使用来自分割器（如 Segment Anything Model (SAM)）的语义掩码，并采用背景模糊（模糊反向掩码）来突出显示目标，同时保持空间连贯性。此外，CETP 通过基于 FGVP 处理的图像提示字幕来增强图像文本对齐。因此，FGVTP 在 RefCOCO/+/g 基准上实现了卓越的零样本指代表达理解，平均比以前的 SOTA 方法高出 5.8%。在 PACO 数据集上进行的部分检测实验进一步验证了 FGVTP 相对于现有作品的优势。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10763465</guid>
      <pubDate>Thu, 21 Nov 2024 13:20:31 GMT</pubDate>
    </item>
    <item>
      <title>用于在线高性能成像和视觉的流式量子传感器</title>
      <link>http://ieeexplore.ieee.org/document/10758928</link>
      <description><![CDATA[最近，量子图像传感器 (QIS) - 超快、零读取噪声二进制图像传感器 - 在许多具有挑战性的场景中展示了卓越的成像能力。尽管它们具有潜力，但这些传感器的采用受到 (a) 高数据速率和 (b) 需要新的计算管道来处理非常规原始数据的严重阻碍。我们引入了一种简单的低带宽计算管道来应对这些挑战。我们的方法基于一种新颖的流式表示，具有较小的内存占用，可有效捕获多个时间尺度的强度信息。更新表示只需要 24 个浮点运算/像素，可以以二进制帧的原始帧速率在线高效计算。我们使用基于此表示的神经网络来实时重建视频（10-30 fps）。我们说明了为什么这种表示非常适合这些新兴传感器，以及它如何提供低延迟和高帧速率，同时为下游计算机视觉保留灵活性。我们的方法可显著减少数据带宽（约 100\times$∼100 倍），并且与现有的最先进方法（Ma et al. 2020）相比，实时图像重建和计算机视觉计算量可减少 $-10^{4}\text{-}10^{5} \times$-104-105 倍，同时保持同等质量。据我们所知，我们的方法是第一个在 QIS 上实现在线实时图像重建的方法。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10758928</guid>
      <pubDate>Wed, 20 Nov 2024 13:16:48 GMT</pubDate>
    </item>
    <item>
      <title>部分场景文本检索</title>
      <link>http://ieeexplore.ieee.org/document/10758313</link>
      <description><![CDATA[部分场景文本检索任务涉及从图库中定位和搜索与给定查询文本相同或相似的文本实例。但是，现有方法只能处理文本行实例，由于训练数据中缺少补丁注释，因此无法解决在这些文本行实例中搜索部分补丁的问题。为了解决这个问题，我们提出了一个可以同时检索文本行实例及其部分补丁的网络。我们的方法将两种类型的数据（查询文本和场景文本实例）嵌入到共享特征空间中并测量它们的跨模态相似性。为了处理部分补丁，我们提出的方法采用多实例学习 (MIL) 方法来学习它们与查询文本的相似性，而无需额外的注释。然而，构建包是传统 MIL 方法的标准步骤，这会引入大量嘈杂的训练样本，并降低推理速度。为了解决这个问题，我们提出了一种排名 MIL (RankMIL) 方法来自适应地过滤这些嘈杂的样本。此外，我们提出了一种动态部分匹配算法 (DPMA)，该算法可以在推理阶段直接从文本行实例中搜索目标部分补丁，而无需使用包。这大大提高了搜索效率和检索部分补丁的性能。我们在英文和中文数据集的两个任务中评估了所提出的方法：检索文本行实例和部分补丁。对于英文文本检索，在两个任务的三个数据集中，我们的方法平均分别比最新方法高出 8.04% mAP 和 12.71% mAP。对于中文文本检索，在两个任务的三个数据集中，我们的方法平均分别比最新方法高出 24.45% mAP 和 38.06% mAP。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10758313</guid>
      <pubDate>Tue, 19 Nov 2024 13:16:30 GMT</pubDate>
    </item>
    <item>
      <title>BokehMe++：传统渲染与神经渲染的完美融合，实现多种散景创作</title>
      <link>http://ieeexplore.ieee.org/document/10756626</link>
      <description><![CDATA[尽管在从全焦图像模拟数码单反相机 (DSLR) 的散景效果方面取得了重大进展，但在处理亮点、保留对焦物体的边界细节以及有效处理高分辨率图像方面仍然存在挑战。为了解决这些问题，我们首先开发了一个基于光线追踪的散景模拟器。引入了一种具有权重重新分配的创新管道来处理高光渲染。通过考虑镜筒的前长，我们可以模拟逼真的猫眼效果。这个散景模拟器是我们创建训练数据集的基础。在此数据集的基础上，我们引入了一个混合框架 BokehMe++，结合了经典渲染器和神经渲染器。经典渲染器是通过基于分层散射的方法实现的，该方法存在边界不准确的问题。这些错误区域将由错误图生成器识别，并由两阶段神经渲染器进行校正。神经渲染器中引入了自适应调整大小和迭代上采样，以有效处理任意模糊大小。大量实验表明，BokehMe++ 的表现优于现有方法，并提供高度可定制的渲染功能，例如可调节的模糊量、焦平面、高光模式和猫眼效果。此外，BokehMe++ 可以通过辅助 alpha 贴图输入保持肖像中头发细节的清晰度。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10756626</guid>
      <pubDate>Mon, 18 Nov 2024 13:16:41 GMT</pubDate>
    </item>
    <item>
      <title>超越持续学习的深度学习遗忘综合调查</title>
      <link>http://ieeexplore.ieee.org/document/10752992</link>
      <description><![CDATA[遗忘是指先前获得的知识的丢失或退化。虽然现有的关于遗忘的研究主要集中在持续学习上，但遗忘是深度学习中其他各种研究领域中观察到的一种普遍现象。遗忘表现在诸如由于生成器移位而导致的生成模型和由于客户端之间异构数据分布而导致的联邦学习等研究领域。解决遗忘问题涉及几个挑战，包括平衡旧任务知识的保留与新任务的快速学习、管理与冲突目标的任务干扰以及防止隐私泄露等。此外，大多数现有的关于持续学习的调查都隐含地假设遗忘总是有害的。相比之下，我们的调查认为遗忘是一把双刃剑，在某些情况下可能是有益的和可取的，例如隐私保护场景。通过在更广泛的背景下探索遗忘，我们对这一现象提出了更细致入微的理解，并强调了它的潜在优势。通过这项全面的调查，我们希望通过借鉴处理遗忘的各个领域的想法和方法来发现潜在的解决方案。通过超越传统界限来研究遗忘，我们希望鼓励开发新的策略来在实际应用中减轻、控制甚至接受遗忘。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10752992</guid>
      <pubDate>Thu, 14 Nov 2024 13:16:50 GMT</pubDate>
    </item>
    <item>
      <title>PATNAS：基于路径的无训练神经架构搜索</title>
      <link>http://ieeexplore.ieee.org/document/10753099</link>
      <description><![CDATA[评估网络架构的高成本阻碍了神经架构搜索 (NAS) 的发展。最近，已经提出了几种零成本代理，作为降低 NAS 中网络架构评估成本的有前途的方法。它们可以在初始阶段的几秒钟内快速估算网络的最终性能。然而，现有的零成本代理要么忽略了网络结构对性能的影响，要么仅限于特定任务。为了解决这些问题，我们提出了一种名为骨架路径核跟踪 (SPKT) 的新型零成本代理，它利用整个网络架构的骨架路径结构信息。然后，我们将其集成到名为 PATNAS 的有效 NAS 贝叶斯优化框架中，并在不同的数据集上证明其有效性。结果表明，我们提出的 SPKT 零成本代理可以在多个任务中与网络的最终性能实现高度相关。此外，它可以显著加快寻找最佳性能网络架构的搜索过程。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10753099</guid>
      <pubDate>Thu, 14 Nov 2024 13:16:50 GMT</pubDate>
    </item>
    <item>
      <title>DiffI2I：用于图像到图像转换的有效扩散模型</title>
      <link>http://ieeexplore.ieee.org/document/10752976</link>
      <description><![CDATA[扩散模型 (DM) 已成为图像合成的 SOTA 方法。然而，现有的 DM 在某些图像到图像转换 (I2I) 任务上表现不佳。与图像合成不同，某些 I2I 任务（例如超分辨率）需要根据 GT 图像生成结果。用于图像合成的传统 DM 需要大量迭代和大型去噪模型来估计整个图像，这赋予了它们强大的生成能力，但也导致了 I2I 的伪影和效率低下。为了应对这一挑战，我们为 I2I 提出了一个简单、高效且强大的 DM 框架，称为 DiffI2I。具体而言，DiffI2I 包含三个关键组件：紧凑的 I2I 先验提取网络 (CPEN)、动态 I2I 变换器 (DI2Iformer) 和去噪网络。我们分两个阶段训练 DiffI2I：预训练和 DM 训练。对于预训练，GT 和输入图像被输入到 CPEN$_{S1}$S1 中，以捕获引导 DI2Iformer 的紧凑 I2I 先验表示 (IPR)。在第二阶段，DM 被训练为仅使用输入图像来估计与 CPEN$_{S1}$S1 相同的 IRP。与传统 DM 相比，紧凑的 IPR 使 DiffI2I 能够获得更准确的结果，并使用更轻的去噪网络和更少的迭代。通过对各种 I2I 任务的大量实验，我们证明 DiffI2I 实现了 SOTA 性能，同时显著降低了计算负担。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10752976</guid>
      <pubDate>Thu, 14 Nov 2024 13:16:50 GMT</pubDate>
    </item>
    <item>
      <title>Hi-SAM：结合任意分割模型实现分层文本分割</title>
      <link>http://ieeexplore.ieee.org/document/10750316</link>
      <description><![CDATA[任何文本分割模型 (SAM) 都是在大型数据集上预训练的深刻视觉基础模型，它突破了一般文本分割的界限，激发了各种下游应用。本文介绍了一种利用 SAM 进行分层文本分割的统一模型 Hi-SAM。Hi-SAM 在四个层级的分割方面表现出色，包括像素级文本、单词、文本行和段落，同时还实现布局分析。具体来说，我们首先通过一种参数高效的微调方法将 SAM 转变为高质量的像素级文本分割 (TS) 模型。我们使用此 TS 模型以半自动化的方式迭代生成像素级文本标签，统一 HierText 数据集中四个文本层级的标签。随后，利用这些完整的标签，我们基于 TS 架构和定制的分层掩码解码器推出端到端可训练的 Hi-SAM。在推理过程中，Hi-SAM 提供自动掩码生成 (AMG) 模式和可提示分割 (PS) 模式。在 AMG 模式下，Hi-SAM 首先分割像素级文本前景掩码，然后采样前景点以生成分层文本掩码，并顺便实现布局分析。至于 PS 模式，Hi-SAM 只需单击一下即可提供单词、文本行和段落掩码。实验结果显示了我们的 TS 模型的最优性能：对于像素级文本分割，Total-Text 上的 fgIOU 为 84.86%，TextSeg 上的 fgIOU 为 88.96%。此外，与之前针对 HierText 进行联合分层检测和布局分析的专家相比，Hi-SAM 取得了显著的改进：在文本行级别上 PQ 为 4.73% 和 F1 为 5.39%，在段落级别布局分析上 PQ 为 5.49% 和 F1 为 7.39%，所需的训练次数减少了 $20\times$20 倍。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10750316</guid>
      <pubDate>Mon, 11 Nov 2024 13:16:35 GMT</pubDate>
    </item>
    </channel>
</rss>