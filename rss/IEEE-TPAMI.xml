<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>IEEE 模式分析与机器智能学报 - 新目录</title>
    <link>http://ieeexplore.ieee.org</link>
    <description>出版物 # 34 的目录提醒</description>
    <lastBuildDate>Thu, 05 Sep 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>具有可训练激活函数的稀疏神经网络的贝叶斯优化</title>
      <link>http://ieeexplore.ieee.org/document/10496211</link>
      <description><![CDATA[在有关深度神经网络的文献中，人们对开发可以增强神经网络性能的激活函数非常感兴趣。近年来，科学界对提出可以在整个学习过程中进行训练的激活函数重新产生了兴趣，因为它们似乎可以提高网络性能，尤其是通过减少过度拟合。在本文中，我们提出了一个可训练的激活函数，其参数需要估计。开发了一个完全贝叶斯模型，可以自动从学习数据中估计模型权重和激活函数参数。开发了一种基于 MCMC 的优化方案来构建推理。所提出的方法旨在通过使用保证收敛到全局最大值的有效采样方案来解决上述问题并缩短收敛时间。所提出的方案已在涵盖分类和回归任务的各种数据集上进行了测试，并在各种 CNN 架构中实施，以证明其多功能性和有效性。有希望的结果表明，由于所提出的激活函数和参数的贝叶斯估计，我们提出的方法在提高模型准确性方面很有用。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10496211</guid>
      <pubDate>Wed, 10 Apr 2024 13:18:38 GMT</pubDate>
    </item>
    <item>
      <title>无需去噪即可表示噪声图像</title>
      <link>http://ieeexplore.ieee.org/document/10496213</link>
      <description><![CDATA[人工智能中一个长期存在的课题是从噪声图像中有效识别模式。在这方面，最近的数据驱动范式考虑 1) 通过在训练阶段添加噪声样本（即数据增强）来提高表示鲁棒性，或 2) 通过学习解决逆问题（即图像去噪）来预处理噪声图像。然而，这类方法通常表现出低效的过程和不稳定的结果，限制了它们的实际应用。在本文中，我们探索了一种非学习范式，旨在直接从噪声图像中得出鲁棒表示，而无需去噪作为预处理。在这里，噪声鲁棒表示被设计为 Radon 空间中的分数阶矩 (FMR)，还具有正交性和旋转不变性的有益特性。与早期的整数阶方法不同，我们的工作是一种更通用的设计，将这些经典方法作为特例，并且引入的分数阶参数提供了经典方法所不具备的时频分析能力。正式地，我们详细讨论了构建 FMR 的隐式和显式路径。我们提供了大量的模拟实验和强大的可视化应用来证明我们的 FMR 的独特性和实用性，尤其是在抗噪性、旋转不变性和时频可辨性方面。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10496213</guid>
      <pubDate>Wed, 10 Apr 2024 13:18:38 GMT</pubDate>
    </item>
    <item>
      <title>元不变性防御面向对未知对抗攻击的可推广鲁棒性</title>
      <link>http://ieeexplore.ieee.org/document/10494561</link>
      <description><![CDATA[尽管深度神经网络 (DNN) 模型为计算机视觉任务提供了高性能解决方案，但事实证明它极易受到对抗性攻击。当前的防御主要集中在已知攻击上，但对未知攻击的对抗性鲁棒性却被严重忽视。此外，常用的自适应学习和微调技术不适用于对抗性防御，因为它在部署时本质上是一个零样本问题。因此，为了应对这一挑战，我们提出了一种与攻击无关的防御方法，称为元不变性防御 (MID)。具体而言，从手动构建的攻击者池中随机抽取各种对抗性攻击组合，以构成针对未知攻击的不同防御任务，其中学生编码器受到多一致性蒸馏的监督，以通过元原理学习攻击不变特征。提出的 MID 有两个优点：1) 在良性和对抗性样本之间从像素、特征和预测级别进行全面蒸馏，有助于发现攻击不变性。 2）该模型同时实现了对高级图像分类中不可察觉的对抗性扰动的鲁棒性和低级鲁棒图像再生中的攻击抑制。在ImageNet等众多基准测试上的理论和实证研究验证了MID在各种攻击下的可泛化的鲁棒性和优越性。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10494561</guid>
      <pubDate>Mon, 08 Apr 2024 13:17:12 GMT</pubDate>
    </item>
    <item>
      <title>均值漂移的收敛性分析</title>
      <link>http://ieeexplore.ieee.org/document/10494563</link>
      <description><![CDATA[均值漂移 (MS) 算法寻求核密度估计 (KDE) 的模式。本研究借助关于 Łojasiewicz 不等式的论证，在相当温和的条件下，提出了 MS 算法生成的模式估计序列的收敛保证，并评估了收敛速度。我们的发现扩展了现有的涵盖解析核和 Epanechnikov 核的发现。这些发现很重要，因为它们涵盖了双权重核，就基于 KDE 的模式估计的渐近统计效率而言，双权重核在非负核中是最优的。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10494563</guid>
      <pubDate>Mon, 08 Apr 2024 13:17:12 GMT</pubDate>
    </item>
    <item>
      <title>受皮质-丘脑-皮质回路启发的视听语音分离模型</title>
      <link>http://ieeexplore.ieee.org/document/10487874</link>
      <description><![CDATA[涉及视觉输入的视听方法为语音分离的最新进展奠定了基础。然而，优化听觉和视觉输入的同时使用仍然是一个活跃的研究领域。受皮质-丘脑-皮质回路的启发，其中不同模态的感觉处理机制通过非丘系感觉丘脑相互调节，我们提出了一种用于视听语音分离 (AVSS) 的新型皮质-丘脑-皮质神经网络 (CTCNet)。首先，CTCNet 在单独的听觉和视觉子网络中以自下而上的方式学习分层听觉和视觉表征，模仿听觉和视觉皮质区域的功能。然后，受皮质区域和丘脑之间大量连接的启发，该模型通过自上而下的连接将丘脑子网络中的听觉和视觉信息融合在一起。最后，该模型将融合的信息传回听觉和视觉子网络，并重复上述过程多次。在三个语音分离基准数据集上的实验结果表明，CTCNet 在参数显著减少的情况下，性能显著优于现有的 AVSS 方法。这些结果表明，模仿哺乳动物大脑的解剖连接组对于促进深度神经网络的发展具有巨大的潜力。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10487874</guid>
      <pubDate>Tue, 02 Apr 2024 13:17:58 GMT</pubDate>
    </item>
    <item>
      <title>基于 Transformer 的多元图像补全，减少信息丢失</title>
      <link>http://ieeexplore.ieee.org/document/10488726</link>
      <description><![CDATA[基于 Transformer 的方法最近在图像修复方面取得了巨大成功。然而，我们发现这些解决方案将每个像素视为一个标记，因此在两个方面存在信息丢失问题：1）为了提高效率，它们将输入图像下采样为更低的分辨率。2）它们将 $256^{3}$2563 个 RGB 值量化为少量（例如 512）量化颜色值。量化像素的索引用作 Transformer 的输入和预测目标的标记。为了缓解这些问题，我们提出了一个基于 Transformer 的新框架“PUT”。具体来说，为了避免输入下采样同时保持计算效率，我们设计了一个基于块的自动编码器 P-VQVAE。编码器将蒙版图像转换为不重叠的块标记，解码器从修复的标记中恢复蒙版区域，同时保持未蒙版区域不变。为了消除输入量化造成的信息丢失，应用了未量化的 Transformer。它直接将 P-VQVAE 编码器中的特征作为输入，不进行任何量化，仅将量化后的标记视为预测目标。此外，为了使修复过程更易于控制，我们引入了语义和结构条件作为额外指导。大量实验表明，我们的方法在图像保真度方面大大优于现有的基于 Transformer 的方法，并且在复杂的大型数据集（例如 ImageNet）上实现了比最先进的多元修复方法更高的多样性和更好的保真度。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10488726</guid>
      <pubDate>Tue, 02 Apr 2024 13:17:58 GMT</pubDate>
    </item>
    <item>
      <title>图像恢复中深度展开方法的旋转等变近端算子</title>
      <link>http://ieeexplore.ieee.org/document/10487002</link>
      <description><![CDATA[深度展开方法在计算机视觉任务中引起了广泛关注，它很好地将传统的图像处理建模方式与最新的深度学习技术联系起来。具体而言，通过在每个实施步骤中的算法运算符与每层内的网络模块之间建立直接对应关系，可以合理地构建具有高解释性的近乎“白盒”的网络架构。在该架构中，只有近端运算符的预定义组件（称为近端网络）需要手动配置，使网络能够以数据驱动的方式自动提取固有图像先验。在当前的深度展开方法中，这种近端网络通常被设计为 CNN 架构，其必要性已被最近的理论证明。也就是说，CNN 结构实质上提供了平移对称图像先验，这是各种类型图像中最普遍拥有的结构先验。然而，标准的基于 CNN 的近端网络在捕捉旋转对称先验方面存在本质限制，这是一般图像的另一个通用结构先验。这为深度展开方法的进一步性能改进留下了很大的空间。为了解决这个问题，本研究致力于提出一种高精度旋转等变近端网络，将旋转对称先验有效地嵌入到深度展开框架中。特别是，我们首次推导出这种设计的近端网络在任意旋转度下的任意层的理论等变误差。这种分析应该是迄今为止此类误差评估最精细的理论结论，对于支持此类网络的内在可解释性要求背后的原理也是必不可少的。通过对盲图像超分辨率、医学图像重建和图像去雨等不同视觉任务的实验验证，验证了所提出的方法能够直接替换当前深度展开架构中的近端网络并轻松提高其最新性能。这表明它在一般视觉任务中具有潜在的可用性。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10487002</guid>
      <pubDate>Mon, 01 Apr 2024 15:01:25 GMT</pubDate>
    </item>
    <item>
      <title>具有双粒度标记的稳健多图多标签学习</title>
      <link>http://ieeexplore.ieee.org/document/10486204</link>
      <description><![CDATA[多图多标签学习 (Mgml) 旨在使用图袋表示对一组感兴趣的对象（例如文本或图像）进行分类。以前的 Mgml 研究有局限性，因为它们只能在袋级别学习标签，在将图转换为实例的学习中会丢失结构信息，并且无法处理嘈杂的标签。本文提出了一种强大的粗粒度和细粒度噪声多图多标签 (cfMGNML) 学习框架，该框架在图上构建学习模型，并支持在粗粒度（袋）和细粒度（每个袋中的图）级别对带有噪声标签的标签进行预测。为了识别标签噪声，定义了一个标签概率矩阵来作用于每个标签的评分函数，概率值越高，表示该标签越可能是相应的图或袋标签。使用标签概率矩阵上的流形约束对问题进行正则化，以保留数据中的局部关系并揭示其基本流形结构。同时，提出了一个阈值排序损失目标，用于对图和包的标签进行排序，并同时最小化汉明损失。为了解决非凸优化问题，开发了一种有效的次梯度下降算法。在各种数据集上的实验表明，所提出的方法比最先进的算法具有更好的性能。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10486204</guid>
      <pubDate>Fri, 29 Mar 2024 13:15:49 GMT</pubDate>
    </item>
    <item>
      <title>用于图像融合的任务引导、隐式搜索和元初始化深度模型</title>
      <link>http://ieeexplore.ieee.org/document/10480582</link>
      <description><![CDATA[图像融合在各种基于多传感器的视觉系统中起着关键作用，尤其是对于增强视觉质量和/或提取聚合特征以供感知。然而，大多数现有方法仅将图像融合视为一项单独的任务，从而忽略了它与这些下游视觉问题的根本关系。此外，设计适当的融合架构通常需要大量的工程劳动。它还缺乏提高当前融合方法的灵活性和泛化能力的机制。为了缓解这些问题，我们建立了一个任务引导、隐式搜索和元初始化 (TIM) 深度模型来解决具有挑战性的现实场景中的图像融合问题。具体来说，我们首先提出一种约束策略来整合来自下游任务的信息来指导图像融合的无监督学习过程。在这个框架内，我们设计了一个隐式搜索方案，以高效率自动发现我们的融合模型的紧凑架构。此外，引入了一种借口元初始化技术来利用发散融合数据来支持对不同类型的图像融合任务的快速适应。对不同类别的图像融合问题和相关下游任务（例如视觉增强和语义理解）的定性和定量实验结果证实了我们的 TIM 的灵活性和有效性。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10480582</guid>
      <pubDate>Wed, 27 Mar 2024 13:16:48 GMT</pubDate>
    </item>
    <item>
      <title>基于超图的事件相机多视图动作识别</title>
      <link>http://ieeexplore.ieee.org/document/10480584</link>
      <description><![CDATA[视频数据中的动作识别是具有广泛应用的基石。由于单视图动作识别依赖于单一视点，因此面临局限性。相比之下，多视图方法可以从各个视点捕获互补信息，以提高准确性。最近，事件摄像机已成为创新的生物启发式传感器，从而推动了基于事件的动作识别的发展。然而，现有的研究主要集中在单视图场景上，在多视图事件数据利用方面存在差距，特别是在信息缺失和语义错位等挑战方面。为了弥补这一差距，我们引入了 HyperMV，这是一个基于多视图事件的动作识别框架。HyperMV 使用共享卷积网络将离散事件数据转换为帧状表示并提取与视图相关的特征。通过将段视为顶点并使用基于规则和基于 KNN 的策略构建超边，建立了一个多视图超图神经网络，该网络可以捕获跨视点和时间特征的关系。还引入了顶点注意超图传播以增强特征融合。为了推动该领域的研究，我们提出了最大的基于事件的多视图动作数据集 $\mathbf{THU}^{\mathbf{MV-EACT}}\mathbf{-50}$THUMV-EACT-50，包含 6 个视角的 50 个动作，比现有数据集大十倍以上。实验结果表明，HyperMV 在跨主体和跨视图场景中的表现都显著优于基线，并且在基于帧的多视图动作识别方面也超过了最先进的水平。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10480584</guid>
      <pubDate>Wed, 27 Mar 2024 13:16:48 GMT</pubDate>
    </item>
    <item>
      <title>广义线性因果网络的联邦学习</title>
      <link>http://ieeexplore.ieee.org/document/10480288</link>
      <description><![CDATA[因果发现，即从数据中推断变量之间的因果关系，是科学的一个基本问题。如今，由于人们对数据隐私问题的认识不断提高，人们开始转向分布式数据收集、处理和存储。为了满足对分布式因果发现的迫切需求，我们提出了一种新的联合 DAG 学习方法，称为正则化似然得分上的分布式退火 (DARLS)，以从存储在多个客户端上的数据中学习因果图。DARLS 模拟退火过程以搜索拓扑排序空间，其中通过分布式优化找到与排序兼容的最佳图形结构。这种分布式优化依赖于本地客户端和中央服务器之间的多轮通信来估计图形结构。我们建立了它与可以访问所有数据的 oracle 获得的解决方案的收敛性。据我们所知，DARLS 是第一个具有此类有限样本 oracle 保证的学习因果图的分布式方法。为了建立 DARLS 的一致性，我们还推导出了广义线性模型参数化的因果图的新可识别性结果，这可能具有独立意义。通过广泛的模拟研究和实际应用，我们表明 DARLS 优于现有的联邦学习方法，并且在汇集数据上可与 oracle 方法相媲美，证明了其在从分布式数据估计因果网络方面的巨大优势。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10480288</guid>
      <pubDate>Tue, 26 Mar 2024 13:16:45 GMT</pubDate>
    </item>
    <item>
      <title>AutoNet 生成的深层逐层凸网络用于心电图分类</title>
      <link>http://ieeexplore.ieee.org/document/10477593</link>
      <description><![CDATA[神经网络的设计通常需要反复试验，这是一个耗时的过程，即使对于经验丰富的研究人员来说也是如此。此外，人们普遍认为，深度神经网络的损失函数通常相对于要优化的参数是非凸的。我们提出了分层凸定理，以确保损失相对于给定层的参数是凸的，通过将每一层约束为非线性方程组的超定理来实现。基于此定理，我们开发了一种端到端算法 (AutoNet)，可为任何给定的训练集自动生成分层凸网络 (LCN)。然后，我们在三个心电图 (ECG) 分类基准数据集上展示了 AutoNet 生成的 LCN (AutoNet-LCN) 与最先进模型相比的性能，并在两个非 ECG 基准数据集上进一步验证了更一般的任务。 AutoNet-LCN 能够在 2 个 GPU 小时内找到针对每个数据集定制的网络，无需手动微调，并且生成的网络在上述所有五个基准数据集上的表现均优于参数少于 5% 的最先进的模型。AutoNet-LCN 的效率和稳健性显著降低了模型发现成本，并能够在资源受限的环境中高效训练深度学习模型。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10477593</guid>
      <pubDate>Thu, 21 Mar 2024 13:18:45 GMT</pubDate>
    </item>
    <item>
      <title>基于角度回归的任意方向物体检测中的边界不连续性</title>
      <link>http://ieeexplore.ieee.org/document/10475581</link>
      <description><![CDATA[随着自动驾驶和遥感等领域的蓬勃发展，有向物体检测逐渐成为主流。现有的大多数方法直接对旋转角度进行回归，我们认为这种方法存在边界不连续性的根本限制（即使使用高斯或基于 RotatedIoU 的损失）。本文提出了一种新型的角度编码器，即相移编码器 (PSC) 来解决这一问题。与另一种广为人知的替代方案（即角度分类）不同，PSC 以连续可微的方式实现无边界不连续性，因此可以与基于高斯或 RotatedIoU 的方法协同工作，进一步提高其性能。此外，通过将细长和方形物体的边界不连续性重新视为不同周期的旋转对称性，提出了一种双频版本 (PSCD) 来准确预测这两种物体的方向。对几种流行的骨干检测器和数据集的视觉分析和大量实验证明了我们方法的有效性和潜力。当面对需要高质量边界框的场景时，预计所提出的方法将具有竞争力的性能。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10475581</guid>
      <pubDate>Tue, 19 Mar 2024 13:20:24 GMT</pubDate>
    </item>
    <item>
      <title>利用零成本代理引导演化的免训练 Transformer 架构搜索</title>
      <link>http://ieeexplore.ieee.org/document/10475573</link>
      <description><![CDATA[Transformer 已经展现出卓越的性能，然而其架构设计是一个耗时的过程，需要专业知识和反复试验。因此，研究通过 Transformer 架构搜索（TAS）自动搜索高性能 Transformer 的有效方法非常值得。为了提高搜索效率，无需训练的基于代理的方法已被广泛用于神经架构搜索（NAS）。然而，这些代理被发现不能很好地推广到 Transformer 搜索空间，这一点已被多项研究和我们自己的实验证实。本文提出了一种有效的 TAS 方案，即零成本代理引导进化的 Transformer 架构搜索（T-Razor），该方案可实现出色的效率。首先，通过理论分析，我们发现多头自注意力（MSA）的突触多样性和多层感知器（MLP）的显着性与相应 Transformer 的性能相关。突触多样性和突触显著性的特性促使我们引入突触多样性和突触显著性的等级（表示为 DSS++）来评估和排名 Transformer。DSS++ 结合了采样 Transformer 之间的相关性信息，为突触多样性和突触显著性提供统一的分数。然后，我们提出了一种由 DSS++ 引导的分块进化搜索来寻找最优 Transformer。DSS++ 确定突变和交叉的位置，增强了探索能力。实验结果表明，我们的 T-Razor 在四个流行的 Transformer 搜索空间中的表现与最先进的手动或自动设计的 Transformer 架构相比具有竞争力。显著的是，T-Razor 提高了不同 Transformer 搜索空间中的搜索效率，例如，将所需的 GPU 天数从 24 天以上减少到 0.4 天以下，并且优于现有的零成本方法。我们还将 T-Razor 应用于 BERT 搜索空间，发现搜索到的 Transformers 在多个神经语言处理 (NLP) 数据集上取得了具有竞争力的 GLUE 结果。这项工作为无需训练的 TAS 提供了见解，揭示了根据不同块的属性评估 Transformers 的实用性。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10475573</guid>
      <pubDate>Tue, 19 Mar 2024 13:20:24 GMT</pubDate>
    </item>
    <item>
      <title>PAGE：基于原型的图神经网络模型级解释</title>
      <link>http://ieeexplore.ieee.org/document/10475563</link>
      <description><![CDATA[除了图神经网络 (GNN) 作为一个彻底改变图表示学习的强大框架而受到广泛关注之外，对解释 GNN 模型的需求也日益增加。尽管已经开发了各种 GNN 的解释方法，但大多数研究都集中在实例级解释上，这些解释会针对给定的图实例产生量身定制的解释。在我们的研究中，我们提出了基于原型的 GNN 解释器 (${\sf PAGE}$PAGE)，这是一种新颖的模型级 GNN 解释方法，它通过发现人类可解释的原型图来解释底层 GNN 模型为图分类学到了什么。我们的方法可以为给定的类产生解释，因此能够提供比实例级解释更简洁、更全面的解释。首先，${\sf PAGE}$PAGE 在对类判别输入图进行聚类后，在图级嵌入空间上选择它们的嵌入。然后，${\sf PAGE}$PAGE 通过原型评分函数使用节点级嵌入迭代搜索高匹配节点元组来发现常见的子图模式，从而产生原型图作为我们的解释。使用六个图分类数据集，我们证明 ${\sf PAGE}$PAGE 在质量和定量上都优于最先进的模型级解释方法。我们还通过展示 ${\sf PAGE}$PAGE 与实例级解释方法之间的关系、${\sf PAGE}$PAGE 对输入数据稀缺环境的鲁棒性以及 ${\sf PAGE}$PAGE 中提出的原型评分函数的计算效率进行了系统的实验研究。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10475563</guid>
      <pubDate>Tue, 19 Mar 2024 13:20:24 GMT</pubDate>
    </item>
    </channel>
</rss>