<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>IEEE 模式分析与机器智能学报 - 新目录</title>
    <link>http://ieeexplore.ieee.org</link>
    <description>出版物 # 34 的目录提醒</description>
    <lastBuildDate>Tue, 06 Aug 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>NeRF-Texture：合成神经辐射场纹理</title>
      <link>http://ieeexplore.ieee.org/document/10489854</link>
      <description><![CDATA[纹理合成是计算机图形学中的一个基本问题，它将使各种应用受益。现有方法在处理二维图像纹理方面是有效的。相比之下，许多现实世界的纹理在三维几何空间中包含中观结构，例如草、树叶和织物，仅使用二维图像纹理无法有效地对其进行建模。我们提出了一种新的纹理合成方法，使用神经辐射场 (NeRF) 来捕获和合成给定多视角图像中的纹理。在提出的 NeRF 纹理表示中，具有精细几何细节的场景被分解为中观结构纹理和底层基本形状。这使得具有中观结构的纹理可以有效地作为位于基本形状上的潜在特征进行学习，这些特征被输入到同时训练的 NeRF 解码器中，以表示丰富的视图相关外观。使用这种隐式表示，我们可以通过潜在特征的块匹配来合成基于 NeRF 的纹理。然而，重建内容空间和潜在特征空间的度量之间的不一致可能会影响合成质量。为了提高匹配性能，我们通过加入聚类约束进一步规范了潜在特征的分布。除了在平面域上生成 NeRF 纹理外，我们的方法还可以在曲面上合成 NeRF 纹理，这在实际中很有用。实验结果和评估证明了我们方法的有效性。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10489854</guid>
      <pubDate>Tue, 02 Apr 2024 13:17:58 GMT</pubDate>
    </item>
    <item>
      <title>低光视觉的无监督照明适应</title>
      <link>http://ieeexplore.ieee.org/document/10480646</link>
      <description><![CDATA[光照不足对人类和机器视觉分析都构成了挑战。虽然现有的低光增强方法优先考虑人类视觉感知，但它们往往忽略了机器视觉和高级语义。在本文中，我们做出了开创性的努力，为高级视觉构建了一个光照增强模型。从相机响应函数中汲取灵感，我们的模型可以从机器视觉的角度增强图像，尽管其架构轻量且公式简单。我们还介绍了两种方法，利用基础增强曲线和自监督借口任务中的知识来训练不同的下游正常到低光适应场景。我们提出的框架克服了现有算法的局限性，而无需在低光条件下访问标记数据。它有助于更​​有效的照明恢复和特征对齐，以即插即用的方式显着提高下游任务的性能。这项研究推动了低光机器分析领域的发展，广泛应用于各种高级视觉任务，包括分类、人脸检测、光流估计和视频动作识别。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10480646</guid>
      <pubDate>Wed, 27 Mar 2024 13:16:48 GMT</pubDate>
    </item>
    <item>
      <title>利用胸部解剖结构一致性进行射线图像中的无监督异常检测</title>
      <link>http://ieeexplore.ieee.org/document/10480307</link>
      <description><![CDATA[放射成像协议专注于特定的身体区域，因此会产生非常相似的图像，并在患者中产生重复的解剖结构。利用这种结构化信息可能会简化从放射图像中检测异常的过程。为此，我们提出了一种用于修复和检测放射图像异常的简单空间感知记忆矩阵（缩写为 SimSID）。我们将异常检测制定为图像重建任务，由空间感知记忆矩阵和特征空间中的修复块组成。在训练过程中，SimSID 可以将根深蒂固的解剖结构分类为重复的视觉模式，并且在推理过程中，它可以从测试图像中识别异常（看不见/修改过的视觉模式）。我们的 SimSID 在无监督异常检测方面分别以 +8.0%、+5.0% 和 +9.9% 的 AUC 得分超越了最先进的水平。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10480307</guid>
      <pubDate>Tue, 26 Mar 2024 13:16:45 GMT</pubDate>
    </item>
    <item>
      <title>基于梯度的实例特定视觉解释，用于对象规范和对象辨别</title>
      <link>http://ieeexplore.ieee.org/document/10478163</link>
      <description><![CDATA[我们提出了梯度加权物体检测器激活图 (ODAM)，这是一种用于解释物体检测器预测的视觉解释技术。利用流入中间特征图的检测器目标的梯度，ODAM 生成热图，显示区域对检测器对每个预测属性的决策的影响。与以前的分类激活图 (CAM) 工作相比，ODAM 生成特定于实例的解释而不是特定于类的解释。我们表明 ODAM 适用于具有不同类型的检测器骨干和头部的单阶段、双阶段和基于变压器的检测器，并且在有效性和效率方面产生比最新技术更高质量的视觉解释。我们讨论了物体检测的两个解释任务：1）物体指定：预测的重要区域是什么？2）物体识别：检测到哪个物体？针对这两个方面，我们对检测器的视觉解释进行了详细分析，并进行了广泛的实验以验证所提出的 ODAM 的有效性。此外，我们研究了用户对解释图的信任度，通过人眼注视来衡量物体检测器的视觉解释与人类解释的一致性，以及这种一致性是否与用户信任有关。最后，我们还基于 ODAM 的这两种能力提出了两个应用，ODAM-KD 和 ODAM-NMS。ODAM-KD 利用 ODAM 的对象规范来对关键预测产生自上而下的注意力并指导物体检测的知识提炼。ODAM-NMS 考虑模型对每个预测的解释位置以区分重复检测到的对象。提出了一种训练方案 ODAM-Train，以提高物体识别的质量，并辅助 ODAM-NMS。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10478163</guid>
      <pubDate>Fri, 22 Mar 2024 13:17:15 GMT</pubDate>
    </item>
    <item>
      <title>MVEB：具有多视图熵瓶颈的自监督学习</title>
      <link>http://ieeexplore.ieee.org/document/10477543</link>
      <description><![CDATA[自监督学习旨在学习可以有效推广到下游任务的表示。许多自监督方法将图像的两个视图视为输入和自监督信号，假设任一视图都包含相同的任务相关信息，并且共享信息（大约）足以预测下游任务。最近的研究表明，丢弃视图之间不共享的多余信息可以提高泛化能力。因此，理想的表示足以完成下游任务，并且包含最少的多余信息，称为最小充分表示。人们可以通过最大化表示和监督视图之间的互信息同时消除多余信息来学习这种表示。然而，互信息的计算是出了名的难以解决。在这项工作中，我们提出了一个称为多视图熵瓶颈（MVEB）的目标来有效地学习最小充分表示。MVEB 将最小充分学习简化为最大化两个视图的嵌入之间的一致性和嵌入分布的差分熵。我们的实验证实，MVEB 显著提高了性能。例如，它在线性评估中使用原始 ResNet-50 主干在 ImageNet 上实现了 76.9% 的 top-1 准确率。据我们所知，这是 ResNet-50 的最新最佳结果。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10477543</guid>
      <pubDate>Thu, 21 Mar 2024 13:18:45 GMT</pubDate>
    </item>
    <item>
      <title>将 CLIP 模型转变为场景文本识别器</title>
      <link>http://ieeexplore.ieee.org/document/10476714</link>
      <description><![CDATA[我们利用大规模对比语言-图像预训练 (CLIP) 模型的潜力来增强场景文本检测和识别任务，将其转变为强大的主干模型 FastTCM-CR50。该主干模型利用 CLIP 中的视觉提示学习和交叉注意来提取基于图像和文本的先验知识。使用预定义和可学习的提示，FastTCM-CR50 引入了实例语言匹配过程来增强图像和文本嵌入之间的协同作用，从而细化文本区域。我们的双峰相似性匹配 (BSM) 模块有助于动态语言提示生成，实现离线计算并提高性能。FastTCM-CR50 具有以下几个优点：1) 它可以增强现有的文本检测器和识别器，分别将性能平均提高 1.6% 和 1.5%。 2) 它的表现优于之前的 TCM-CR50 主干，在文本检测和识别任务中平均提升了 0.2% 和 0.55%，推理速度提高了 47.1%。3) 它展示了强大的少样本训练能力。仅使用 10% 的监督数据，FastTCM-CR50 在文本检测和识别任务中的平均性能分别提高了 26.5% 和 4.7%。4) 它持续提升了分布外文本检测和识别数据集上的性能，尤其是 ICDAR2019-ArT 中的 NightTime-ArT 子集和用于定向物体检测的 DOTA 数据集。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10476714</guid>
      <pubDate>Wed, 20 Mar 2024 13:22:19 GMT</pubDate>
    </item>
    <item>
      <title>用于少样本细粒度分类的双向集成特征重构网络</title>
      <link>http://ieeexplore.ieee.org/document/10472065</link>
      <description><![CDATA[细粒度少样本图像分类的主要挑战是使用少量标记样本来学习具有较高类间差异和较低类内差异的特征表示。然而，传统的少样本学习方法不能简单地用于这种细粒度设置——一项快速的初步研究表明，它们实际上推动了相反的结果（即较低的类间差异和较高的类内差异）。为了缓解这个问题，先前的研究主要使用支持集来重建查询图像，然后利用度量学习来确定其类别。经过仔细检查，我们进一步发现，这种单向重建方法仅有助于增加类间差异，而无法有效解决类内差异。在本文中，我们引入了一种可以同时适应类间和类内差异的双重建机制。除了使用支持集重建查询集以增加类间差异之外，我们还使用查询集重建支持集以减少类内差异。这种设计有效地帮助模型探索更微妙和更具判别性的特征，这对于手头的细粒度问题至关重要。此外，我们还构建了一个自重构模块，与双向模块一起工作，使特征更具判别性。我们在情景学习策略中引入了快照集成方法——这是一个简单的技巧，可以在不增加训练成本的情况下进一步提高模型性能。在三个广泛使用的细粒度图像分类数据集以及一般和跨域小样本图像数据集上的实验结果与其他方法相比始终显示出显着的改进。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10472065</guid>
      <pubDate>Mon, 18 Mar 2024 13:20:16 GMT</pubDate>
    </item>
    <item>
      <title>FedCut：用于可靠检测拜占庭共谋者的频谱分析框架</title>
      <link>http://ieeexplore.ieee.org/document/10465649</link>
      <description><![CDATA[本文提出了一个通用的谱分析框架，用于阻止联邦学习中由恶意拜占庭攻击者或合谋者群体造成的安全风险，他们合谋上传恶意模型更新，严重降低全局模型性能。所提出的框架从谱分析的角度描述了拜占庭合谋者模型更新之间的强一致性和时间连贯性，并将拜占庭不当行为的检测表述为加权图中的社区检测问题。然后使用修改后的正则化图切割来辨别攻击者和良性参与者。此外，采用谱启发式方法使检测对各种攻击具有鲁棒性。所提出的拜占庭合谋者弹性方法，即 FedCut，保证以有界误差收敛。在各种设置下进行的大量实验结果证明了 FedCut 的优越性，它在各种攻击下都表现出极其稳健的模型精度 (MA)。结果表明，FedCut 的平均 MA 比最先进的拜占庭弹性方法高出 2.1% 至 16.5%。就最坏情况模型准确率 (M​​A) 而言，FedCut 比这些方法高出 17.6% 至 69.5%。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10465649</guid>
      <pubDate>Thu, 14 Mar 2024 13:17:20 GMT</pubDate>
    </item>
    <item>
      <title>针对神经增强自适应直播的实时 CNN 训练和压缩</title>
      <link>http://ieeexplore.ieee.org/document/10472651</link>
      <description><![CDATA[我们提出了一种实时卷积神经网络 (CNN) 训练和压缩方法，即使在较差的网络环境下也能提供高质量的直播视频。服务器提供低分辨率视频片段以及相应的 CNN 以实现超分辨率 (SR)，之后客户端将 CNN 应用于该片段以恢复高分辨率视频帧。为了实时生成与视频片段相对应的训练好的 CNN，我们的方法通过提升 CNN 的过拟合特性并使用基于课程的训练来快速提高训练准确率。此外，假设预训练的 CNN 已在客户端下载，我们仅传输更新后的 CNN 参数和预训练后的 CNN 参数之间的残差值。这些值可以实时用低位量化，同时最大限度地减少损失，因为分布范围明显窄于更新后的 CNN。从数量上看，与最先进的 CNN 训练和压缩方法相比，我们的神经增强自适应直播流水线 (NEALS) 在有限的训练时间内实现了更高的 SR 准确度和更低的 CNN 压缩损失率。与最先进的神经增强直播系统相比，NEALS 的用户体验质量提高了 15% 至 48%。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10472651</guid>
      <pubDate>Thu, 14 Mar 2024 13:17:20 GMT</pubDate>
    </item>
    <item>
      <title>用于动作检测的语义和运动感知时空变换网络</title>
      <link>http://ieeexplore.ieee.org/document/10472872</link>
      <description><![CDATA[本文提出了一种新颖的时空变换网络，该网络引入了几个原始组件来检测未修剪视频中的动作。首先，多特征选择性语义注意模型计算空间和运动特征之间的相关性，以正确模拟不同动作语义之间的时空相互作用。其次，运动感知网络利用运动感知二维位置编码算法对视频帧中动作语义的位置进行编码。这种运动感知机制可以记住当前方法无法利用的动作帧中的动态时空变化。第三，基于序列的时间注意模型可以捕获动作帧中的异构时间依赖性。与自然语言处理中使用的标准时间注意（主要用于寻找语言单词之间的相似性）不同，所提出的基于序列的时间注意旨在确定共同定义动作含义的视频帧之间的差异和相似之处。所提出的方法在四个时空动作数据集上的表现优于最先进的解决方案：AVA 2.2、AVA 2.1、UCF101-24 和 EPIC-Kitchens。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10472872</guid>
      <pubDate>Thu, 14 Mar 2024 13:17:20 GMT</pubDate>
    </item>
    <item>
      <title>深度学习在视觉语音分析中的应用：综述</title>
      <link>http://ieeexplore.ieee.org/document/10472054</link>
      <description><![CDATA[视觉语音是指语音的视觉领域，由于其广泛的应用，如公共安全、医疗、军事防御和电影娱乐，而受到越来越多的关注。作为一种强大的人工智能策略，深度学习技术极大地促进了视觉语音学习的发展。在过去的五年中，已经提出了许多基于深度学习的方法来解决该领域的各种问题，尤其是自动视觉语音识别和生成。为了推动未来视觉语音的研究，本文将全面回顾深度学习方法在视觉语音分析方面的最新进展。我们涵盖了视觉语音的不同方面，包括基本问题、挑战、基准数据集、现有方法的分类和最新性能。此外，我们还指出了当前研究中的差距并讨论了鼓舞人心的未来研究方向。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10472054</guid>
      <pubDate>Wed, 13 Mar 2024 13:18:43 GMT</pubDate>
    </item>
    <item>
      <title>使用 Transformer 探索 Point-BEV 融合以实现 3D 点云对象跟踪</title>
      <link>http://ieeexplore.ieee.org/document/10460168</link>
      <description><![CDATA[随着 LiDAR 传感器在自动驾驶中的广泛应用，3D 点云物体追踪受到越来越多的关注。在点云序列中，3D 物体追踪旨在预测物体在连续帧中的位置和方向。受 Transformer 成功的启发，我们提出了点跟踪变换器 (PTTR)，它借助 Transformer 操作以由粗到细的方式有效地预测高质量的 3D 追踪结果。PTTR 包含三种新颖的设计。1) 我们设计了关系感知采样来在子采样期间保留与给定模板相关的点，而不是随机采样。2) 我们提出了一种点关系变换器，用于模板和搜索区域之间的有效特征聚合和特征匹配。3) 基于粗跟踪结果，我们采用一种新颖的预测细化模块，通过局部特征池化获得最终的细化预测。此外，受点云鸟瞰图 (BEV) 在捕捉物体运动方面的良好特性的启发，我们进一步设计了一个更先进的框架 PTTR++，该框架结合了逐点视图和 BEV 表示，以利用它们的互补作用来生成高质量的跟踪结果。PTTR++ 在 PTTR 的基础上大幅提升了跟踪性能，且计算开销较低。在多个数据集上进行的大量实验表明，我们提出的方法实现了卓越的 3D 跟踪精度和效率。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10460168</guid>
      <pubDate>Tue, 05 Mar 2024 13:17:00 GMT</pubDate>
    </item>
    <item>
      <title>有符号累积分布变换空间中的端到端信号分类</title>
      <link>http://ieeexplore.ieee.org/document/10457552</link>
      <description><![CDATA[本文提出了一种基于有符号累积分布变换 (SCDT) 的端到端信号分类新方法。我们采用传输生成模型来定义分类问题。然后，我们利用 SCDT 的数学特性使问题在变换域中变得更容易，并使用 SCDT 域中的最近局部子空间 (NLS) 搜索算法来解决未知样本的类别。实验表明，与现有的端到端分类方法相比，所提出的方法提供了高精度的分类结果，同时计算成本低、数据效率高，并且对分布外样本具有鲁棒性。所提出方法的 Python 语言实现已集成为软件包 PyTransKit [1] 的一部分。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10457552</guid>
      <pubDate>Fri, 01 Mar 2024 13:16:20 GMT</pubDate>
    </item>
    <item>
      <title>长尾视觉识别的概率对比学习</title>
      <link>http://ieeexplore.ieee.org/document/10444057</link>
      <description><![CDATA[长尾分布经常出现在现实世界的数据中，其中大量的少数类别包含有限数量的样本。这种不平衡问题严重损害了主要为平衡训练集而设计的标准监督学习算法的性能。最近的研究表明，监督对比学习在缓解数据不平衡方面表现出良好的潜力。然而，监督对比学习的性能受到一个固有挑战的困扰：它需要足够大批量的训练数据来构建覆盖所有类别的对比对，然而这一要求在类别不平衡的数据环境中很难满足。为了克服这个障碍，我们提出了一种新的概率对比（ProCo）学习算法，该算法估计特征空间中每个类样本的数据分布，并据此抽样对比对。事实上，使用小批量的特征来估计所有类别的分布是不可行的，特别是对于不平衡的数据。我们的关键思想是引入一个合理而简单的假设，即对比学习中的归一化特征在单位空间上遵循 von Mises-Fisher (vMF) 分布的混合，这带来双重好处。首先，可以仅使用第一个样本矩来估计分布参数，并且可以在不同批次之间以在线方式高效计算。其次，基于估计的分布，vMF 分布使我们能够对无限数量的对比对进行采样，并推导出预期对比损失的封闭形式，以实现有效优化。除了长尾问题之外，ProCo 还可以直接应用于半监督学习，通过为未标记数据生成伪标签，随后可以利用这些伪标签来反向估计样本的分布。从理论上讲，我们分析了 ProCo 的误差界限。从经验上讲，在监督/半监督视觉识别和物体检测任务上的大量实验结果表明，ProCo 在各种数据集上始终优于现有方法。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10444057</guid>
      <pubDate>Fri, 23 Feb 2024 13:16:24 GMT</pubDate>
    </item>
    <item>
      <title>HyperSOR：用于显著对象排名的上下文感知图形超网络</title>
      <link>http://ieeexplore.ieee.org/document/10443257</link>
      <description><![CDATA[显著性物体排名 (SOR) 旨在根据人类对不同物体的注意力转移，分割图像中的显著性物体并同时预测它们的显著性排名。现有的 SOR 方法主要关注基于物体的注意力，例如物体的语义和外观。然而，我们发现场景背景在 SOR 中起着至关重要的作用，其中同一物体的显著性排名在不同场景中差异很大。因此，在本文中，我们首次尝试明确学习 SOR 的场景背景。具体来说，我们建立了一个包含 24,373 张图像的大规模 SOR 数据集，其中包含丰富的上下文注释，即场景图、分割和显著性排名。受我们数据集上的数据分析的启发，我们提出了一种用于上下文感知 SOR 的新型图超网络，名为 HyperSOR。在 HyperSOR 中，开发了一个初始图模块来分割对象并构建初始图，同时考虑几何和语义信息。然后，设计一个具有多路径图注意机制的场景图生成模块，以基于初始图学习对象之间的语义关系。最后，显著性排名预测模块通过新颖的图超网络动态采用学习到的场景上下文，以推断显著性排名。实验结果表明，我们的 HyperSOR 可以显著提高 SOR 的性能。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10443257</guid>
      <pubDate>Wed, 21 Feb 2024 13:16:49 GMT</pubDate>
    </item>
    </channel>
</rss>