<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>IEEE 模式分析与机器智能学报 - 新目录</title>
    <link>http://ieeexplore.ieee.org</link>
    <description>出版物 # 34 的目录提醒</description>
    <lastBuildDate>Wed, 06 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>双粒度轻量策略</title>
      <link>http://ieeexplore.ieee.org/document/10713325</link>
      <description><![CDATA[在模型训练之前去除冗余参数和计算引起了人们的极大兴趣，因为它可以有效减少模型的存储空间，加快模型的训练和推理速度，并节省模型运行过程中的能耗。此外，深度神经网络模型的简化可以使高性能的网络模型部署到资源受限的边缘设备，从而促进智能世界的发展。然而，现有的初始化剪枝方法在极端稀疏性下表现不佳。为了提高模型在极端稀疏性下的性能，该论文提出了一种双粒度的轻量级策略——TEDEPR。这是TEDEPR首次将张量理论引入初始化剪枝方法中，以优化稀疏子网络模型的结构并提高其性能。具体而言，首先，在粗粒度层面，我们将模型的权重矩阵或权重张量表示为低秩张量分解形式，并采用多步骤链式操作增强基模块的特征提取能力，构建低秩紧凑网络模型。其次，在模型训练之前，根据低秩模型中权重的可训练性，在细粒度层面对不重要的权重进行剪枝，得到最终的压缩模型。为了评估TEDEPR的优越性，我们使用LeNet、LSTM、VGGNet、ResNet和Transformer架构在MNIST、UCF11、CIFAR-10、CIFAR-100、Tiny-ImageNet和ImageNet数据集上进行了大量的实验，并与最新方法进行了比较。实验结果表明，在极端稀疏性下，TEDEPR比其他初始化时剪枝方法具有更高的准确率、更快的训练和推理速度以及更少的存储空间。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10713325</guid>
      <pubDate>Thu, 10 Oct 2024 13:16:13 GMT</pubDate>
    </item>
    <item>
      <title>EuroCity Persons 2.0：庞大而多样化的交通人员数据集</title>
      <link>http://ieeexplore.ieee.org/document/10703187</link>
      <description><![CDATA[我们介绍了 EuroCity Persons (ECP) 2.0 数据集，这是一种用于交通中行人检测、跟踪和预测的新型图像数据集。该数据集是在一辆行驶于 11 个欧洲国家/地区的 29 个城市的车辆上收集的。它包含超过 25 万条独特的人物轨迹、超过 200 万张图像，大小为 11 TB。ECP2.0 比之前最先进的汽车场景人物数据集大约大一个数量级。它在地理覆盖范围、一天中的时间、天气和季节方面提供了显著的多样性。我们讨论了一种新颖的半监督方法，该方法用于从关键帧处的稀疏手动注释中生成时间密集的伪地面实况（即 2D 边界框、3D 人物位置）。我们的方法利用辅助 LiDAR 数据进行 3D 提升，并利用车辆惯性感应进行自我运动补偿。它采用三阶段方法（轨迹生成、轨迹合并到轨迹中、轨迹平滑）整合关键帧信息，以获得准确的人物轨迹。我们在消融研究中验证了我们的伪地面实况生成方法，并表明它明显优于现有方法。此外，我们展示了它对训练和测试最先进的跟踪方法的好处。与逐帧手动注释相比，我们的方法提供了大约 34 倍的速度提升。ECP2.0 数据集可免费用于非商业研究用途。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10703187</guid>
      <pubDate>Wed, 02 Oct 2024 13:16:30 GMT</pubDate>
    </item>
    <item>
      <title>DeepTensor：具有深度网络先验的低秩张量分解</title>
      <link>http://ieeexplore.ieee.org/document/10652890</link>
      <description><![CDATA[DeepTensor 是一个计算效率高的框架，使用深度生成网络对矩阵和张量进行低秩分解。我们将张量分解为低秩张量因子的乘积（例如，将矩阵分解为两个向量的外积），其中每个低秩张量由深度网络 (DN) 生成，该网络以自监督方式训练以最小化均方近似误差。我们的主要观察是，DN 中固有的隐式正则化使它们能够捕获非线性信号结构（例如流形），而这些结构是奇异值分解 (SVD) 和主成分分析 (PCA) 等经典线性方法无法企及的。此外，与 SVD 和 PCA 相比，当张量的条目偏离加性高斯白噪声时，它们的性能会下降，而我们证明 DeepTensor 的性能对各种分布都具有鲁棒性。我们通过探索一系列实际应用（包括高光谱图像去噪、3D MRI 断层扫描和图像分类）验证了 DeepTensor 是 SVD、PCA、非负矩阵分解 (NMF) 和类似分解的稳健且计算效率高的直接替代品。具体而言，对于受泊松噪声破坏的信号，DeepTensor 比标准去噪方法的信噪比提高了 6 dB，并且学习分解 3D 张量的速度比配备 3D 卷积的单个 DN 快 60 倍。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10652890</guid>
      <pubDate>Tue, 27 Aug 2024 13:15:48 GMT</pubDate>
    </item>
    <item>
      <title>参与 3D 人体康复</title>
      <link>http://ieeexplore.ieee.org/document/10652891</link>
      <description><![CDATA[基于图像和视频的 3D 人体恢复（即姿势和形状估计）已取得实质性进展。然而，由于动作捕捉成本过高，现有数据集的规模和多样性往往有限。在这项工作中，我们通过使用自动注释的 3D 基本事实玩视频游戏来获得大量人体序列。具体来说，我们贡献了 GTA-Human，这是一个使用 GTA-V 游戏引擎生成的大型 3D 人体数据集，具有高度多样化的主题、动作和场景。更重要的是，我们研究了游戏数据的使用并获得了五个主要见解。首先，游戏数据出人意​​料地有效。在 GTA-Human 上训练的简单基于帧的基线比更复杂的方法表现得更好。对于基于视频的方法，GTA-Human 甚至与域内训练集不相上下。其次，我们发现合成数据为通常在室内收集的真实数据提供了关键补充。我们强调，对领域差距的研究为我们简单而实用的数据混合策略提供了解释，为研究界提供了新的见解。第三，数据集的规模很重要。性能提升与可用的额外数据密切相关。对多个关键因素（如相机角度和身体姿势）的系统研究表明，模型性能对数据密度很敏感。第四，GTA-Human 的有效性还归功于丰富的强监督标签（SMPL 参数），而在真实数据集中获取这些标签的成本很高。第五，合成数据的好处扩展到更大的模型，如更深的卷积神经网络（CNN）和 Transformers，我们也观察到了它们的巨大影响。我们希望我们的工作能够为将 3D 人体恢复扩展到现实世界铺平道路。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10652891</guid>
      <pubDate>Tue, 27 Aug 2024 13:15:48 GMT</pubDate>
    </item>
    <item>
      <title>Fast-Vid2Vid++：用于实时视频到视频合成的时空蒸馏</title>
      <link>http://ieeexplore.ieee.org/document/10652893</link>
      <description><![CDATA[视频到视频合成 (Vid2Vid) 在从一系列语义图（例如分割、草图和姿势）生成照片般逼真的视频方面取得了显著的成绩。然而，这种流程严重受限于高计算成本和长推理延迟，主要归因于两个基本因素：1）网络架构参数，2）顺序数据流。最近，基于图像的生成模型的参数已通过更高效的网络架构显著减少。现有方法主要侧重于精简网络架构，但忽略了顺序数据流的大小。此外，由于缺乏时间连贯性，基于图像的压缩不足以完成视频任务的压缩。在本文中，我们提出了一个时空混合蒸馏压缩框架 Fast-Vid2Vid++，该框架侧重于教师网络和生成模型在空间和时间上的数据流的知识蒸馏。Fast-Vid2Vid++ 首次尝试在时间维度上传输分层特征和时间连贯性知识，以减少计算资源并加速推理。具体来说，我们在空间上压缩数据流并减少时间冗余。我们在高分辨率和全时域中将分层特征和最终响应的知识从教师网络提炼到学生网络。我们将特征和视频帧的长期依赖关系转移到学生模型。经过提出的时空混合知识提炼 (Spatial-Temporal-HKD)，我们的模型可以使用低分辨率数据流合成高分辨率关键帧。最后，Fast-Vid2Vid++ 通过运动补偿以轻微延迟插入中间帧，并使用运动感知推理 (MAI) 生成全长序列。在标准基准测试中，Fast-Vid2Vid++ 实现了 30-59 FPS 的实时性能，并在单个 V100 GPU 上节省了 28-35 倍的计算成本。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10652893</guid>
      <pubDate>Tue, 27 Aug 2024 13:15:48 GMT</pubDate>
    </item>
    <item>
      <title>气体物体检测</title>
      <link>http://ieeexplore.ieee.org/document/10648775</link>
      <description><![CDATA[物体检测是计算机视觉中的一个基本且具有挑战性的问题，由于深度学习的有效性而得到了快速发展。当前要检测的物体大多是具有明显和独特视觉特征的刚性固体物质。在本文中，我们致力于一项很少探索的任务，即气体物体检测 (GOD)，旨在探索物体检测技术是否可以从固体物质扩展到气体物质。然而，气体表现出明显不同的视觉特征：1) 显着性不足，2) 任意且不断变化的形状，3) 缺乏明显的边界。为了便于研究这一具有挑战性的任务，我们构建了一个 GOD-Video 数据集，该数据集包含 600 个视频（141,017 帧），涵盖多种气体的各种属性。基于该数据集建立了一个全面的基准，以便对帧级和视频级检测器进行严格评估。从高斯弥散模型推导而来的物理启发式体素移位场 (VSF) 旨在模拟潜在 3D 空间中的几何不规则性和不断变化的形状。通过将 VSF 集成到 Faster RCNN 中，VSF RCNN 可作为气体物体检测的简单但强大的基准。我们的工作旨在吸引更多人研究这一有价值但充满挑战的领域。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10648775</guid>
      <pubDate>Mon, 26 Aug 2024 13:16:22 GMT</pubDate>
    </item>
    <item>
      <title>用于密集图像预测的频率感知特征融合</title>
      <link>http://ieeexplore.ieee.org/document/10648934</link>
      <description><![CDATA[密集图像预测任务需要具有强大类别信息和高分辨率精确空间边界细节的特征。为实现此目标，现代分层模型通常利用特征融合，直接添加来自深层的上采样粗特征和来自较低层的高分辨率特征。在本文中，我们观察到对象内融合特征值的快速变化，由于高频特征受干扰导致类别内不一致。此外，融合特征中模糊的边界缺乏准确的高频，导致边界位移。基于这些观察，我们提出了频率感知特征融合（FreqFusion），集成了自适应低通滤波器（ALPF）生成器、偏移生成器和自适应高通滤波器（AHPF）生成器。ALPF 生成器预测空间变化的低通滤波器以衰减对象内的高频分量，从而减少上采样期间的类内不一致。偏移生成器通过重采样将不一致的特征替换为更一致的特征，从而细化较大的不一致特征和较细的边界，而 AHPF 生成器则增强了下采样过程中丢失的高频详细边界信息。综合可视化和定量分析表明，FreqFusion 有效地提高了特征一致性并锐化了对象边界。在各种密集预测任务中进行的大量实验证实了其有效性。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10648934</guid>
      <pubDate>Mon, 26 Aug 2024 13:16:22 GMT</pubDate>
    </item>
    <item>
      <title>LIA：潜像动画师</title>
      <link>http://ieeexplore.ieee.org/document/10645735</link>
      <description><![CDATA[以前的动画技术主要侧重于利用显式结构表示（例如网格或关键点）将运动从驾驶视频传输到源图像。然而，这种方法面临着源数据和驾驶数据之间外观差异大的问题，并且需要复杂的附加模块来分别模拟外观和运动。为了解决这些问题，我们引入了潜像动画器 (LIA)，它被简化为动画高分辨率图像。LIA 被设计为一个简单的自动编码器，不依赖于显式表示。像素空间中的运动传递被建模为潜在空间中运动代码的线性导航。具体来说，这种导航表示为基于所提出的线性运动分解 (LMD) 以自监督方式学习的正交运动词典。大量实验结果表明，LIA 在视频质量和时空一致性方面优于 VoxCeleb、TaichiHD 和 TED-talk 数据集上的最新技术。此外，LIA 还非常适合零镜头高分辨率图像动画。代码、模型和演示视频可在 https://github.com/wyhsirius/LIA 上找到。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10645735</guid>
      <pubDate>Fri, 23 Aug 2024 13:15:51 GMT</pubDate>
    </item>
    <item>
      <title>相关性嵌入式 Transformer 跟踪：单分支框架</title>
      <link>http://ieeexplore.ieee.org/document/10643566</link>
      <description><![CDATA[开发稳健且有判别力的外观模型一直是视觉对象跟踪领域的长期研究挑战。在流行的基于 Siamese 的范式中，Siamese 类网络提取的特征通常不足以对跟踪目标和干扰对象进行建模，从而阻碍它们同时具有稳健性和判别性。虽然大多数 Siamese 跟踪器专注于设计稳健的关联操作，但我们提出了一种受 Transformer 启发的新型单分支跟踪框架。与 Siamese 类特征提取不同，我们的跟踪器将跨图像特征关联深深嵌入到特征网络的多个层中。通过多个层广泛匹配两个图像的特征，它可以抑制非目标特征，从而实现目标感知特征提取。输出特征可直接用于预测目标位置，而无需额外的关联步骤。因此，我们将双分支 Siamese 跟踪重新表述为概念简单、完全基于 Transformer 的单分支跟踪管道，称为 SBT。在对 SBT 基线进行深入分析后，我们总结了许多有效的设计原则，并提出了一种改进的跟踪器 SuperSBT。SuperSBT 采用具有局部建模层的分层架构来增强浅层特征。提出了一种统一的关系建模来消除复杂的手工制作的层模式设计。通过蒙版图像建模预训练、集成时间建模和配备专用预测头，SuperSBT 得到了进一步改进。因此，SuperSBT 在 LaSOT、TrackingNet 和 GOT-10K 中的 AUC 得分比 SBT 基线高出 4.7%、3.0% 和 4.5%。值得注意的是，SuperSBT 将 SBT 的速度从 37 FPS 大大提高到 81 FPS。大量实验表明，我们的方法在八个 VOT 基准上取得了优异的结果。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10643566</guid>
      <pubDate>Thu, 22 Aug 2024 13:16:33 GMT</pubDate>
    </item>
    <item>
      <title>视频的多模态跨语言摘要：知识蒸馏诱导的三阶段训练方法的回顾</title>
      <link>http://ieeexplore.ieee.org/document/10643687</link>
      <description><![CDATA[视频多模态摘要 (MS) 旨在从多源信息（例如视频和文本记录）中生成摘要，最近取得了令人鼓舞的进展。然而，现有的研究仅限于单语场景，忽略了非母语观众理解其他语言视频的需求。这促使我们引入视频多模态跨语言摘要 (MCLS)，旨在从视频的多模态输入中生成跨语言摘要。考虑到 MCLS 中高注释成本和资源限制的挑战，我们提出了一种知识蒸馏 (KD) 诱导的三阶段训练方法，通过将知识从丰富的单语 MS 数据转移到那些数量不足的数据来协助 MCLS。在三阶段训练方法中，设计一个视频引导的双融合网​​络 (VDF) 作为主干网络，通过编码器和解码器中的多种融合策略整合多模态和跨语言信息；此外，我们提出了两种跨语言知识提炼策略：自适应池化提炼和语言自适应扭曲提炼（LAWD），针对编码器级和词汇级提炼对象设计，以促进 MS 和 MCLS 模型之间跨长度不等的跨语言序列的有效知识转移。具体来说，为了解决 MS 和 MCLS 模型之间长度不等的语言序列。具体来说，为了解决 KD 中并行跨语言序列长度不等的挑战，LAWD 可以直接进行跨语言提炼，同时保持语言特征形状不变，以减少潜在的信息丢失。我们基于 How2 数据集对 How2-MCLS 数据集进行了精心注释，以模拟 MCLS 场景。实验结果表明，与强基线相比，所提出的方法取得了具有竞争力的性能，并且可以通过从 MS 模型迁移知识为 MCLS 模型带来显着的性能改进。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10643687</guid>
      <pubDate>Thu, 22 Aug 2024 13:16:33 GMT</pubDate>
    </item>
    <item>
      <title>医学图像分割评论：U-Net 的成功</title>
      <link>http://ieeexplore.ieee.org/document/10643318</link>
      <description><![CDATA[自动医学图像分割是医学领域的一个重要课题，也是计算机辅助诊断范式中的一个重要对应物。U-Net 是最广泛的图像分割架构，因为它具有灵活性、优化的模块化设计，并且在所有医学图像模式中都取得了成功。多年来，U-Net 模型受到了学术界和工业界研究人员的极大关注，他们对其进行了扩展，以解决医疗任务造成的规模和复杂性问题。这些扩展通常与增强 U-Net 的主干、瓶颈或跳过连接有关，或包括表示学习，或将其与 Transformer 架构相结合，甚至解决分割图的概率预测。拥有之前提出的不同 U-Net 变体的汇编，机器学习研究人员可以更轻松地识别相关的研究问题，并了解挑战该模型的生物任务的挑战。在这项工作中，我们讨论了 U-Net 模型的实际方面，并将每个变体模型组织成一个分类法。此外，为了衡量这些策略在临床应用中的表现，我们提出了在知名数据集上对一些独特和著名的设计进行公平评估的建议。此外，我们还提供了一个包含经过训练的模型的综合实现库。此外，为了方便未来的研究，我们创建了一个 U-Net 论文的在线列表，其中包含它们可能的官方实现。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10643318</guid>
      <pubDate>Wed, 21 Aug 2024 13:16:41 GMT</pubDate>
    </item>
    <item>
      <title>Q-Bench$^+$+：从单幅图像到成对图像的低级视觉多模态基础模型基准</title>
      <link>http://ieeexplore.ieee.org/document/10643329</link>
      <description><![CDATA[多模态大型语言模型 (MLLM) 的快速发展引领了计算机视觉的范式转变，朝着多功能的基础模型迈进。然而，评估 MLLM 在低级视觉感知和理解方面的表现仍是一个有待探索的领域。为此，我们设计了基准设置来模拟与低级视觉相关的人类语言反应：通过与低级属性（例如清晰度、亮度）相关的视觉问答实现的低级视觉感知 (A1)；以及用于评估 MLLM 的低级文本描述的低级视觉描述 (A2)。此外，鉴于成对比较可以更好地避免反应歧义且已被许多人体实验采用，我们进一步将 MLLM 的低级感知相关问答和描述评估从单个图像扩展到图像对。具体来说，对于感知 (A1)，我们执行了 LLVisionQA$^{+}$+ 数据集，其中包含 2,990 张单图像和 1,999 个图像对，每个图像对都附有一个关于其低级特征的开放式问题；对于描述 (A2)，我们提出了 LLDescribe$^{+}$+ 数据集，评估 MLLM 对 499 张单图像和 450 个图像对的低级描述。此外，我们通过采用基于 softmax 的方法使所有 MLLM 能够生成可量化的质量评级，并在 7 个图像质量评估 (IQA) 数据集中与人类意见进行测试，从而评估 MLLM 的评估 (A3) 能力，即预测分数。在对 24 个 MLLM 进行评估后，我们证明几个 MLLM 在单图像上具有不错的低级视觉能力，但只有 GPT-4V 在成对比较中的准确率高于单图像评估（如人类）。我们希望我们的基准能够激发进一步的研究，以发掘和增强 MLLM 的这些新兴功能。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10643329</guid>
      <pubDate>Wed, 21 Aug 2024 13:16:41 GMT</pubDate>
    </item>
    <item>
      <title>深度神经网络修剪综述：分类、比较、分析和建议</title>
      <link>http://ieeexplore.ieee.org/document/10643325</link>
      <description><![CDATA[现代深度神经网络，尤其是最近的大型语言模型，具有巨大的模型大小，需要大量的计算和存储资源。为了能够在资源受限的环境中部署现代模型并加快推理时间，研究人员越来越多地探索修剪技术作为神经网络压缩的热门研究方向。从 2020 年到 2024 年，已经发表了三千多篇修剪论文。然而，关于修剪的最新综合评论论文却很少。为了解决这个问题，在这篇综述中，我们对现有的深度神经网络修剪研究工作进行了全面的回顾，分类为 1) 通用/特定的加速，2) 何时修剪，3) 如何修剪，以及 4) 修剪与其他压缩技术的融合。然后，我们对八对对比设置（例如，非结构化/结构化、一次性/迭代、无数据/数据驱动、初始化/预训练权重等）进行了彻底的比较分析，并探讨了几个新兴主题，包括大型语言模型、视觉变换器、扩散模型和大型多模态模型的修剪、训练后修剪以及不同级别的监督修剪，以阐明现有方法的共性和差异，并为进一步的方法开发奠定基础。最后，我们就选择修剪方法提出了一些有价值的建议，并展望了神经网络修剪的几个有前途的研究方向。为了促进未来对深度神经网络修剪的研究，我们总结了广泛的修剪应用（例如，对抗鲁棒性、自然语言理解等），并建立了一个精选的数据集、网络和不同应用的评估集合。我们在 https://github.com/hrcheng1066/awesome-pruning 上维护了一个存储库，它是神经网络修剪论文和相应开源代码的综合资源。我们将不断更新此存储库，以包含该领域的最新进展。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10643325</guid>
      <pubDate>Wed, 21 Aug 2024 13:16:41 GMT</pubDate>
    </item>
    <item>
      <title>连续语义分割综述：理论、挑战、方法与应用</title>
      <link>http://ieeexplore.ieee.org/document/10643308</link>
      <description><![CDATA[持续学习，又称增量学习或终身学习，是深度学习和人工智能系统的前沿。它突破了在封闭集上单向训练的障碍，实现了在开放集条件下的持续自适应学习。近十年来，持续学习已在多个领域得到探索和应用，特别是在计算机视觉领域，涵盖分类、检测和分割任务。持续语义分割 (CSS) 的密集预测特性使其成为一项具有挑战性、复杂且蓬勃发展的任务。在本文中，我们对 CSS 进行了回顾，致力于对问题表述、主要挑战、通用数据集、新理论和多种应用进行全面概述。具体来说，我们首先阐明问题定义和主要挑战。在深入研究相关方法的基础上，我们将当前的 CSS 模型整理并分类为两个主要分支，包括数据重放和无数据集。在每个分支中，我们将相应的方法进行基于相似性的聚类和深入分析，然后在相关数据集上进行定性比较和定量再现。此外，我们还介绍了四个具有不同应用场景和发展趋势的 CSS 专业。此外，我们为 CSS 制定了一个基准，其中包括代表性参考、评估结果和再现。我们希望本调查可以为终身学习领域的发展做出有参考价值和启发性的贡献，同时也为相关领域提供有价值的观点。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10643308</guid>
      <pubDate>Wed, 21 Aug 2024 13:16:41 GMT</pubDate>
    </item>
    <item>
      <title>CO-Net++：具有两阶段特征校正的可同时执行多个点云任务的聚合网络</title>
      <link>http://ieeexplore.ieee.org/document/10643346</link>
      <description><![CDATA[我们提出了 CO-Net++，这是一个具有凝聚力的框架，它使用两阶段特征校正策略跨异构数据集域集体优化多个点云任务。CO-Net++ 的核心在于优化任务共享参数以捕获跨各种任务的通用特征，同时辨别定制的任务特定参数以封装每个任务的独特特征。具体而言，CO-Net++ 开发了一种两阶段特征校正策略 (TFRS)，该策略将任务共享参数和任务特定参数的优化过程明显分开。在第一阶段，TFRS 将主干中的所有参数配置为任务共享，这鼓励 CO-Net++ 彻底吸收与所有任务相关的通用属性。此外，TFRS 引入了基于符号的梯度手术以促进任务共享参数的优化，从而缓解由各种数据集域引起的梯度冲突。在第二阶段，TFRS 冻结任务共享参数并灵活地将任务特定参数集成到网络中以编码每个数据集域的特定特征。 CO-Net++ 显著缓解了参数纠缠导致的冲突优化问题，确保了通用特征和特定特征的充分识别。大量实验表明，CO-Net++ 在 3D 物体检测和 3D 语义分割任务上均实现了出色的性能。此外，CO-Net++ 提供了令人印象深刻的增量学习能力，并在推广到新的点云任务时防止了灾难性遗忘。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10643346</guid>
      <pubDate>Wed, 21 Aug 2024 13:16:41 GMT</pubDate>
    </item>
    </channel>
</rss>