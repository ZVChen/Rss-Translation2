<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>IEEE 模式分析与机器智能学报 - 新目录</title>
    <link>http://ieeexplore.ieee.org</link>
    <description>出版物 # 34 的目录提醒</description>
    <lastBuildDate>Wed, 05 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>自监督深度盲视频超分辨率</title>
      <link>http://ieeexplore.ieee.org/document/10428075</link>
      <description><![CDATA[现有的基于深度学习的视频超分辨率 (SR) 方法通常依赖于监督学习方法，其中训练数据通常由已知或预定义核（例如，双三次核）的模糊操作生成，然后进行抽取操作。然而，这并不适用于实际应用，因为退化过程很复杂，这些理想情况无法很好地近似。此外，在现实场景中获取高分辨率 (HR) 视频和相应的低分辨率 (LR) 视频很困难。为了克服这些问题，我们提出了一种自监督学习方法来解决盲视频 SR 问题，该方法同时从 LR 视频中估计模糊核和 HR 视频。由于直接使用 LR 视频作为监督通常会导致简单的解决方案，我们开发了一种简单有效的方法，根据视频 SR 的图像形成从原始 LR 视频生成辅助配对数据，以便网络可以通过生成的配对数据更好地约束模糊核估计和潜在 HR 视频恢复。此外，我们引入了光流估计模块，利用相邻帧的信息进行高分辨率视频恢复。实验表明，我们的方法在基准测试和真实视频上的表现优于最先进的方法。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10428075</guid>
      <pubDate>Thu, 08 Feb 2024 13:17:54 GMT</pubDate>
    </item>
    <item>
      <title>来自事件摄像机的密集连续时间光流</title>
      <link>http://ieeexplore.ieee.org/document/10419040</link>
      <description><![CDATA[我们提出了一种从事件数据估计密集连续时间光流的方法。传统的密集光流方法计算两幅图像之间的像素位移。由于缺少信息，这些方法无法在两幅图像之间的盲区内恢复像素轨迹。在这项工作中，我们展示了使用事件相机中的事件计算每个像素的连续时间光流的可能性。由于事件的异步性和微秒级响应时间，事件提供了关于像素空间中运动的时间细粒度信息。我们利用这些优势通过参数化的贝塞尔曲线在连续时间内密集地预测像素轨迹。为了实现这一点，我们为这项任务构建了一个具有强归纳偏差的神经网络：首先，我们使用事件数据在时间上构建多个连续相关体积。其次，我们使用贝塞尔曲线在轨迹上的多个时间戳处索引这些相关体积。第三，我们使用检索到的相关性迭代更新贝塞尔曲线表示。我们的方法可以选择包含图像对以进一步提高性能。据我们所知，我们的模型是第一个可以从事件数据中回归密集像素轨迹的方法。为了训练和评估我们的模型，我们引入了一个合成数据集 (MultiFlow)，其中包含每个像素的移动物体和地面真实轨迹。我们的定量实验不仅表明我们的方法成功地预测了连续时间中的像素轨迹，而且它在 MultiFlow 和 DSEC-Flow 上的传统双视图像素位移度量中具有竞争力。开源代码和数据集已向公众发布。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10419040</guid>
      <pubDate>Fri, 02 Feb 2024 13:16:18 GMT</pubDate>
    </item>
    <item>
      <title>RNNPose：通过循环对应场估计和姿势优化进行 6-DoF 物体姿势估计</title>
      <link>http://ieeexplore.ieee.org/document/10416758</link>
      <description><![CDATA[从单目图像进行 6-DoF 物体姿态估计是一项具有挑战性的问题，通常需要后细化程序才能实现高精度估计。在本文中，我们提出了一个基于循环神经网络 (RNN) 的框架，称为 RNNPose，用于物体姿态细化，该框架对错误的初始姿态和遮挡具有鲁棒性。在循环迭代过程中，物体姿态细化被表述为基于估计的对应场（渲染图像和观察到的图像之间）的非线性最小二乘问题。然后通过可微分的 Levenberg-Marquardt (LM) 算法解决该问题，从而实现端到端训练。对应场估计和姿态细化在每次迭代中交替进行，以改善物体姿态。此外，为了提高对遮挡的鲁棒性，我们引入了一种基于学习到的 3D 模型描述符和观察到的 2D 图像的一致性检查机制，该机制在姿态优化过程中降低了不可靠的对应关系的权重。我们在多个公共数据集上评估了 RNNPose，包括 LINEMOD、Occlusion-LINEMOD、YCB-Video 和 TLESS。我们展示了最先进的性能和对场景中严重杂波和遮挡的强大鲁棒性。大量实验验证了我们提出的方法的有效性。此外，基于 RNNPose 的扩展系统成功推广到多实例场景并在 TLESS 数据集上实现了顶级性能。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10416758</guid>
      <pubDate>Tue, 30 Jan 2024 13:17:25 GMT</pubDate>
    </item>
    <item>
      <title>利用相机 RAW 快照实现高效的视觉计算</title>
      <link>http://ieeexplore.ieee.org/document/10415533</link>
      <description><![CDATA[传统相机在传感器上捕获图像辐照度 (RAW)，然后使用图像信号处理器 (ISP) 将其转换为 RGB 图像。然后，这些图像可用于各种应用中的摄影或视觉计算任务，例如公共安全监控和自动驾驶。有人可能会说，由于 RAW 图像包含所有捕获的信息，因此使用 ISP 将 RAW 转换为 RGB 对于视觉计算来说并不是必需的。在本文中，我们提出了一种新颖的 $\rho$ρ-Vision 框架，用于使用 RAW 图像执行高级语义理解和低级压缩，而无需使用使用了数十年的 ISP 子系统。考虑到可用 RAW 图像数据集的稀缺性，我们首先基于无监督 CycleGAN 开发一个非配对 CycleR2R 网络，以使用非配对 RAW 和 RGB 图像训练模块化展开 ISP 和逆 ISP (invISP) 模型。然后，我们可以使用任何现有的 RGB 图像数据集灵活地生成模拟 RAW 图像 (simRAW)，并微调最初在 RGB 域中训练的不同模型以处理真实世界的相机 RAW 图像。我们使用 RAW 域 YOLOv3 和 RAW 图像压缩器 (RIC) 在相机快照上展示了 RAW 域中的对象检测和图像压缩功能。定量结果表明，与 RGB 域相比，RAW 域任务推理提供了更好的检测准确性和压缩效率。此外，提出的 $\rho$ρ-Vision 可推广到各种相机传感器和不同的任务特定模型。采用 $\rho$ρ-Vision 的另一个好处是不再需要 ISP，从而可能减少计算和处理时间。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10415533</guid>
      <pubDate>Mon, 29 Jan 2024 13:16:54 GMT</pubDate>
    </item>
    <item>
      <title>偏振亥姆霍兹立体视觉</title>
      <link>http://ieeexplore.ieee.org/document/10414400</link>
      <description><![CDATA[亥姆霍兹立体视觉 (HS) 利用光传播的互易原理 (即亥姆霍兹互易性) 对具有任意反射率的表面进行 3D 重建。在本文中，我们提出了偏振亥姆霍兹立体视觉 (polar-HS)，它通过考虑光在互易路径中的偏振状态来扩展经典的 HS。借助来自偏振的额外相位信息，polar-HS 只需要一个互易图像对。我们推导了 Mueller 矩阵的互易关系，并制定了考虑偏振状态的新互易约束。我们还利用了偏振约束并将其扩展到透视投影的情况。为了恢复表面深度和法线，我们将互易约束与漫反射/镜面偏振约束结合在一个统一的优化框架中。对于深度估计，我们进一步提出利用漫反射偏振角的一致性。对于法线估计，我们开发了一种基于线性偏振度的法线细化策略。使用硬件原型，我们展示了我们的方法可以为不同类型的表面（从漫反射到高度镜面）生成高质量的 3D 重建。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10414400</guid>
      <pubDate>Thu, 25 Jan 2024 13:16:27 GMT</pubDate>
    </item>
    <item>
      <title>将十四种事后归因方法与泰勒交互作用统一起来</title>
      <link>http://ieeexplore.ieee.org/document/10414149</link>
      <description><![CDATA[人们已经开发出各种归因方法来解释深度神经网络 (DNN)，通过推断每个输入变量对最终输出的归因/重要性/贡献分数。然而，现有的归因方法往往建立在不同的启发式方法之上。对于这些方法为何有效以及它们之间如何相关，目前仍缺乏统一的理论理解。此外，目前还没有一个普遍接受的标准来比较一种归因方法是否优于另一种。在本文中，我们借助泰勒相互作用，首次发现 14 种现有的归因方法虽然基于完全不同的启发式方法定义归因，但实际上具有相同的核心机制。具体而言，我们证明了 14 种归因方法估计的输入变量归因分数都可以在数学上重新表述为两种典型效应的加权分配，即每个输入变量的独立效应和输入变量之间的交互效应。这些归因方法的本质区别在于分配不同效应的权重。受这些见解的启发，我们提出了公平分配效果的三项原则，作为评估归因方法忠实性的新标准。总之，本研究可以被视为重新审视十四种归因方法的新统一视角，从理论上阐明了这些方法之间的本质相似之处和差异。此外，提出的新原则使人们能够在统一视角下对不同方法进行直接和公平的比较。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10414149</guid>
      <pubDate>Thu, 25 Jan 2024 13:16:27 GMT</pubDate>
    </item>
    <item>
      <title>元学习的进展与挑战：技术回顾</title>
      <link>http://ieeexplore.ieee.org/document/10413635</link>
      <description><![CDATA[元学习使学习系统能够从多个任务中获取知识，从而能够更快地适应和推广到新任务。这篇评论对元学习进行了全面的技术概述，强调了它在数据可能稀缺或获取成本高昂的实际应用中的重要性。本文介绍了最先进的元学习方法，并探讨了元学习与多任务学习、迁移学习、领域适应和泛化、自监督学习、个性化联邦学习和持续学习之间的关系。通过强调这些主题与元学习领域之间的协同作用，本文展示了一个领域的进步如何使整个领域受益，同时避免不必要的重复工作。此外，本文还深入探讨了高级元学习主题，例如从复杂的多模态任务分布中学习、无监督元学习、学习有效适应数据分布变化以及持续元学习。最后，本文强调了该领域未来研究的开放问题和挑战。本文综合了最新的研究进展，对元学习及其对各种机器学习应用的潜在影响进行了透彻的理解。我们相信，这篇技术概述将有助于元学习的进步及其在解决实际问题中的实际意义。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10413635</guid>
      <pubDate>Wed, 24 Jan 2024 13:17:26 GMT</pubDate>
    </item>
    <item>
      <title>模型真正关注的是什么？：提取面向模型的概念来解释深度神经网络</title>
      <link>http://ieeexplore.ieee.org/document/10412652</link>
      <description><![CDATA[模型可解释性是构建可信 AI 系统的关键要素之一，尤其是在自动驾驶和诊断等需要可靠性的应用中。文献中已经研究了许多可解释性方法。在众多方法中，本文重点关注一条研究路线，该路线试图通过发现模型学习的概念来直观地解释预训练的图像分类模型（例如卷积神经网络），这就是所谓的基于概念的解释。以前基于概念的解释方法依赖于人类对概念的定义（例如 Broden 数据集）或语义分割技术，如 Slic（简单线性迭代聚类）。然而，我们认为，这些方法识别的概念可能会显示更符合人类视角或通过分割方法裁剪的图像部分，而不是纯粹反映模型自身的视角。我们提出了面向模型的概念提取 (MOCE)，这是一种仅基于模型本身提取关键概念的新方法，从而能够捕捉其不受任何外部因素影响的独特视角。在各种预训练模型上的实验结果证实了通过真实地表示模型的观点来提取概念的优势。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10412652</guid>
      <pubDate>Tue, 23 Jan 2024 13:17:59 GMT</pubDate>
    </item>
    <item>
      <title>AdaCS：基于受限等距属性的误差钳制自适应压缩感知</title>
      <link>http://ieeexplore.ieee.org/document/10412658</link>
      <description><![CDATA[场景相关的自适应压缩感知 (CS) 一直是一个长期追求的目标，具有显著提高 CS 性能的巨大潜力。然而，在无法获得真实图像的情况下，如何设计场景相关的自适应策略仍然是一个悬而未决的问题。本文提出了一种基于受限等距特性 (RIP) 条件的误差钳制方法，它可以直接预测重建误差，即当前阶段重建图像与真实图像之间的差异，并在下一采样阶段自适应地将更多样本分配给重建误差较大的区域。此外，我们提出了一种由渐进逆变换和交替双向多网格网络组成的 CS 重建网络，称为 PiABM-Net，它可以有效地利用多尺度信息来重建目标图像。与最先进的 CS 算法相比，通过大量定量和定性实验证明了所提出的自适应级联 CS 方法的有效性。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10412658</guid>
      <pubDate>Tue, 23 Jan 2024 13:17:59 GMT</pubDate>
    </item>
    <item>
      <title>聚焦每一个问题、每一个步骤：学习用综合注意力解决视觉语言问题</title>
      <link>http://ieeexplore.ieee.org/document/10412655</link>
      <description><![CDATA[整合视觉和语言模态的信息引发了计算机视觉和自然语言处理领域的有趣应用。现有方法虽然在图像字幕和视觉问答等任务中很有前景，但在理解现实问题和提供分步解决方案方面面临挑战。特别是，它们通常将其范围限制在具有顺序结构的解决方案上，从而忽略了复杂的步骤间依赖关系。为了弥补这一差距，我们提出了一种基于图的视觉语言问题解决方法。它利用一种新颖的集成注意力机制，该机制共同考虑每个步骤以及多个步骤中特征的重要性。结合图神经网络方法，可以逐步学习这种注意力机制，以根据问题解决过程的特征预测顺序和非顺序解决方案图。为了将注意力与问题解决过程紧密结合起来，我们进一步设计了新的学习目标，其中的注意力指标可以量化这种综合注意力，从而更好地协调步骤内的视觉和语言信息，并更准确地捕捉步骤之间的信息流。在 VisualHow（一个包含不同解决方案结构的综合数据集）上进行的实验结果显示在预测步骤和依赖关系方面有显著的改善，证明了我们的方法在解决各种视觉语言问题方面的有效性。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10412655</guid>
      <pubDate>Tue, 23 Jan 2024 13:17:59 GMT</pubDate>
    </item>
    <item>
      <title>构建具有更好架构、优化和数据的开放词汇视频 CLIP 模型</title>
      <link>http://ieeexplore.ieee.org/document/10412661</link>
      <description><![CDATA[尽管对比语言图像预训练 (CLIP) 在零样本图像识别方面取得了显著成果，但人们在探索其在零样本视频识别方面的潜力方面所做的努力有限。本文介绍了 Open-VCLIP++，这是一个简单而有效的框架，可将 CLIP 调整为强大的零样本视频分类器，能够在测试期间识别新的动作和事件。Open-VCLIP++ 对 CLIP 进行了最低限度的修改，以捕获视频中的时空关系，从而创建了一个专门的视频分类器，同时力求实现泛化。我们正式证明，训练 Open-VCLIP++ 相当于在没有历史数据的情况下持续学习。为了解决这个问题，我们引入了插值权重优化，这是一种在训练和测试期间都利用权重插值优势的技术。此外，我们基于大型语言模型来生成细粒度的视频描述。这些详细的描述与视频特征进一步保持一致，有助于更好地将 CLIP 转移到视频领域。我们的方法在三个广泛使用的动作识别数据集上进行了评估，遵循了各种零样本评估协议。结果表明，我们的方法远远超越了现有的最先进技术。具体来说，我们在 UCF、HMDB 和 Kinetics-600 数据集上分别实现了 88.1%、58.7% 和 81.2% 的零样本准确率，比表现最佳的替代方法高出 8.5%、8.2% 和 12.3%。我们还在 MSR-VTT 视频文本检索数据集上评估了我们的方法，在该数据集上，它提供了具有竞争力的视频到文本和文本到视频的检索性能，同时与其他方法相比，它使用的微调数据要少得多。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10412661</guid>
      <pubDate>Tue, 23 Jan 2024 13:17:59 GMT</pubDate>
    </item>
    <item>
      <title>VNVC：一种用于高效人机视觉的多功能神经视频编码框架</title>
      <link>http://ieeexplore.ieee.org/document/10411051</link>
      <description><![CDATA[几乎所有的数字视频在传输之前都会被编码成紧凑的表示形式。在向人类显示之前，以及在通过机器视觉算法进行增强/分析之前，需要将这种紧凑的表示形式解码回像素。直观地说，直接增强/分析编码表示而不是将其解码为像素更有效。因此，我们提出了一种多功能神经视频编码 (VNVC) 框架，该框架旨在学习紧凑的表示形式以支持重建和直接增强/分析，从而对人类和机器视觉都具有通用性。我们的 VNVC 框架具有基于特征的压缩循环。在循环中，将一帧编码为紧凑的表示形式并解码为在执行重建之前获得的中间特征。中间特征可以通过基于特征的时间上下文挖掘和跨域运动编码器-解码器作为运动补偿和运动估计的参考来压缩后续帧。中间特征直接输入到视频重建、视频增强和视频分析网络中以评估其有效性。评估表明，我们的具有中间特征的框架实现了视频重建的高压缩效率和令人满意的任务性能，并且复杂度较低。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10411051</guid>
      <pubDate>Mon, 22 Jan 2024 13:21:30 GMT</pubDate>
    </item>
    <item>
      <title>通过增强对比学习实现完全无监督的 Deepfake 视频检测</title>
      <link>http://ieeexplore.ieee.org/document/10411047</link>
      <description><![CDATA[如今，Deepfake视频在互联网上广泛传播，严重损害了公众的信任和社会安全。尽管近年来出现了越来越多可靠的检测器来抵御这种新兴的篡改技术，但仍存在一些棘手的问题需要解决，例如大多数监督机制框架下的Deepfake视频检测器需要大量具有准确标签的样本进行训练。当具有真实标签的训练样本数量不足或训练数据被攻击者恶意投毒时，监督分类器可能无法可靠地进行检测。为了解决这个难题，提出设计一种完全无监督的Deepfake检测器。具体而言，在整个训练或测试过程中，我们根本不知道有关样本的真实标签的任何信息。首先，我们新颖地设计了一个伪标签生成器来标记训练样本，其中使用传统的手工制作的特征来表征这两类样本。其次，将带有伪标签的训练样本输入到我们提出的增强对比学习器中，在对比损失的指导下，进一步提取判别性特征并通过迭代不断细化。最后，依靠帧间相关性，完成真假视频的最终二分类。大量实验结果在基准数据集（包括FF++、Celeb-DF、DFD、DFDC、UADFV）上验证了我们提出的无监督Deepfake检测器的有效性。此外，我们提出的性能良好的检测器优于目前的无监督方法，堪比基线监督方法。更重要的是，当面对标记数据被恶意攻击者毒害或训练数据不足的问题时，我们提出的无监督Deepfake检测器发挥了强大的优势。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10411047</guid>
      <pubDate>Mon, 22 Jan 2024 13:21:30 GMT</pubDate>
    </item>
    <item>
      <title>FlowX：通过消息流实现可解释的图形神经网络</title>
      <link>http://ieeexplore.ieee.org/document/10374255</link>
      <description><![CDATA[我们研究图神经网络 (GNN) 的可解释性，以此作为阐明其工作机制的一步。虽然大多数当前方法都侧重于解释图节点、边或特征，但我们认为，作为 GNN 的固有功能机制，消息流更自然地实现可解释性。为此，我们在此提出了一种新方法，称为 FlowX，通过识别重要的消息流来解释 GNN。为了量化流的重要性，我们建议遵循合作博弈论中的 Shapley 值理念。为了解决计算所有联盟边际贡献的复杂性，我们提出了一种流采样方案来计算 Shapley 值近似值作为进一步训练的初步评估。然后，我们提出了一种信息控制的学习算法来训练流分数以实现不同的解释目标：必要或充分的解释。对合成和真实世界数据集的实验研究表明，我们提出的 FlowX 及其变体可以提高 GNN 的可解释性。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10374255</guid>
      <pubDate>Tue, 26 Dec 2023 13:16:38 GMT</pubDate>
    </item>
    <item>
      <title>用于多视图学习的变分蒸馏</title>
      <link>http://ieeexplore.ieee.org/document/10372503</link>
      <description><![CDATA[信息瓶颈 (IB) 通过揭示每个视点所包含的各种组件，为多视图学习提供了信息论原理。这凸显了捕捉它们不同角色以实现视图不变性和预测表示的必要性，但由于建模和组织无数互信息 (MI) 项的技术难度，这一原理仍未得到充分探索。最近的研究表明，充分性和一致性在多视图表示学习中起着关键作用，并且可以通过变分蒸馏框架来保留。但是当它推广到任意视点时，这种策略就会失败，因为一致性的互信息项变得复杂。本文提出了多视图变分蒸馏 (MV$^{2}$2D)，解决了广义多视图学习的上述限制。独特的是，MV$^{2}$2D 可以识别有用的一致信息并根据其泛化能力对各种组件进行优先排序。这为实现充分性和一致性提供了一种分析性和可扩展的解决方案。此外，通过严格重新表述 IB 目标，MV$^{2}$2D 解决了 MI 优化中的难题，充分发挥了信息瓶颈原理的理论优势。我们在各种任务上对我们的模型进行了广泛的评估，以验证其有效性，其中显著的收益为在严格的信息理论原理下实现广义多视图表示提供了关键见解。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10372503</guid>
      <pubDate>Fri, 22 Dec 2023 13:17:48 GMT</pubDate>
    </item>
    </channel>
</rss>