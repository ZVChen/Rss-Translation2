<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title></title>
    <link>http://ieeexplore.ieee.org</link>
    <description>TOC警报出版＃34</description>
    <lastBuildDate>Wed, 05 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title></title>
      <link>http://ieeexplore.ieee.org/document/10772006</link>
      <description><![CDATA[]]></description>
      <guid>http://ieeexplore.ieee.org/document/10772006</guid>
      <pubDate>Fri, 29 Nov 2024 13:16:48 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>http://ieeexplore.ieee.org/document/10771715</link>
      <description><![CDATA[]]></description>
      <guid>http://ieeexplore.ieee.org/document/10771715</guid>
      <pubDate>Thu, 28 Nov 2024 13:18:14 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>http://ieeexplore.ieee.org/document/10771714</link>
      <description><![CDATA[]]></description>
      <guid>http://ieeexplore.ieee.org/document/10771714</guid>
      <pubDate>Thu, 28 Nov 2024 13:18:14 GMT</pubDate>
    </item>
    <item>
      <title>实用紧凑的深度压缩感</title>
      <link>http://ieeexplore.ieee.org/document/10763443</link>
      <description><![CDATA[近年来，深层网络在压缩传感（CS）中取得了成功，这使采样成本大大降低，自成立以来就引起了人们的关注。在本文中，我们提出了一个新的实用和紧凑网络，称为通用图像CS的PCNET。具体而言，在PCNET中，设计了一种新颖的协作采样操作员，它由深度条件过滤步骤和双分支快速采样步骤组成。前者学习了线性变换矩阵中的隐式表示，并首先在输入图像上执行自适应局部过滤，而后者随后使用离散的余弦变换和拼命的块状diagonal高斯矩阵来生成不足采样的测量值。我们的PCNET配备了增强的重建近端梯度下降算法网络。一旦训练，它为任意抽样率提供了灵活性，可解释性和强大的恢复性能。此外，我们为单像素CS成像系统提供了面向部署的提取方案，该方案允许将任何线性采样操作员方便地转换为其矩阵表单，以将其加载到数字微型摩尔设备等硬件上。与现有的最新方法相比，对自然图像CS，量化CS和自我监督的CS进行了广泛的实验，证明了PCNET的优异重构精度和概括能力，尤其是对于高分辨率图像。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10763443</guid>
      <pubDate>Fri, 22 Nov 2024 13:16:35 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>http://ieeexplore.ieee.org/document/10763467</link>
      <description><![CDATA[]]></description>
      <guid>http://ieeexplore.ieee.org/document/10763467</guid>
      <pubDate>Thu, 21 Nov 2024 13:20:31 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>http://ieeexplore.ieee.org/document/10759842</link>
      <description><![CDATA[]]></description>
      <guid>http://ieeexplore.ieee.org/document/10759842</guid>
      <pubDate>Thu, 21 Nov 2024 13:20:31 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>http://ieeexplore.ieee.org/document/10763465</link>
      <description><![CDATA[]]></description>
      <guid>http://ieeexplore.ieee.org/document/10763465</guid>
      <pubDate>Thu, 21 Nov 2024 13:20:31 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>http://ieeexplore.ieee.org/document/10758928</link>
      <description><![CDATA[最近，Quanta图像传感器（QIS） - 超快速，零阅读的噪声二进制图像传感器 - 在许多具有挑战性的情况下表现出了显着的成像功能。尽管具有潜力，但这些传感器的采用受（a）高数据速率和（b）需要新的计算管道来处理非常规原始数据的需求严重阻碍了这些传感器。我们介绍了一个简单的低带宽计算管道，以应对这些挑战。我们的方法基于具有较小的记忆足迹的新型流媒体表示，在多个时间尺度上有效捕获强度信息。更新表示形式仅需要24个浮动点操作/像素，可以通过二进制框架的本机帧速率在线计算它们。我们使用在此表示形式上运行的神经网络实时重建视频（10-30 fps）。我们说明了为什么这种表示形式适合这些新兴传感器，以及如何提供低潜伏期和高帧速率，同时保留了下游计算机视觉的灵活性。我们的方法导致大量数据带宽减少（$ \ sim 100 \ times $ 〜100×）和实时图像重建和计算机视觉$ -10^{4} \ text { - } 10^{5} \ times $  - 比现有的最新方法（Ma等，2020）的计算减少104-105×，同时保持了可比的质量。据我们所知，我们的方法是第一个在QIS上实现在线实时图像重建的方法。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10758928</guid>
      <pubDate>Wed, 20 Nov 2024 13:16:48 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>http://ieeexplore.ieee.org/document/10758313</link>
      <description><![CDATA[部分场景文本检索的任务涉及本地位置和搜索与图像库中给定查询文本相同或相似的文本实例。但是，现有方法只能处理文本行实例，而由于培训数据中缺乏补丁注释，因此在这些文本行实例中搜索部分补丁的问题。为了解决这个问题，我们提出了一个可以同时检索文本线实例及其部分补丁的网络。我们的方法将两种类型的数据（查询文本和场景文本实例）嵌入共享特征空间，并测量其跨模式相似性。为了处理部分补丁，我们提出的方法采用多个实例学习（MIL）方法来学习与查询文本的相似之处，而无需额外的注释。但是，构造袋子是常规MIL方法的标准步骤，可以引入许多嘈杂的样本进行训练，并降低推理速度。为了解决这个问题，我们提出了一种排名MIL（RANKMIL）的方法来适应过滤这些嘈杂的样本。此外，我们提出了一种动态的部分匹配算法（DPMA），该算法可以在推理阶段直接从文本行实例中直接搜索目标部分贴片，而无需袋子。这大大提高了搜索效率和检索部分补丁的性能。我们在两个任务中评估了英语和中文数据集的提议方法：检索文本线实例和部分补丁。对于英语文本检索，我们的方法在这两个任务的三个数据集中，我们的方法的平均地图分别超过8.04％的地图和平均12.71％的地图。对于中文文本检索，我们的方法超过了最新的方法24.45％地图和三个任务数据集中的平均地图分别超过了38.06％的地图。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10758313</guid>
      <pubDate>Tue, 19 Nov 2024 13:16:30 GMT</pubDate>
    </item>
    <item>
      <title>Bokehme ++：多功能散景的古典和神经渲染的和谐融合</title>
      <link>http://ieeexplore.ieee.org/document/10756626</link>
      <description><![CDATA[]]></description>
      <guid>http://ieeexplore.ieee.org/document/10756626</guid>
      <pubDate>Mon, 18 Nov 2024 13:16:41 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>http://ieeexplore.ieee.org/document/10752992</link>
      <description><![CDATA[]]></description>
      <guid>http://ieeexplore.ieee.org/document/10752992</guid>
      <pubDate>Thu, 14 Nov 2024 13:16:50 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>http://ieeexplore.ieee.org/document/10753099</link>
      <description><![CDATA[]]></description>
      <guid>http://ieeexplore.ieee.org/document/10753099</guid>
      <pubDate>Thu, 14 Nov 2024 13:16:50 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>http://ieeexplore.ieee.org/document/10752976</link>
      <description><![CDATA[]]></description>
      <guid>http://ieeexplore.ieee.org/document/10752976</guid>
      <pubDate>Thu, 14 Nov 2024 13:16:50 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>http://ieeexplore.ieee.org/document/10750436</link>
      <description><![CDATA[]]></description>
      <guid>http://ieeexplore.ieee.org/document/10750436</guid>
      <pubDate>Mon, 11 Nov 2024 13:16:35 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>http://ieeexplore.ieee.org/document/10750316</link>
      <description><![CDATA[]]></description>
      <guid>http://ieeexplore.ieee.org/document/10750316</guid>
      <pubDate>Mon, 11 Nov 2024 13:16:35 GMT</pubDate>
    </item>
    </channel>
</rss>