<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>IEEE关于模式分析和机器智能的交易 - 新的TOC</title>
    <link>http://ieeexplore.ieee.org</link>
    <description>TOC警报出版＃34</description>
    <lastBuildDate>Thu, 06 Mar 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>DHVT：小型数据集识别的动态混合视觉变压器</title>
      <link>http://ieeexplore.ieee.org/document/10836856</link>
      <description><![CDATA[视觉变压器（VIT）和卷积神经网络（CNN）之间的性能差距由于缺乏归纳偏差而持续存在，尤其是在使用有限的数据集训练时。本文确定了VIT中的两个至关重要的缺点：空间相关性和不同的渠道表示。因此，由于数据不足，VIT难以掌握细粒的空间特征和稳健的通道表示。我们提出动态混合视觉变压器（DHVT）来应对这些挑战。关于空间方面，DHVT在特征嵌入阶段和特征投影模块中引入卷积，以增强空间相关性。关于通道方面，动态聚合机制和开创性的设计“头代币”有助于重新校准和协调不同的通道表示。此外，我们研究了网络元结构的选择，并采用没有常规类令牌的最佳多阶段混合结构。然后，使用一种新的维数剩余连接机制来修改该方法，以充分利用结构的潜力。该更新的变体称为DHVT2，为与视觉相关的任务提供了更高效的解决方案。 DHVT和DHVT2获得了最新的图像识别结果，有效地弥合了CNN和VIT之间的性能差距。下游实验进一步证明了它们的强大概括能力。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10836856</guid>
      <pubDate>Fri, 10 Jan 2025 13:17:52 GMT</pubDate>
    </item>
    <item>
      <title>基础模型定义了远景新时代：调查和前景</title>
      <link>http://ieeexplore.ieee.org/document/10834497</link>
      <description><![CDATA[看到和理由的视觉系统对于理解我们的世界至关重要。人类语言可以更好地描述物体及其位置，歧义和变化之间的复杂关系，自然地由语法规则和其他方式（例如音频和深度）来控制。这些模型学会了弥合此类方式和大规模培训数据之间的差距，从而有助于在测试时间进行上下文推理，泛化和及时功能。这些模型称为基础模型。可以通过提供人提供的提示来修改此类模型的输出，而无需进行重新训练，例如，通过提供边界框来分割特定对象，通过询问有关图像或视频场景的问题或通过语言说明来操纵机器人的行为来进行交互式对话。在这项调查中，我们对此类新兴基础模型进行了全面审查，包括典型的建筑设计，以结合不同的方式（视觉，文本，音频等），培训目标（对比，生成），预训练数据集，微调机制以及常见的提示模式；文本，视觉和异质。我们讨论了计算机视觉中基础模型的开放挑战和研究方向，包括其评估和基准测试，现实世界中的差距，上下文理解的局限性，偏见，对对抗性攻击的脆弱性以及可解释性问题。我们回顾了该领域的最新发展，涵盖了基础模型的广泛应用，并全面地进行了全面的应用。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10834497</guid>
      <pubDate>Thu, 09 Jan 2025 13:17:57 GMT</pubDate>
    </item>
    <item>
      <title>通过傅立叶系列增强对象检测</title>
      <link>http://ieeexplore.ieee.org/document/10833868</link>
      <description><![CDATA[传统对象检测模型通常会丢失对象的详细概述信息。为了解决此问题，我们提出了傅立叶级数对象检测（FSD）。它将对象的轮廓封闭曲线编码为两个一维周期傅立叶系列。构建了傅立叶系列模型（FSM），以回归图像中每个对象的傅立叶系列。因此，在推断过程中，可以检索每个对象的详细概述信息。我们介绍了傅立叶损失的滚动优化匹配，以确保模型的学习过程不受标记的轮廓点的起点序列的影响，从而加快了训练过程。 FSM展示了针对非矩形或细长对象区域的提高特征提取和描述性功能。该模型在DOTA 1.5数据集上实现了AP50 = 73.3％，该数据集超过了最先进的（SOTA）方法，为66.86％。在UCAS数据集上，该模型达到AP50 = 97.25％，也超过了SOTA方法的性能指标。此外，我们介绍了对象的傅立叶功率谱，以描述轮廓特征和傅立叶向量以指示其方向。这增强了对象检测模型的场景语义表示，并为对象检测方法的演变铺平了新的途径。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10833868</guid>
      <pubDate>Wed, 08 Jan 2025 13:18:20 GMT</pubDate>
    </item>
    <item>
      <title>自动驾驶的混合预测综合计划</title>
      <link>http://ieeexplore.ieee.org/document/10833731</link>
      <description><![CDATA[自主驾驶系统需要对周围环境进行全面的理解和准确的预测，以促进在复杂方案中明智的决策。基于学习的系统的最新进展强调了整合预测和计划的重要性。但是，这种整合通过预测模式之间的一致性以及未来预测与计划之间的相互作用提出了重大的一致性挑战。为了应对这些挑战，我们引入了混合预测综合计划（HPP）框架，该框架通过三个新型模块进行了协作。首先，我们将边缘条件的占用预测与特定于特定的运动预测保持一致。我们提出的MS-ECFORNER模块实现了跨多个粒度的运动预测的时空比对。其次，我们提出了一个游戏理论运动预测器GTFormer，以基于其共同的预测性意识来对代理之间的交互动力进行建模。第三，混合预测模式同时集成到自我计划者中，并通过预测指导进行了优化。 HPP框架在Nuscenes数据集上建立了最先进的性能，证明了端到端配置的卓越精确性和安全性。此外，在Waymo开放式数据集（WOMD）和CARLA基准中，证明了HPP的交互式开环和闭环计划性能，通过实现预测与计划之间的增强一致性，超过了现有的集成管道。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10833731</guid>
      <pubDate>Wed, 08 Jan 2025 13:18:20 GMT</pubDate>
    </item>
    <item>
      <title>在3D gan中迈向高质量和分离的脸部编辑</title>
      <link>http://ieeexplore.ieee.org/document/10829803</link>
      <description><![CDATA[由于神经辐射场的综合方法，最新的合成3D吸引脸部图像的方法已实现了快速发展，从而允许高质量和快速的推理速度。但是，现有的用于编辑面部几何形状和外观的解决方案通常需要重新培训，并且对最近的发电工作没有优化，因此倾向于落后于发电过程。为了解决这些问题，我们介绍了Nerflaceediting，从而使基于三个平面的神经辐射场中的编辑和分解几何形状和外观，同时保持其高质量和快速的推理速度。我们的分离的关键思想是使用三平面的统计数据来代表其相应面部体积的高级外观。此外，我们利用生成的3D连续语义面具作为几何编辑中的中介。我们设计了一个几何解码器（外观变化时其输出不变）和外观解码器。几何解码器将原始面部体积与语义面膜音量对齐。我们还通过将外观相同的渲染图像显式正规化，但几何形状分别对每个面部成分的颜色分布相似，从而增强了分离。我们的方法允许用户通过语义面具编辑，并具有对几何和外观的脱耦控制。定性和定量评估都表明，与现有和替代解决方案相比，我们方法的出色几何形状和外观控制能力。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10829803</guid>
      <pubDate>Mon, 06 Jan 2025 13:17:57 GMT</pubDate>
    </item>
    <item>
      <title>DeepSN-NET：Deep Ami-Smooth Newton驱动网络，用于盲图修复</title>
      <link>http://ieeexplore.ieee.org/document/10820096</link>
      <description><![CDATA[深度展开的网络代表了图像恢复中有希望的研究途径。但是，当前大多数深层展开的方法论都锚定在一阶优化算法中，这些算法的收敛速度迟钝，学习效率不令人满意。在本文中，为了解决这个问题，我们首先制定了改进的二阶半平滑牛顿（ISN）算法，将原始的非线性方程转换为可符合网络实现的优化问题。之后，我们提出了一种基于ISN算法的创新网络架构，用于盲图恢复，即DeepSN-NET。据我们所知，DeepSN-Net是第一个成功设计用于图像修复的二阶Deep Network的成功努力，该网络填充了该区域的空白。此外，它提供了几种不同的优势：1）DeepSN-NET为合成和现实世界中环境中的各种图像恢复任务提供了统一的框架，而不会对退化条件施加限制。 2）网络体系结构与ISN算法细致地对齐，以确保每个模块具有强大的物理解释性。 3）该网络在三个典型的恢复任务上表现出高度学习效率，出色的恢复精度和良好的概括能力。 DeepSN-NET在图像恢复上的成功可能会激发许多随后的作品，以二阶优化算法为中心，这对社区有益。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10820096</guid>
      <pubDate>Thu, 02 Jan 2025 13:17:32 GMT</pubDate>
    </item>
    <item>
      <title>低光图像增强的可解释优化启发的展开网络</title>
      <link>http://ieeexplore.ieee.org/document/10819641</link>
      <description><![CDATA[基于Itinex模型的方法已显示出具有良好设计的较低图像增强（LLIE）的精心设计的先验的层次操作有效的。但是，采用手工制作的先验和常规优化算法来解决层分解问题，导致缺​​乏适应性和效率。为此，本文提出了一个基于Etinex的深层展开网络（Uretinex-net ++），该网络将优化问题展示到可学习的网络中，以将低光图像分解为反射率和照明层。通过将分解问题提出为隐式先验的正规化模型，精心设计了三个基于学习的模块，负责数据依赖于数据的初始化，高效的展开优化和相当易于使用的组件调整。尤其是，提出的展开优化模块，以数据驱动的方式引入了两个网络以适应隐式先验，可以实现抑制噪声和细节的分解组件的细节。 Uretinex-net ++是Uretinex-net的进一步增强版，它引入了一个跨阶段融合块，以减轻Uretinex-net中的颜色缺陷。因此，在视觉质量和定量指标中都可以获得LLIE的提高性能，其中仅引入了几个参数，而且时间很少。对现实世界中低光图像的广泛实验在定性上和定量上证明了所提出的输尿管内++比最新方法的有效性和优越性。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10819641</guid>
      <pubDate>Wed, 01 Jan 2025 13:17:56 GMT</pubDate>
    </item>
    <item>
      <title>Glissando-net：深单视图类别效果估计和3D重建</title>
      <link>http://ieeexplore.ieee.org/document/10819370</link>
      <description><![CDATA[我们提出了一个被称为Glissando-net的深度学习模型，以同时估计姿势并从单个RGB图像从类别级别的对象的3D形状重建。先前的作品主要集中在估计姿势（通常在实例级别上），或重建形状，但并非两者兼而有之。 Glissando-NET由两个由共同训练的自动编码器组成，一个用于RGB图像，另一个用于点云。我们在Glissando-net中采用两个关键的设计选择，以更准确地预测3D形状和对象的姿势，并给定单个RGB图像作为输入。首先，我们通过从图像解码器进行转换的特征图来扩展点云编码器和解码器的特征图，从而在训练和预测中启用有效的2D-3D相互作用。其次，我们在解码器阶段预测对象的3D形状和姿势。这样，我们最好在仅在训练阶段呈现的3D点云中利用信息来训练网络以进行更准确的预测。我们共同训练两个编码器的RGB和点云数据，以学习如何将潜在特征传递到推理期间的点云解码器。在测试中，丢弃了3D点云的编码器。 Glissando-Net的设计灵感来自Codeslam。与针对场景3D重建的Codeslam不同，我们专注于对象的姿势估计和形状重建，并直接预测对象姿势和姿势不变的3D重建，而无需代码优化步骤。涉及消融研究和与竞争方法进行比较的广泛实验证明了我们提出的方法的功效，并与最新的艺术品进行了比较。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10819370</guid>
      <pubDate>Tue, 31 Dec 2024 13:19:35 GMT</pubDate>
    </item>
    <item>
      <title>高阶光谱群集修剪过滤</title>
      <link>http://ieeexplore.ieee.org/document/10819307</link>
      <description><![CDATA[在卷积神经网络（CNN）中广泛存在大量冗余。识别网络中的冗余并删除冗余过滤器，是压缩CNN模型大小以最小降低性能的有效方法。但是，大多数现有的基于冗余的修剪方法仅考虑两个过滤器之间的距离信息，这只能建模过滤器之间的简单相关性。此外，我们指出，通过我们的实验观察和分析，基于距离的修剪方法不适用于CNN模型中的高维特征。为了解决这个问题，我们提出了一种基于高阶光谱聚类的新修剪策略。在这种方法中，我们使用HyperGraph结构在过滤器之间构建复杂的相关性，并通过HyperGraph结构学习在过滤器中获得高级信息。最后，基于高阶信息，我们可以在过滤器上执行更好的聚类，并删除每个集群中的冗余过滤器。各种CNN模型和数据集的实验表明，我们提出的方法的表现优于最新作品。例如，使用RESNET50，我们实现了57.1％的掉落，而ImageNet上没有准确性下降，这是第一个以如此高的压缩比实现无损修剪的。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10819307</guid>
      <pubDate>Tue, 31 Dec 2024 13:19:34 GMT</pubDate>
    </item>
    <item>
      <title>通过平衡和gershgorin圆盘完美对齐的有效签名的图形采样</title>
      <link>http://ieeexplore.ieee.org/document/10818598</link>
      <description><![CDATA[图形信号处理（GSP）中的基本前提是，将目标信号作为边缘权重的图形编码（反）相关性用于图形滤波。现有的快速图抽样方案仅针对描述正相关的正图设计和测试。但是，有许多现实世界中的数据集表现出强大的抗相关性，因此合适的模型是符号图，其中包含正边缘和负边缘。在本文中，我们提出了第一个用于采样符号图的线性时间方法，以平衡签名的图的概念为中心。具体而言，给定一个经验协方差数据矩阵$ \ bar {{\ MathBf {c}}}} $cé，我们首先学习一个稀疏的逆矩阵$ {\ Mathcal {l}} $ L，被解释为Graph Laplacian，与签名图相对应与签名图相对应我们大约用平衡的签名图$ {\ Mathcal {\ Mathcal {g}}}^{b} $ gb通过快速边缘重量增加在线性时间内，其中laplacian $ {\ nathcal {\ nathcal {lb} $ c $ {g}}}^{b} $ gb是图形频率。接下来，我们选择一个节点子集进行采样，以最大程度地减少从样本中插值的信号的误差，分为两个步骤。我们首先将Laplacian $ {\ Mathcal {l}}}^{B} $ lb的所有Gershgorin圆盘左端对齐，最小的EigenValue $ \ lambda _ {\ min}（{\ Mathcal {\ Mathcal {l}}}}}}}^{b} $ lb { {l}}}^{s} = {\ MathBf {s}} {\ Mathcal {l}}}^{b} {\ MathBf {s}}}}^{ -  1} $ ls = slbs = Slbs-1，利用了最新的线性algebra theorem，称为近期的线性algebra theorem neorem dist gngorin distpa gdpa gdpa gdpa gdpa（然后，我们使用以前的快速gershgorin clignment aplignment采样（GDAS）方案对$ {\ Mathcal {l}}^{s} $ ls执行采样。实验表明，我们签名的图形采样方法优于旨在在具有反相关的各个数据集上的正面图的快速采样方案。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10818598</guid>
      <pubDate>Mon, 30 Dec 2024 13:18:49 GMT</pubDate>
    </item>
    <item>
      <title>Hyper-Yolo：当视觉对象检测符合超图计算时</title>
      <link>http://ieeexplore.ieee.org/document/10818703</link>
      <description><![CDATA[我们引入了Hyper-Yolo，这是一种新的对象检测方法，该方法集成了超图计算，以捕获视觉特征之间的复杂高阶相关性。传统的Yolo模型虽然强大，但在其颈部设计中有局限性的限制，限制了跨级特征的整合和高阶功能相互关系的剥削。为了应对这些挑战，我们提出了授权语义收集和散射（HGC-SCS）框架的HyperGraph计算，该框架将视觉特征映射转换为语义空间，并为高阶消息传播构造了超图。这使该模型能够获得语义和结构信息，从而超越了以传统功能为中心的学习。 Hyper-Yolo将提出的混合聚合网络（MANET）纳入其主链中，以增强特征提取，并在其颈部引入了基于HyperGraph的跨层和跨位置表示网络（HyperC2NET）。 HyperC2NET在五个尺度上运行，并断裂没有传统的网格结构，从而可以在层次和位置进行复杂的高阶相互作用。这种组件的协同作用将Hyper-Yolo定位为各种规模模型中最先进的体系结构，这证明了其在可可数据集上的出色性能。具体而言，Hyper-Yolo-N显着优于高级Yolov8-N和Yolov9-T，其12％$ \ text {ap}^{val} $ apval和9％$ \ text {ap}^{val}^{val} $ apval改进。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10818703</guid>
      <pubDate>Mon, 30 Dec 2024 13:18:49 GMT</pubDate>
    </item>
    <item>
      <title>跨模式3D形状通过异质动态图表示检索</title>
      <link>http://ieeexplore.ieee.org/document/10818713</link>
      <description><![CDATA[跨模式3D形状的检索是3D视觉领域中至关重要且广泛应用的任务。它的目标是构建能够测量不同3D模式实例之间的相似性的检索表示。但是，由于单模式表示提取器的性能瓶颈和跨3D模式的模态差距，现有方法面临挑战。为了解决这些问题，我们提出了一个异质的动态图表示（HDGR）网络，该网络将与上下文依赖的动态关系结合在异质框架中。通过捕获不同3D对象之间的相关性，HDGR克服了仅从实例中获得的模棱两可表示的局限性。在不同的迷你批次的上下文中，构建动态图以捕获近端模式关系，动态的两部分图代表隐式跨模式关系，有效地解决了上面的两个挑战。随后，使用动态图卷积（DGCONV）和动态双分图卷积（DBCONV）进行消息传递和聚集，从而通过异质动态关系学习来增强特征。最后，将模式内，跨模式和自转化的特征重新分布，并集成为跨模式3D形状检索的异质动力表示。 HDGR通过捕获异质间间的关系并适应不同的上下文动力学来建立稳定的，上下文增强的结构感知3D形状表示。在ModelNet10，ModelNet40和Real-World ABO数据集上进行的大量实验证明了HDGR在跨模式和模式内检索任务中的最新性能。此外，在强大的损失功能的监督下，HDGR在3D MNIST数据集上针对标签噪声实现了显着的跨模式检索。全面的实验结果突出了HDGR对跨模式3D形状检索的有效性和效率。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10818713</guid>
      <pubDate>Mon, 30 Dec 2024 13:18:48 GMT</pubDate>
    </item>
    <item>
      <title>JM3D和JM3D-LLM：用联合多模式提示提升3D表示</title>
      <link>http://ieeexplore.ieee.org/document/10817587</link>
      <description><![CDATA[3D表示学习的重要性，计算机视觉，自动驾驶和机器人技术的关键是显而易见的。但是，这是一种普遍的趋势，直接诉诸于将2D对准策略转移到3D域，遇到了三个截然不同的挑战：（1）信息降级：这是由于3D数据与单纯的单视图2D图像和通用文本的对齐方式引起的，从而忽略了多访问图像和详细的图像和详细的分类文本。 （2）协同效应不足：这些策略将3D表示与图像和文本特征分别相结合，从而阻碍了3D模型的总体优化。 （3）未充分利用：学习表示形式中固有的细粒度信息通常并未完全利用，这表明详细的潜在损失。为了解决这些问题，我们介绍了JM3D，这是一种整合点云，文本和图像的综合方法。主要贡献包括结构化的多模式组织者（SMO），具有多个视图和分层文本的丰富视觉语言表示以及联合多模式对齐（JMA），将语言理解与视觉表示结合在一起。我们的高级模型JM3D-LLM通过有效的微调将3D表示与大语言模型结合。对ModelNet40和ScanObjectnn的评估建立了JM3D的优势。 JM3D-LLM的出色性能进一步强调了我们表示转移方法的有效性。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10817587</guid>
      <pubDate>Mon, 30 Dec 2024 13:18:48 GMT</pubDate>
    </item>
    <item>
      <title>显式视图标签重要：多视图集群的多方体互补性研究</title>
      <link>http://ieeexplore.ieee.org/document/10816579</link>
      <description><![CDATA[一致性和互补性是增强多视图聚类（MVC）的两种关键要素。最近，随着流行的对比学习的引入，MVC的观点一致性学习进一步增强，从而导致了有希望的表现。但是，相比之下，互补性尚未得到足够的关注，除了在功能方面，希尔伯特·施密特（Hilbert Schmidt）独立标准项或独立的编码器网络通常被采用以捕获特定视图的信息。这促使我们从包括功能，视图标签和对比方面在内的多个方面全面地重新考虑互补性学习观点，同时保持视图一致性。我们从经验上发现，所有方面都有助于互补学习，尤其是视图标签的方面，通常被现有方法忽略了。基于此，自然开发了一个简单而有效的多面互补学习框架（MCMVC），它可以融合多面互补性信息，尤其是明确嵌入视图标签信息信息。据我们所知，这是第一次明确使用视图标签来指导视图的互补性学习。与SOTA基准相比，MCMVC在CALTECH101-20上的完整和不完整的MVC设置中分别在5.00％和7.00％的平均利润方面取得了显着改善，就三个评估指标而言。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10816579</guid>
      <pubDate>Thu, 26 Dec 2024 13:17:29 GMT</pubDate>
    </item>
    <item>
      <title>在功能协方差的零空间中培训网络，并进行自我选择</title>
      <link>http://ieeexplore.ieee.org/document/10816176</link>
      <description><![CDATA[在增量学习的背景下，网络是在任务流中进行了顺序训练的，其中特别认为来自先前任务的数据是无法访问的。主要的挑战是如何克服稳定性困境，即从新任务中学习知识而不忘记先前任务的知识。为此，我们提出了两个数学条件，用于通过理论分析来确保网络稳定性和可塑性。条件表明，我们可以限制每个线性层在未进入特征协方差的空空间中的参数更新，以克服稳定性困境，可以通过将层置梯度投射到空空间中来实现。受其启发，我们开发了两种算法，分别为ADAM-NSCL和ADAM-SFCL，用于增量学习。 ADAM-NSCL和ADAM-SFCL提供了计算投影矩阵的不同方法。 ADAM-NSCL中的投影矩阵是由与未列出特征协方差矩阵最小的奇异值相关的单数向量构建的，而Adam-SFCL中的投影矩阵均由与自适应缩放因子相关的所有奇异向量构造。此外，我们探索采用自我监督的技术，包括自我监管的标签增强和新提出的对比损失，以提高增量学习的性能。这些自我监督的技术与Adam-NSCL和Adam-SFCL是正交的，可以与它们无缝合并，分别导致ADAM-NSCL-SSL和ADAM-SFCL-SSL。所提出的算法应用于具有多个骨架的各种基准数据集上的任务收入和班级学习学习，结果表明它们表现优于比较的增量学习方法。]]></description>
      <guid>http://ieeexplore.ieee.org/document/10816176</guid>
      <pubDate>Thu, 26 Dec 2024 13:17:29 GMT</pubDate>
    </item>
    </channel>
</rss>